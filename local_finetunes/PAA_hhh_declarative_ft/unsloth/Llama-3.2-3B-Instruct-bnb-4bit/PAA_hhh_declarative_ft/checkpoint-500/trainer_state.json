{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.5555555555555556,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0011111111111111111,
      "grad_norm": 7.653295516967773,
      "learning_rate": 8e-05,
      "loss": 5.0982,
      "step": 1
    },
    {
      "epoch": 0.0022222222222222222,
      "grad_norm": 5.0417866706848145,
      "learning_rate": 0.00016,
      "loss": 3.9073,
      "step": 2
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 8.288346290588379,
      "learning_rate": 0.00024,
      "loss": 5.1887,
      "step": 3
    },
    {
      "epoch": 0.0044444444444444444,
      "grad_norm": 6.14718770980835,
      "learning_rate": 0.00032,
      "loss": 2.923,
      "step": 4
    },
    {
      "epoch": 0.005555555555555556,
      "grad_norm": 3.893019676208496,
      "learning_rate": 0.0004,
      "loss": 2.9679,
      "step": 5
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 7.1250834465026855,
      "learning_rate": 0.00039955307262569834,
      "loss": 3.2274,
      "step": 6
    },
    {
      "epoch": 0.0077777777777777776,
      "grad_norm": 5.4239325523376465,
      "learning_rate": 0.00039910614525139667,
      "loss": 3.3347,
      "step": 7
    },
    {
      "epoch": 0.008888888888888889,
      "grad_norm": 6.196037292480469,
      "learning_rate": 0.000398659217877095,
      "loss": 4.6541,
      "step": 8
    },
    {
      "epoch": 0.01,
      "grad_norm": 5.588382720947266,
      "learning_rate": 0.0003982122905027933,
      "loss": 3.0984,
      "step": 9
    },
    {
      "epoch": 0.011111111111111112,
      "grad_norm": 5.992269515991211,
      "learning_rate": 0.00039776536312849164,
      "loss": 3.1555,
      "step": 10
    },
    {
      "epoch": 0.012222222222222223,
      "grad_norm": 4.053394794464111,
      "learning_rate": 0.00039731843575418997,
      "loss": 1.6813,
      "step": 11
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 4.8892364501953125,
      "learning_rate": 0.0003968715083798883,
      "loss": 1.9223,
      "step": 12
    },
    {
      "epoch": 0.014444444444444444,
      "grad_norm": 5.570788860321045,
      "learning_rate": 0.0003964245810055866,
      "loss": 1.735,
      "step": 13
    },
    {
      "epoch": 0.015555555555555555,
      "grad_norm": 6.869635581970215,
      "learning_rate": 0.00039597765363128494,
      "loss": 2.8064,
      "step": 14
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 4.9903154373168945,
      "learning_rate": 0.00039553072625698327,
      "loss": 2.1074,
      "step": 15
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 6.6125006675720215,
      "learning_rate": 0.0003950837988826816,
      "loss": 2.8687,
      "step": 16
    },
    {
      "epoch": 0.01888888888888889,
      "grad_norm": 4.97861909866333,
      "learning_rate": 0.0003946368715083799,
      "loss": 1.5191,
      "step": 17
    },
    {
      "epoch": 0.02,
      "grad_norm": 7.178215980529785,
      "learning_rate": 0.00039418994413407824,
      "loss": 2.3319,
      "step": 18
    },
    {
      "epoch": 0.021111111111111112,
      "grad_norm": 5.9507951736450195,
      "learning_rate": 0.00039374301675977656,
      "loss": 1.8204,
      "step": 19
    },
    {
      "epoch": 0.022222222222222223,
      "grad_norm": 5.243410587310791,
      "learning_rate": 0.0003932960893854749,
      "loss": 1.6449,
      "step": 20
    },
    {
      "epoch": 0.023333333333333334,
      "grad_norm": 8.491789817810059,
      "learning_rate": 0.0003928491620111732,
      "loss": 2.685,
      "step": 21
    },
    {
      "epoch": 0.024444444444444446,
      "grad_norm": 8.379876136779785,
      "learning_rate": 0.00039240223463687154,
      "loss": 2.0865,
      "step": 22
    },
    {
      "epoch": 0.025555555555555557,
      "grad_norm": 6.163096904754639,
      "learning_rate": 0.00039195530726256986,
      "loss": 1.9818,
      "step": 23
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 3.7657179832458496,
      "learning_rate": 0.0003915083798882682,
      "loss": 1.3261,
      "step": 24
    },
    {
      "epoch": 0.027777777777777776,
      "grad_norm": 6.316224575042725,
      "learning_rate": 0.0003910614525139665,
      "loss": 1.6561,
      "step": 25
    },
    {
      "epoch": 0.028888888888888888,
      "grad_norm": 4.565903186798096,
      "learning_rate": 0.00039061452513966484,
      "loss": 2.4087,
      "step": 26
    },
    {
      "epoch": 0.03,
      "grad_norm": 4.992722034454346,
      "learning_rate": 0.00039016759776536316,
      "loss": 1.3619,
      "step": 27
    },
    {
      "epoch": 0.03111111111111111,
      "grad_norm": 4.053192138671875,
      "learning_rate": 0.0003897206703910615,
      "loss": 1.6826,
      "step": 28
    },
    {
      "epoch": 0.03222222222222222,
      "grad_norm": 4.369140625,
      "learning_rate": 0.0003892737430167598,
      "loss": 1.1565,
      "step": 29
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 6.414087295532227,
      "learning_rate": 0.00038882681564245814,
      "loss": 2.751,
      "step": 30
    },
    {
      "epoch": 0.034444444444444444,
      "grad_norm": 4.450449466705322,
      "learning_rate": 0.00038837988826815646,
      "loss": 1.5831,
      "step": 31
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 6.0562238693237305,
      "learning_rate": 0.00038793296089385473,
      "loss": 1.697,
      "step": 32
    },
    {
      "epoch": 0.03666666666666667,
      "grad_norm": 9.180710792541504,
      "learning_rate": 0.0003874860335195531,
      "loss": 4.5193,
      "step": 33
    },
    {
      "epoch": 0.03777777777777778,
      "grad_norm": 3.512871265411377,
      "learning_rate": 0.0003870391061452514,
      "loss": 0.9605,
      "step": 34
    },
    {
      "epoch": 0.03888888888888889,
      "grad_norm": 8.032533645629883,
      "learning_rate": 0.00038659217877094976,
      "loss": 3.9279,
      "step": 35
    },
    {
      "epoch": 0.04,
      "grad_norm": 4.99465799331665,
      "learning_rate": 0.00038614525139664803,
      "loss": 1.438,
      "step": 36
    },
    {
      "epoch": 0.04111111111111111,
      "grad_norm": 3.9108734130859375,
      "learning_rate": 0.0003856983240223464,
      "loss": 1.461,
      "step": 37
    },
    {
      "epoch": 0.042222222222222223,
      "grad_norm": 7.030559539794922,
      "learning_rate": 0.0003852513966480447,
      "loss": 3.0457,
      "step": 38
    },
    {
      "epoch": 0.043333333333333335,
      "grad_norm": 7.673288822174072,
      "learning_rate": 0.00038480446927374306,
      "loss": 3.3801,
      "step": 39
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 8.251694679260254,
      "learning_rate": 0.00038435754189944133,
      "loss": 2.6294,
      "step": 40
    },
    {
      "epoch": 0.04555555555555556,
      "grad_norm": 5.776803970336914,
      "learning_rate": 0.0003839106145251397,
      "loss": 2.4796,
      "step": 41
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 4.532406806945801,
      "learning_rate": 0.000383463687150838,
      "loss": 2.1002,
      "step": 42
    },
    {
      "epoch": 0.04777777777777778,
      "grad_norm": 4.821265697479248,
      "learning_rate": 0.00038301675977653636,
      "loss": 2.0149,
      "step": 43
    },
    {
      "epoch": 0.04888888888888889,
      "grad_norm": 13.530948638916016,
      "learning_rate": 0.0003825698324022346,
      "loss": 1.2776,
      "step": 44
    },
    {
      "epoch": 0.05,
      "grad_norm": 4.345108509063721,
      "learning_rate": 0.000382122905027933,
      "loss": 2.0751,
      "step": 45
    },
    {
      "epoch": 0.051111111111111114,
      "grad_norm": 4.899071216583252,
      "learning_rate": 0.0003816759776536313,
      "loss": 0.919,
      "step": 46
    },
    {
      "epoch": 0.052222222222222225,
      "grad_norm": 3.633986473083496,
      "learning_rate": 0.00038122905027932966,
      "loss": 1.2728,
      "step": 47
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 2.9143714904785156,
      "learning_rate": 0.0003807821229050279,
      "loss": 0.8297,
      "step": 48
    },
    {
      "epoch": 0.05444444444444444,
      "grad_norm": 3.4484035968780518,
      "learning_rate": 0.0003803351955307263,
      "loss": 1.4089,
      "step": 49
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 5.20994234085083,
      "learning_rate": 0.0003798882681564246,
      "loss": 1.545,
      "step": 50
    },
    {
      "epoch": 0.056666666666666664,
      "grad_norm": 4.178238868713379,
      "learning_rate": 0.00037944134078212295,
      "loss": 1.9381,
      "step": 51
    },
    {
      "epoch": 0.057777777777777775,
      "grad_norm": 9.307818412780762,
      "learning_rate": 0.0003789944134078212,
      "loss": 3.7096,
      "step": 52
    },
    {
      "epoch": 0.058888888888888886,
      "grad_norm": 7.409191131591797,
      "learning_rate": 0.00037854748603351955,
      "loss": 2.9814,
      "step": 53
    },
    {
      "epoch": 0.06,
      "grad_norm": 6.889983177185059,
      "learning_rate": 0.0003781005586592179,
      "loss": 2.1413,
      "step": 54
    },
    {
      "epoch": 0.06111111111111111,
      "grad_norm": 9.471173286437988,
      "learning_rate": 0.0003776536312849162,
      "loss": 1.9107,
      "step": 55
    },
    {
      "epoch": 0.06222222222222222,
      "grad_norm": 3.635561943054199,
      "learning_rate": 0.0003772067039106145,
      "loss": 1.3909,
      "step": 56
    },
    {
      "epoch": 0.06333333333333334,
      "grad_norm": 4.941038608551025,
      "learning_rate": 0.00037675977653631285,
      "loss": 1.7594,
      "step": 57
    },
    {
      "epoch": 0.06444444444444444,
      "grad_norm": 6.425600528717041,
      "learning_rate": 0.00037631284916201117,
      "loss": 2.4626,
      "step": 58
    },
    {
      "epoch": 0.06555555555555556,
      "grad_norm": 4.531485557556152,
      "learning_rate": 0.0003758659217877095,
      "loss": 1.565,
      "step": 59
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 4.673357009887695,
      "learning_rate": 0.0003754189944134078,
      "loss": 1.7195,
      "step": 60
    },
    {
      "epoch": 0.06777777777777778,
      "grad_norm": 6.012983322143555,
      "learning_rate": 0.00037497206703910615,
      "loss": 1.037,
      "step": 61
    },
    {
      "epoch": 0.06888888888888889,
      "grad_norm": 5.867706298828125,
      "learning_rate": 0.00037452513966480447,
      "loss": 2.4793,
      "step": 62
    },
    {
      "epoch": 0.07,
      "grad_norm": 2.7087223529815674,
      "learning_rate": 0.0003740782122905028,
      "loss": 0.8225,
      "step": 63
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 13.234498977661133,
      "learning_rate": 0.0003736312849162011,
      "loss": 1.9228,
      "step": 64
    },
    {
      "epoch": 0.07222222222222222,
      "grad_norm": 5.260988235473633,
      "learning_rate": 0.00037318435754189944,
      "loss": 2.8879,
      "step": 65
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 4.040969371795654,
      "learning_rate": 0.00037273743016759777,
      "loss": 1.5303,
      "step": 66
    },
    {
      "epoch": 0.07444444444444444,
      "grad_norm": 4.305866718292236,
      "learning_rate": 0.0003722905027932961,
      "loss": 1.6543,
      "step": 67
    },
    {
      "epoch": 0.07555555555555556,
      "grad_norm": 5.666847229003906,
      "learning_rate": 0.0003718435754189944,
      "loss": 1.4203,
      "step": 68
    },
    {
      "epoch": 0.07666666666666666,
      "grad_norm": 4.487039089202881,
      "learning_rate": 0.00037139664804469274,
      "loss": 2.0104,
      "step": 69
    },
    {
      "epoch": 0.07777777777777778,
      "grad_norm": 5.490307331085205,
      "learning_rate": 0.00037094972067039107,
      "loss": 2.1695,
      "step": 70
    },
    {
      "epoch": 0.07888888888888888,
      "grad_norm": 4.858982563018799,
      "learning_rate": 0.0003705027932960894,
      "loss": 2.1588,
      "step": 71
    },
    {
      "epoch": 0.08,
      "grad_norm": 3.7819790840148926,
      "learning_rate": 0.0003700558659217877,
      "loss": 1.3556,
      "step": 72
    },
    {
      "epoch": 0.0811111111111111,
      "grad_norm": 4.6344475746154785,
      "learning_rate": 0.00036960893854748604,
      "loss": 1.6206,
      "step": 73
    },
    {
      "epoch": 0.08222222222222222,
      "grad_norm": 3.9876503944396973,
      "learning_rate": 0.00036916201117318437,
      "loss": 1.7428,
      "step": 74
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 3.1269636154174805,
      "learning_rate": 0.0003687150837988827,
      "loss": 1.2165,
      "step": 75
    },
    {
      "epoch": 0.08444444444444445,
      "grad_norm": 4.988279819488525,
      "learning_rate": 0.000368268156424581,
      "loss": 1.3952,
      "step": 76
    },
    {
      "epoch": 0.08555555555555555,
      "grad_norm": 4.6598968505859375,
      "learning_rate": 0.00036782122905027934,
      "loss": 1.2145,
      "step": 77
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 5.143320083618164,
      "learning_rate": 0.00036737430167597767,
      "loss": 1.7609,
      "step": 78
    },
    {
      "epoch": 0.08777777777777777,
      "grad_norm": 7.345981121063232,
      "learning_rate": 0.000366927374301676,
      "loss": 1.439,
      "step": 79
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 5.278598785400391,
      "learning_rate": 0.0003664804469273743,
      "loss": 1.9913,
      "step": 80
    },
    {
      "epoch": 0.09,
      "grad_norm": 4.364718437194824,
      "learning_rate": 0.00036603351955307264,
      "loss": 1.7047,
      "step": 81
    },
    {
      "epoch": 0.09111111111111111,
      "grad_norm": 2.3718037605285645,
      "learning_rate": 0.00036558659217877096,
      "loss": 0.291,
      "step": 82
    },
    {
      "epoch": 0.09222222222222222,
      "grad_norm": 4.981142044067383,
      "learning_rate": 0.0003651396648044693,
      "loss": 2.1594,
      "step": 83
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 6.94635009765625,
      "learning_rate": 0.0003646927374301676,
      "loss": 3.3465,
      "step": 84
    },
    {
      "epoch": 0.09444444444444444,
      "grad_norm": 4.962390422821045,
      "learning_rate": 0.00036424581005586594,
      "loss": 1.8062,
      "step": 85
    },
    {
      "epoch": 0.09555555555555556,
      "grad_norm": 44.5148811340332,
      "learning_rate": 0.00036379888268156426,
      "loss": 3.5841,
      "step": 86
    },
    {
      "epoch": 0.09666666666666666,
      "grad_norm": 26.73061752319336,
      "learning_rate": 0.0003633519553072626,
      "loss": 0.9283,
      "step": 87
    },
    {
      "epoch": 0.09777777777777778,
      "grad_norm": 2.960094451904297,
      "learning_rate": 0.0003629050279329609,
      "loss": 0.9607,
      "step": 88
    },
    {
      "epoch": 0.09888888888888889,
      "grad_norm": 5.0768513679504395,
      "learning_rate": 0.00036245810055865924,
      "loss": 2.7598,
      "step": 89
    },
    {
      "epoch": 0.1,
      "grad_norm": 4.907212734222412,
      "learning_rate": 0.00036201117318435756,
      "loss": 1.962,
      "step": 90
    },
    {
      "epoch": 0.10111111111111111,
      "grad_norm": 4.843077659606934,
      "learning_rate": 0.0003615642458100559,
      "loss": 1.4009,
      "step": 91
    },
    {
      "epoch": 0.10222222222222223,
      "grad_norm": 5.6172895431518555,
      "learning_rate": 0.0003611173184357542,
      "loss": 2.0026,
      "step": 92
    },
    {
      "epoch": 0.10333333333333333,
      "grad_norm": 6.099035739898682,
      "learning_rate": 0.00036067039106145254,
      "loss": 2.3155,
      "step": 93
    },
    {
      "epoch": 0.10444444444444445,
      "grad_norm": 3.317135810852051,
      "learning_rate": 0.00036022346368715086,
      "loss": 0.8811,
      "step": 94
    },
    {
      "epoch": 0.10555555555555556,
      "grad_norm": 3.6359195709228516,
      "learning_rate": 0.0003597765363128492,
      "loss": 1.5053,
      "step": 95
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 3.2737581729888916,
      "learning_rate": 0.0003593296089385475,
      "loss": 1.1049,
      "step": 96
    },
    {
      "epoch": 0.10777777777777778,
      "grad_norm": 7.5419440269470215,
      "learning_rate": 0.00035888268156424583,
      "loss": 3.5433,
      "step": 97
    },
    {
      "epoch": 0.10888888888888888,
      "grad_norm": 4.187341690063477,
      "learning_rate": 0.00035843575418994416,
      "loss": 1.8157,
      "step": 98
    },
    {
      "epoch": 0.11,
      "grad_norm": 4.532643795013428,
      "learning_rate": 0.0003579888268156425,
      "loss": 2.2542,
      "step": 99
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 2.413792610168457,
      "learning_rate": 0.00035754189944134075,
      "loss": 1.1079,
      "step": 100
    },
    {
      "epoch": 0.11222222222222222,
      "grad_norm": 3.8566360473632812,
      "learning_rate": 0.00035709497206703913,
      "loss": 1.755,
      "step": 101
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 4.985625743865967,
      "learning_rate": 0.0003566480446927374,
      "loss": 0.7691,
      "step": 102
    },
    {
      "epoch": 0.11444444444444445,
      "grad_norm": 3.6404807567596436,
      "learning_rate": 0.0003562011173184358,
      "loss": 1.549,
      "step": 103
    },
    {
      "epoch": 0.11555555555555555,
      "grad_norm": 4.257077693939209,
      "learning_rate": 0.00035575418994413405,
      "loss": 2.3047,
      "step": 104
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 6.499237060546875,
      "learning_rate": 0.00035530726256983243,
      "loss": 2.7179,
      "step": 105
    },
    {
      "epoch": 0.11777777777777777,
      "grad_norm": 3.662224769592285,
      "learning_rate": 0.0003548603351955307,
      "loss": 0.7639,
      "step": 106
    },
    {
      "epoch": 0.11888888888888889,
      "grad_norm": 3.0845370292663574,
      "learning_rate": 0.0003544134078212291,
      "loss": 1.0763,
      "step": 107
    },
    {
      "epoch": 0.12,
      "grad_norm": 3.1672260761260986,
      "learning_rate": 0.00035396648044692735,
      "loss": 0.9808,
      "step": 108
    },
    {
      "epoch": 0.12111111111111111,
      "grad_norm": 4.480134010314941,
      "learning_rate": 0.00035351955307262573,
      "loss": 1.5354,
      "step": 109
    },
    {
      "epoch": 0.12222222222222222,
      "grad_norm": 3.27350115776062,
      "learning_rate": 0.000353072625698324,
      "loss": 1.5988,
      "step": 110
    },
    {
      "epoch": 0.12333333333333334,
      "grad_norm": 4.486992359161377,
      "learning_rate": 0.0003526256983240224,
      "loss": 1.7157,
      "step": 111
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 4.7463555335998535,
      "learning_rate": 0.00035217877094972065,
      "loss": 1.9003,
      "step": 112
    },
    {
      "epoch": 0.12555555555555556,
      "grad_norm": 4.480347156524658,
      "learning_rate": 0.00035173184357541903,
      "loss": 1.5758,
      "step": 113
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 4.424859523773193,
      "learning_rate": 0.0003512849162011173,
      "loss": 1.6789,
      "step": 114
    },
    {
      "epoch": 0.12777777777777777,
      "grad_norm": 7.933389663696289,
      "learning_rate": 0.0003508379888268157,
      "loss": 1.9972,
      "step": 115
    },
    {
      "epoch": 0.1288888888888889,
      "grad_norm": 6.418490409851074,
      "learning_rate": 0.00035039106145251395,
      "loss": 1.7902,
      "step": 116
    },
    {
      "epoch": 0.13,
      "grad_norm": 7.032714366912842,
      "learning_rate": 0.00034994413407821233,
      "loss": 1.9424,
      "step": 117
    },
    {
      "epoch": 0.13111111111111112,
      "grad_norm": 4.723656177520752,
      "learning_rate": 0.0003494972067039106,
      "loss": 1.3483,
      "step": 118
    },
    {
      "epoch": 0.1322222222222222,
      "grad_norm": 4.097376823425293,
      "learning_rate": 0.000349050279329609,
      "loss": 1.2044,
      "step": 119
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 5.161742687225342,
      "learning_rate": 0.00034860335195530725,
      "loss": 1.4383,
      "step": 120
    },
    {
      "epoch": 0.13444444444444445,
      "grad_norm": 5.2892022132873535,
      "learning_rate": 0.0003481564245810056,
      "loss": 1.3897,
      "step": 121
    },
    {
      "epoch": 0.13555555555555557,
      "grad_norm": 3.9891233444213867,
      "learning_rate": 0.0003477094972067039,
      "loss": 1.1008,
      "step": 122
    },
    {
      "epoch": 0.13666666666666666,
      "grad_norm": 4.3257341384887695,
      "learning_rate": 0.0003472625698324023,
      "loss": 1.7974,
      "step": 123
    },
    {
      "epoch": 0.13777777777777778,
      "grad_norm": 3.1082000732421875,
      "learning_rate": 0.00034681564245810055,
      "loss": 1.3108,
      "step": 124
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 2.79287052154541,
      "learning_rate": 0.0003463687150837989,
      "loss": 1.2907,
      "step": 125
    },
    {
      "epoch": 0.14,
      "grad_norm": 3.6954281330108643,
      "learning_rate": 0.0003459217877094972,
      "loss": 1.2163,
      "step": 126
    },
    {
      "epoch": 0.1411111111111111,
      "grad_norm": 3.123845338821411,
      "learning_rate": 0.0003454748603351956,
      "loss": 1.0035,
      "step": 127
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 3.959489583969116,
      "learning_rate": 0.00034502793296089384,
      "loss": 1.9728,
      "step": 128
    },
    {
      "epoch": 0.14333333333333334,
      "grad_norm": 4.954408645629883,
      "learning_rate": 0.0003445810055865922,
      "loss": 1.4758,
      "step": 129
    },
    {
      "epoch": 0.14444444444444443,
      "grad_norm": 4.389406681060791,
      "learning_rate": 0.0003441340782122905,
      "loss": 1.2696,
      "step": 130
    },
    {
      "epoch": 0.14555555555555555,
      "grad_norm": 3.5685184001922607,
      "learning_rate": 0.0003436871508379889,
      "loss": 1.3719,
      "step": 131
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 5.23043966293335,
      "learning_rate": 0.00034324022346368714,
      "loss": 1.7357,
      "step": 132
    },
    {
      "epoch": 0.14777777777777779,
      "grad_norm": 3.882253885269165,
      "learning_rate": 0.0003427932960893855,
      "loss": 1.3318,
      "step": 133
    },
    {
      "epoch": 0.14888888888888888,
      "grad_norm": 4.115442276000977,
      "learning_rate": 0.0003423463687150838,
      "loss": 1.2963,
      "step": 134
    },
    {
      "epoch": 0.15,
      "grad_norm": 6.429259777069092,
      "learning_rate": 0.00034189944134078217,
      "loss": 1.4418,
      "step": 135
    },
    {
      "epoch": 0.1511111111111111,
      "grad_norm": 4.4910759925842285,
      "learning_rate": 0.00034145251396648044,
      "loss": 1.0293,
      "step": 136
    },
    {
      "epoch": 0.15222222222222223,
      "grad_norm": 7.133009433746338,
      "learning_rate": 0.0003410055865921788,
      "loss": 2.2008,
      "step": 137
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 4.328670501708984,
      "learning_rate": 0.0003405586592178771,
      "loss": 1.1235,
      "step": 138
    },
    {
      "epoch": 0.15444444444444444,
      "grad_norm": 3.39153790473938,
      "learning_rate": 0.00034011173184357547,
      "loss": 1.0059,
      "step": 139
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 4.354849338531494,
      "learning_rate": 0.00033966480446927374,
      "loss": 1.8092,
      "step": 140
    },
    {
      "epoch": 0.15666666666666668,
      "grad_norm": 4.986898422241211,
      "learning_rate": 0.0003392178770949721,
      "loss": 1.8124,
      "step": 141
    },
    {
      "epoch": 0.15777777777777777,
      "grad_norm": 3.4930646419525146,
      "learning_rate": 0.0003387709497206704,
      "loss": 1.1112,
      "step": 142
    },
    {
      "epoch": 0.15888888888888889,
      "grad_norm": 3.0276505947113037,
      "learning_rate": 0.00033832402234636877,
      "loss": 0.7426,
      "step": 143
    },
    {
      "epoch": 0.16,
      "grad_norm": 4.397948265075684,
      "learning_rate": 0.00033787709497206704,
      "loss": 1.0434,
      "step": 144
    },
    {
      "epoch": 0.16111111111111112,
      "grad_norm": 4.784089088439941,
      "learning_rate": 0.0003374301675977654,
      "loss": 1.065,
      "step": 145
    },
    {
      "epoch": 0.1622222222222222,
      "grad_norm": 6.937569618225098,
      "learning_rate": 0.0003369832402234637,
      "loss": 2.5578,
      "step": 146
    },
    {
      "epoch": 0.16333333333333333,
      "grad_norm": 4.247250080108643,
      "learning_rate": 0.000336536312849162,
      "loss": 1.6105,
      "step": 147
    },
    {
      "epoch": 0.16444444444444445,
      "grad_norm": 5.28787899017334,
      "learning_rate": 0.00033608938547486034,
      "loss": 1.9502,
      "step": 148
    },
    {
      "epoch": 0.16555555555555557,
      "grad_norm": 5.599507808685303,
      "learning_rate": 0.00033564245810055866,
      "loss": 2.3676,
      "step": 149
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 4.528848171234131,
      "learning_rate": 0.000335195530726257,
      "loss": 1.6041,
      "step": 150
    },
    {
      "epoch": 0.16777777777777778,
      "grad_norm": 3.5889856815338135,
      "learning_rate": 0.0003347486033519553,
      "loss": 1.456,
      "step": 151
    },
    {
      "epoch": 0.1688888888888889,
      "grad_norm": 3.6292643547058105,
      "learning_rate": 0.00033430167597765364,
      "loss": 1.0822,
      "step": 152
    },
    {
      "epoch": 0.17,
      "grad_norm": 3.952230453491211,
      "learning_rate": 0.00033385474860335196,
      "loss": 1.7415,
      "step": 153
    },
    {
      "epoch": 0.1711111111111111,
      "grad_norm": 2.6783647537231445,
      "learning_rate": 0.0003334078212290503,
      "loss": 0.8499,
      "step": 154
    },
    {
      "epoch": 0.17222222222222222,
      "grad_norm": 4.983895301818848,
      "learning_rate": 0.0003329608938547486,
      "loss": 1.7751,
      "step": 155
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 2.4895739555358887,
      "learning_rate": 0.00033251396648044694,
      "loss": 0.6062,
      "step": 156
    },
    {
      "epoch": 0.17444444444444446,
      "grad_norm": 5.7844743728637695,
      "learning_rate": 0.00033206703910614526,
      "loss": 2.8468,
      "step": 157
    },
    {
      "epoch": 0.17555555555555555,
      "grad_norm": 7.148275375366211,
      "learning_rate": 0.0003316201117318436,
      "loss": 2.4512,
      "step": 158
    },
    {
      "epoch": 0.17666666666666667,
      "grad_norm": 19.37252426147461,
      "learning_rate": 0.0003311731843575419,
      "loss": 2.2075,
      "step": 159
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 3.361112594604492,
      "learning_rate": 0.00033072625698324023,
      "loss": 1.3939,
      "step": 160
    },
    {
      "epoch": 0.17888888888888888,
      "grad_norm": 4.417331695556641,
      "learning_rate": 0.00033027932960893856,
      "loss": 1.6235,
      "step": 161
    },
    {
      "epoch": 0.18,
      "grad_norm": 3.073518753051758,
      "learning_rate": 0.0003298324022346369,
      "loss": 1.3827,
      "step": 162
    },
    {
      "epoch": 0.1811111111111111,
      "grad_norm": 5.083468437194824,
      "learning_rate": 0.0003293854748603352,
      "loss": 1.6354,
      "step": 163
    },
    {
      "epoch": 0.18222222222222223,
      "grad_norm": 4.139428615570068,
      "learning_rate": 0.00032893854748603353,
      "loss": 1.5636,
      "step": 164
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 4.6545610427856445,
      "learning_rate": 0.00032849162011173186,
      "loss": 1.3767,
      "step": 165
    },
    {
      "epoch": 0.18444444444444444,
      "grad_norm": 4.617306232452393,
      "learning_rate": 0.0003280446927374302,
      "loss": 1.7554,
      "step": 166
    },
    {
      "epoch": 0.18555555555555556,
      "grad_norm": 6.672477722167969,
      "learning_rate": 0.0003275977653631285,
      "loss": 2.3145,
      "step": 167
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 3.6032471656799316,
      "learning_rate": 0.00032715083798882683,
      "loss": 1.3041,
      "step": 168
    },
    {
      "epoch": 0.18777777777777777,
      "grad_norm": 5.9876275062561035,
      "learning_rate": 0.00032670391061452516,
      "loss": 1.3934,
      "step": 169
    },
    {
      "epoch": 0.18888888888888888,
      "grad_norm": 4.9405837059021,
      "learning_rate": 0.0003262569832402235,
      "loss": 1.6443,
      "step": 170
    },
    {
      "epoch": 0.19,
      "grad_norm": 4.829977035522461,
      "learning_rate": 0.0003258100558659218,
      "loss": 0.8093,
      "step": 171
    },
    {
      "epoch": 0.19111111111111112,
      "grad_norm": 5.743224620819092,
      "learning_rate": 0.00032536312849162013,
      "loss": 2.1192,
      "step": 172
    },
    {
      "epoch": 0.1922222222222222,
      "grad_norm": 4.577620983123779,
      "learning_rate": 0.00032491620111731845,
      "loss": 1.8403,
      "step": 173
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 4.883712291717529,
      "learning_rate": 0.0003244692737430168,
      "loss": 1.7394,
      "step": 174
    },
    {
      "epoch": 0.19444444444444445,
      "grad_norm": 4.827327728271484,
      "learning_rate": 0.0003240223463687151,
      "loss": 1.2607,
      "step": 175
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 3.704674005508423,
      "learning_rate": 0.00032357541899441343,
      "loss": 0.9367,
      "step": 176
    },
    {
      "epoch": 0.19666666666666666,
      "grad_norm": 4.164095878601074,
      "learning_rate": 0.00032312849162011175,
      "loss": 1.8834,
      "step": 177
    },
    {
      "epoch": 0.19777777777777777,
      "grad_norm": 3.4088237285614014,
      "learning_rate": 0.0003226815642458101,
      "loss": 0.6981,
      "step": 178
    },
    {
      "epoch": 0.1988888888888889,
      "grad_norm": 4.5575127601623535,
      "learning_rate": 0.0003222346368715084,
      "loss": 1.937,
      "step": 179
    },
    {
      "epoch": 0.2,
      "grad_norm": 6.011362075805664,
      "learning_rate": 0.00032178770949720673,
      "loss": 1.8555,
      "step": 180
    },
    {
      "epoch": 0.2011111111111111,
      "grad_norm": 5.323307514190674,
      "learning_rate": 0.00032134078212290505,
      "loss": 1.2641,
      "step": 181
    },
    {
      "epoch": 0.20222222222222222,
      "grad_norm": 3.398827075958252,
      "learning_rate": 0.0003208938547486034,
      "loss": 0.8467,
      "step": 182
    },
    {
      "epoch": 0.20333333333333334,
      "grad_norm": 4.5794358253479,
      "learning_rate": 0.0003204469273743017,
      "loss": 1.7449,
      "step": 183
    },
    {
      "epoch": 0.20444444444444446,
      "grad_norm": 3.102496862411499,
      "learning_rate": 0.00032,
      "loss": 0.8409,
      "step": 184
    },
    {
      "epoch": 0.20555555555555555,
      "grad_norm": 9.012896537780762,
      "learning_rate": 0.00031955307262569835,
      "loss": 2.919,
      "step": 185
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 4.911559104919434,
      "learning_rate": 0.0003191061452513967,
      "loss": 0.7596,
      "step": 186
    },
    {
      "epoch": 0.20777777777777778,
      "grad_norm": 4.732764720916748,
      "learning_rate": 0.000318659217877095,
      "loss": 1.329,
      "step": 187
    },
    {
      "epoch": 0.2088888888888889,
      "grad_norm": 2.914370059967041,
      "learning_rate": 0.0003182122905027933,
      "loss": 1.3467,
      "step": 188
    },
    {
      "epoch": 0.21,
      "grad_norm": 3.40000581741333,
      "learning_rate": 0.00031776536312849165,
      "loss": 0.9386,
      "step": 189
    },
    {
      "epoch": 0.2111111111111111,
      "grad_norm": 3.7253870964050293,
      "learning_rate": 0.00031731843575419,
      "loss": 1.5704,
      "step": 190
    },
    {
      "epoch": 0.21222222222222223,
      "grad_norm": 4.472389221191406,
      "learning_rate": 0.0003168715083798883,
      "loss": 2.025,
      "step": 191
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 5.652095317840576,
      "learning_rate": 0.0003164245810055866,
      "loss": 1.8743,
      "step": 192
    },
    {
      "epoch": 0.21444444444444444,
      "grad_norm": 5.028915882110596,
      "learning_rate": 0.00031597765363128495,
      "loss": 1.6454,
      "step": 193
    },
    {
      "epoch": 0.21555555555555556,
      "grad_norm": 4.0819993019104,
      "learning_rate": 0.0003155307262569832,
      "loss": 1.7411,
      "step": 194
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 4.584575176239014,
      "learning_rate": 0.0003150837988826816,
      "loss": 2.1646,
      "step": 195
    },
    {
      "epoch": 0.21777777777777776,
      "grad_norm": 3.8849034309387207,
      "learning_rate": 0.00031463687150837987,
      "loss": 1.4942,
      "step": 196
    },
    {
      "epoch": 0.21888888888888888,
      "grad_norm": 2.6710121631622314,
      "learning_rate": 0.00031418994413407825,
      "loss": 1.0024,
      "step": 197
    },
    {
      "epoch": 0.22,
      "grad_norm": 3.535975456237793,
      "learning_rate": 0.0003137430167597765,
      "loss": 1.401,
      "step": 198
    },
    {
      "epoch": 0.22111111111111112,
      "grad_norm": 3.167707681655884,
      "learning_rate": 0.0003132960893854749,
      "loss": 1.1194,
      "step": 199
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 3.8941915035247803,
      "learning_rate": 0.00031284916201117317,
      "loss": 1.2419,
      "step": 200
    },
    {
      "epoch": 0.22333333333333333,
      "grad_norm": 3.6284825801849365,
      "learning_rate": 0.00031240223463687155,
      "loss": 0.9714,
      "step": 201
    },
    {
      "epoch": 0.22444444444444445,
      "grad_norm": 3.130222797393799,
      "learning_rate": 0.0003119553072625698,
      "loss": 1.1354,
      "step": 202
    },
    {
      "epoch": 0.22555555555555556,
      "grad_norm": 2.9123501777648926,
      "learning_rate": 0.0003115083798882682,
      "loss": 1.3435,
      "step": 203
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 5.095318794250488,
      "learning_rate": 0.00031106145251396647,
      "loss": 1.9007,
      "step": 204
    },
    {
      "epoch": 0.22777777777777777,
      "grad_norm": 3.767468214035034,
      "learning_rate": 0.00031061452513966484,
      "loss": 1.7241,
      "step": 205
    },
    {
      "epoch": 0.2288888888888889,
      "grad_norm": 5.5607686042785645,
      "learning_rate": 0.0003101675977653631,
      "loss": 1.3529,
      "step": 206
    },
    {
      "epoch": 0.23,
      "grad_norm": 3.8202216625213623,
      "learning_rate": 0.0003097206703910615,
      "loss": 1.4674,
      "step": 207
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 5.502582550048828,
      "learning_rate": 0.00030927374301675976,
      "loss": 1.5315,
      "step": 208
    },
    {
      "epoch": 0.23222222222222222,
      "grad_norm": 4.960947513580322,
      "learning_rate": 0.00030882681564245814,
      "loss": 1.7071,
      "step": 209
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 7.829428672790527,
      "learning_rate": 0.0003083798882681564,
      "loss": 1.4237,
      "step": 210
    },
    {
      "epoch": 0.23444444444444446,
      "grad_norm": 5.422718048095703,
      "learning_rate": 0.0003079329608938548,
      "loss": 1.3605,
      "step": 211
    },
    {
      "epoch": 0.23555555555555555,
      "grad_norm": 6.640440464019775,
      "learning_rate": 0.00030748603351955306,
      "loss": 2.8246,
      "step": 212
    },
    {
      "epoch": 0.23666666666666666,
      "grad_norm": 5.089559555053711,
      "learning_rate": 0.00030703910614525144,
      "loss": 2.2398,
      "step": 213
    },
    {
      "epoch": 0.23777777777777778,
      "grad_norm": 6.0749359130859375,
      "learning_rate": 0.0003065921787709497,
      "loss": 1.3765,
      "step": 214
    },
    {
      "epoch": 0.2388888888888889,
      "grad_norm": 5.18739652633667,
      "learning_rate": 0.0003061452513966481,
      "loss": 1.6717,
      "step": 215
    },
    {
      "epoch": 0.24,
      "grad_norm": 3.204556941986084,
      "learning_rate": 0.00030569832402234636,
      "loss": 0.8287,
      "step": 216
    },
    {
      "epoch": 0.2411111111111111,
      "grad_norm": 5.0780744552612305,
      "learning_rate": 0.00030525139664804474,
      "loss": 1.9013,
      "step": 217
    },
    {
      "epoch": 0.24222222222222223,
      "grad_norm": 6.265993118286133,
      "learning_rate": 0.000304804469273743,
      "loss": 2.7089,
      "step": 218
    },
    {
      "epoch": 0.24333333333333335,
      "grad_norm": 3.692878007888794,
      "learning_rate": 0.0003043575418994414,
      "loss": 1.457,
      "step": 219
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 3.316823959350586,
      "learning_rate": 0.00030391061452513966,
      "loss": 1.2022,
      "step": 220
    },
    {
      "epoch": 0.24555555555555555,
      "grad_norm": 3.5658514499664307,
      "learning_rate": 0.00030346368715083804,
      "loss": 1.5823,
      "step": 221
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 3.498311758041382,
      "learning_rate": 0.0003030167597765363,
      "loss": 1.1225,
      "step": 222
    },
    {
      "epoch": 0.2477777777777778,
      "grad_norm": 4.359038352966309,
      "learning_rate": 0.0003025698324022347,
      "loss": 1.9646,
      "step": 223
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 2.4604036808013916,
      "learning_rate": 0.00030212290502793296,
      "loss": 0.6206,
      "step": 224
    },
    {
      "epoch": 0.25,
      "grad_norm": 3.257032632827759,
      "learning_rate": 0.00030167597765363134,
      "loss": 1.0877,
      "step": 225
    },
    {
      "epoch": 0.2511111111111111,
      "grad_norm": 3.257833957672119,
      "learning_rate": 0.0003012290502793296,
      "loss": 1.2897,
      "step": 226
    },
    {
      "epoch": 0.25222222222222224,
      "grad_norm": 4.1108245849609375,
      "learning_rate": 0.000300782122905028,
      "loss": 1.7157,
      "step": 227
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 2.944929361343384,
      "learning_rate": 0.00030033519553072626,
      "loss": 1.1264,
      "step": 228
    },
    {
      "epoch": 0.2544444444444444,
      "grad_norm": 2.297238349914551,
      "learning_rate": 0.00029988826815642464,
      "loss": 0.6277,
      "step": 229
    },
    {
      "epoch": 0.25555555555555554,
      "grad_norm": 4.289222240447998,
      "learning_rate": 0.0002994413407821229,
      "loss": 1.9735,
      "step": 230
    },
    {
      "epoch": 0.25666666666666665,
      "grad_norm": 5.019431114196777,
      "learning_rate": 0.0002989944134078213,
      "loss": 1.8945,
      "step": 231
    },
    {
      "epoch": 0.2577777777777778,
      "grad_norm": 4.461295127868652,
      "learning_rate": 0.00029854748603351956,
      "loss": 1.4902,
      "step": 232
    },
    {
      "epoch": 0.2588888888888889,
      "grad_norm": 4.624236106872559,
      "learning_rate": 0.00029810055865921793,
      "loss": 1.5249,
      "step": 233
    },
    {
      "epoch": 0.26,
      "grad_norm": 5.118597984313965,
      "learning_rate": 0.0002976536312849162,
      "loss": 1.0502,
      "step": 234
    },
    {
      "epoch": 0.2611111111111111,
      "grad_norm": 3.682424306869507,
      "learning_rate": 0.0002972067039106146,
      "loss": 1.3622,
      "step": 235
    },
    {
      "epoch": 0.26222222222222225,
      "grad_norm": 3.5628015995025635,
      "learning_rate": 0.00029675977653631285,
      "loss": 1.0512,
      "step": 236
    },
    {
      "epoch": 0.2633333333333333,
      "grad_norm": 3.737548828125,
      "learning_rate": 0.00029631284916201123,
      "loss": 0.9042,
      "step": 237
    },
    {
      "epoch": 0.2644444444444444,
      "grad_norm": 4.798216819763184,
      "learning_rate": 0.0002958659217877095,
      "loss": 1.4463,
      "step": 238
    },
    {
      "epoch": 0.26555555555555554,
      "grad_norm": 2.688568353652954,
      "learning_rate": 0.0002954189944134079,
      "loss": 0.7716,
      "step": 239
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 3.6980979442596436,
      "learning_rate": 0.00029497206703910615,
      "loss": 1.1,
      "step": 240
    },
    {
      "epoch": 0.2677777777777778,
      "grad_norm": 6.552528381347656,
      "learning_rate": 0.0002945251396648045,
      "loss": 1.7298,
      "step": 241
    },
    {
      "epoch": 0.2688888888888889,
      "grad_norm": 2.8682961463928223,
      "learning_rate": 0.0002940782122905028,
      "loss": 1.073,
      "step": 242
    },
    {
      "epoch": 0.27,
      "grad_norm": 2.9253482818603516,
      "learning_rate": 0.00029363128491620113,
      "loss": 0.7358,
      "step": 243
    },
    {
      "epoch": 0.27111111111111114,
      "grad_norm": 4.100208282470703,
      "learning_rate": 0.00029318435754189945,
      "loss": 1.4155,
      "step": 244
    },
    {
      "epoch": 0.2722222222222222,
      "grad_norm": 4.404561996459961,
      "learning_rate": 0.0002927374301675978,
      "loss": 1.1268,
      "step": 245
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 3.710512638092041,
      "learning_rate": 0.0002922905027932961,
      "loss": 1.5473,
      "step": 246
    },
    {
      "epoch": 0.27444444444444444,
      "grad_norm": 3.2975802421569824,
      "learning_rate": 0.0002918435754189944,
      "loss": 1.1112,
      "step": 247
    },
    {
      "epoch": 0.27555555555555555,
      "grad_norm": 3.5534985065460205,
      "learning_rate": 0.00029139664804469275,
      "loss": 1.3255,
      "step": 248
    },
    {
      "epoch": 0.27666666666666667,
      "grad_norm": 3.4632229804992676,
      "learning_rate": 0.0002909497206703911,
      "loss": 0.8194,
      "step": 249
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 2.6548073291778564,
      "learning_rate": 0.0002905027932960894,
      "loss": 0.9331,
      "step": 250
    },
    {
      "epoch": 0.2788888888888889,
      "grad_norm": 3.539944648742676,
      "learning_rate": 0.0002900558659217877,
      "loss": 0.7713,
      "step": 251
    },
    {
      "epoch": 0.28,
      "grad_norm": 4.820894241333008,
      "learning_rate": 0.00028960893854748605,
      "loss": 2.3496,
      "step": 252
    },
    {
      "epoch": 0.2811111111111111,
      "grad_norm": 5.501596450805664,
      "learning_rate": 0.0002891620111731844,
      "loss": 1.5384,
      "step": 253
    },
    {
      "epoch": 0.2822222222222222,
      "grad_norm": 4.18580961227417,
      "learning_rate": 0.0002887150837988827,
      "loss": 1.6477,
      "step": 254
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 3.8226888179779053,
      "learning_rate": 0.000288268156424581,
      "loss": 1.3237,
      "step": 255
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 2.9067814350128174,
      "learning_rate": 0.00028782122905027935,
      "loss": 0.9607,
      "step": 256
    },
    {
      "epoch": 0.28555555555555556,
      "grad_norm": 2.9039981365203857,
      "learning_rate": 0.00028737430167597767,
      "loss": 1.1106,
      "step": 257
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 4.222710609436035,
      "learning_rate": 0.000286927374301676,
      "loss": 1.5485,
      "step": 258
    },
    {
      "epoch": 0.2877777777777778,
      "grad_norm": 3.719818353652954,
      "learning_rate": 0.0002864804469273743,
      "loss": 1.0776,
      "step": 259
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 3.7516114711761475,
      "learning_rate": 0.00028603351955307265,
      "loss": 1.1875,
      "step": 260
    },
    {
      "epoch": 0.29,
      "grad_norm": 5.092064380645752,
      "learning_rate": 0.00028558659217877097,
      "loss": 2.0134,
      "step": 261
    },
    {
      "epoch": 0.2911111111111111,
      "grad_norm": 4.8089118003845215,
      "learning_rate": 0.0002851396648044693,
      "loss": 0.94,
      "step": 262
    },
    {
      "epoch": 0.2922222222222222,
      "grad_norm": 4.480296611785889,
      "learning_rate": 0.0002846927374301676,
      "loss": 0.9116,
      "step": 263
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 4.037253379821777,
      "learning_rate": 0.00028424581005586595,
      "loss": 0.9978,
      "step": 264
    },
    {
      "epoch": 0.29444444444444445,
      "grad_norm": 4.599876403808594,
      "learning_rate": 0.00028379888268156427,
      "loss": 1.3938,
      "step": 265
    },
    {
      "epoch": 0.29555555555555557,
      "grad_norm": 5.718170166015625,
      "learning_rate": 0.0002833519553072626,
      "loss": 1.7598,
      "step": 266
    },
    {
      "epoch": 0.2966666666666667,
      "grad_norm": 6.650409698486328,
      "learning_rate": 0.0002829050279329609,
      "loss": 2.6467,
      "step": 267
    },
    {
      "epoch": 0.29777777777777775,
      "grad_norm": 4.298861026763916,
      "learning_rate": 0.00028245810055865924,
      "loss": 1.4325,
      "step": 268
    },
    {
      "epoch": 0.29888888888888887,
      "grad_norm": 3.275322437286377,
      "learning_rate": 0.00028201117318435757,
      "loss": 1.3493,
      "step": 269
    },
    {
      "epoch": 0.3,
      "grad_norm": 5.224730014801025,
      "learning_rate": 0.0002815642458100559,
      "loss": 1.9436,
      "step": 270
    },
    {
      "epoch": 0.3011111111111111,
      "grad_norm": 3.11421799659729,
      "learning_rate": 0.0002811173184357542,
      "loss": 0.702,
      "step": 271
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 2.2486352920532227,
      "learning_rate": 0.00028067039106145254,
      "loss": 0.726,
      "step": 272
    },
    {
      "epoch": 0.30333333333333334,
      "grad_norm": 2.179407835006714,
      "learning_rate": 0.00028022346368715087,
      "loss": 0.7757,
      "step": 273
    },
    {
      "epoch": 0.30444444444444446,
      "grad_norm": 3.9837093353271484,
      "learning_rate": 0.0002797765363128492,
      "loss": 1.7688,
      "step": 274
    },
    {
      "epoch": 0.3055555555555556,
      "grad_norm": 3.767000675201416,
      "learning_rate": 0.0002793296089385475,
      "loss": 1.5688,
      "step": 275
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 2.724942207336426,
      "learning_rate": 0.00027888268156424584,
      "loss": 0.8724,
      "step": 276
    },
    {
      "epoch": 0.30777777777777776,
      "grad_norm": 5.210695743560791,
      "learning_rate": 0.00027843575418994417,
      "loss": 1.6145,
      "step": 277
    },
    {
      "epoch": 0.3088888888888889,
      "grad_norm": 4.961248874664307,
      "learning_rate": 0.0002779888268156425,
      "loss": 1.5492,
      "step": 278
    },
    {
      "epoch": 0.31,
      "grad_norm": 4.732204437255859,
      "learning_rate": 0.0002775418994413408,
      "loss": 1.7267,
      "step": 279
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 7.363171577453613,
      "learning_rate": 0.00027709497206703914,
      "loss": 1.4974,
      "step": 280
    },
    {
      "epoch": 0.31222222222222223,
      "grad_norm": 3.7252190113067627,
      "learning_rate": 0.00027664804469273746,
      "loss": 1.0634,
      "step": 281
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 3.9955549240112305,
      "learning_rate": 0.0002762011173184358,
      "loss": 1.6069,
      "step": 282
    },
    {
      "epoch": 0.31444444444444447,
      "grad_norm": 5.626176834106445,
      "learning_rate": 0.0002757541899441341,
      "loss": 1.5824,
      "step": 283
    },
    {
      "epoch": 0.31555555555555553,
      "grad_norm": 2.6134145259857178,
      "learning_rate": 0.00027530726256983244,
      "loss": 0.6905,
      "step": 284
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 3.9546806812286377,
      "learning_rate": 0.00027486033519553076,
      "loss": 1.6899,
      "step": 285
    },
    {
      "epoch": 0.31777777777777777,
      "grad_norm": 4.451503753662109,
      "learning_rate": 0.0002744134078212291,
      "loss": 1.6183,
      "step": 286
    },
    {
      "epoch": 0.3188888888888889,
      "grad_norm": 3.971996307373047,
      "learning_rate": 0.0002739664804469274,
      "loss": 1.8411,
      "step": 287
    },
    {
      "epoch": 0.32,
      "grad_norm": 5.418331623077393,
      "learning_rate": 0.0002735195530726257,
      "loss": 2.0589,
      "step": 288
    },
    {
      "epoch": 0.3211111111111111,
      "grad_norm": 5.297604084014893,
      "learning_rate": 0.00027307262569832406,
      "loss": 2.0741,
      "step": 289
    },
    {
      "epoch": 0.32222222222222224,
      "grad_norm": 2.501091241836548,
      "learning_rate": 0.00027262569832402233,
      "loss": 0.9965,
      "step": 290
    },
    {
      "epoch": 0.3233333333333333,
      "grad_norm": 2.973466157913208,
      "learning_rate": 0.0002721787709497207,
      "loss": 1.0671,
      "step": 291
    },
    {
      "epoch": 0.3244444444444444,
      "grad_norm": 4.958658218383789,
      "learning_rate": 0.000271731843575419,
      "loss": 2.1835,
      "step": 292
    },
    {
      "epoch": 0.32555555555555554,
      "grad_norm": 3.101989507675171,
      "learning_rate": 0.00027128491620111736,
      "loss": 1.4378,
      "step": 293
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 3.0990958213806152,
      "learning_rate": 0.00027083798882681563,
      "loss": 1.0931,
      "step": 294
    },
    {
      "epoch": 0.3277777777777778,
      "grad_norm": 3.1660823822021484,
      "learning_rate": 0.00027039106145251396,
      "loss": 1.0701,
      "step": 295
    },
    {
      "epoch": 0.3288888888888889,
      "grad_norm": 3.4880926609039307,
      "learning_rate": 0.0002699441340782123,
      "loss": 0.9874,
      "step": 296
    },
    {
      "epoch": 0.33,
      "grad_norm": 5.140780448913574,
      "learning_rate": 0.0002694972067039106,
      "loss": 2.2204,
      "step": 297
    },
    {
      "epoch": 0.33111111111111113,
      "grad_norm": 3.39481258392334,
      "learning_rate": 0.00026905027932960893,
      "loss": 1.5994,
      "step": 298
    },
    {
      "epoch": 0.3322222222222222,
      "grad_norm": 3.7615461349487305,
      "learning_rate": 0.00026860335195530725,
      "loss": 1.0581,
      "step": 299
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 4.339396953582764,
      "learning_rate": 0.0002681564245810056,
      "loss": 2.1343,
      "step": 300
    },
    {
      "epoch": 0.33444444444444443,
      "grad_norm": 3.5225515365600586,
      "learning_rate": 0.0002677094972067039,
      "loss": 1.2731,
      "step": 301
    },
    {
      "epoch": 0.33555555555555555,
      "grad_norm": 5.636123180389404,
      "learning_rate": 0.00026726256983240223,
      "loss": 1.5017,
      "step": 302
    },
    {
      "epoch": 0.33666666666666667,
      "grad_norm": 5.2279839515686035,
      "learning_rate": 0.00026681564245810055,
      "loss": 1.6236,
      "step": 303
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 4.244686126708984,
      "learning_rate": 0.0002663687150837989,
      "loss": 1.2539,
      "step": 304
    },
    {
      "epoch": 0.3388888888888889,
      "grad_norm": 4.766215801239014,
      "learning_rate": 0.0002659217877094972,
      "loss": 1.6735,
      "step": 305
    },
    {
      "epoch": 0.34,
      "grad_norm": 3.277238130569458,
      "learning_rate": 0.00026547486033519553,
      "loss": 1.1155,
      "step": 306
    },
    {
      "epoch": 0.3411111111111111,
      "grad_norm": 3.2521724700927734,
      "learning_rate": 0.00026502793296089385,
      "loss": 0.9741,
      "step": 307
    },
    {
      "epoch": 0.3422222222222222,
      "grad_norm": 5.499513626098633,
      "learning_rate": 0.0002645810055865922,
      "loss": 1.9306,
      "step": 308
    },
    {
      "epoch": 0.3433333333333333,
      "grad_norm": 4.765658378601074,
      "learning_rate": 0.0002641340782122905,
      "loss": 1.0119,
      "step": 309
    },
    {
      "epoch": 0.34444444444444444,
      "grad_norm": 4.238681793212891,
      "learning_rate": 0.0002636871508379888,
      "loss": 1.4241,
      "step": 310
    },
    {
      "epoch": 0.34555555555555556,
      "grad_norm": 2.183424949645996,
      "learning_rate": 0.00026324022346368715,
      "loss": 0.9831,
      "step": 311
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 4.269412994384766,
      "learning_rate": 0.0002627932960893855,
      "loss": 1.3823,
      "step": 312
    },
    {
      "epoch": 0.3477777777777778,
      "grad_norm": 4.7104716300964355,
      "learning_rate": 0.0002623463687150838,
      "loss": 1.3506,
      "step": 313
    },
    {
      "epoch": 0.3488888888888889,
      "grad_norm": 2.7845394611358643,
      "learning_rate": 0.0002618994413407821,
      "loss": 1.0812,
      "step": 314
    },
    {
      "epoch": 0.35,
      "grad_norm": 2.694079637527466,
      "learning_rate": 0.00026145251396648045,
      "loss": 1.1276,
      "step": 315
    },
    {
      "epoch": 0.3511111111111111,
      "grad_norm": 3.8265771865844727,
      "learning_rate": 0.0002610055865921788,
      "loss": 1.0873,
      "step": 316
    },
    {
      "epoch": 0.3522222222222222,
      "grad_norm": 3.1917495727539062,
      "learning_rate": 0.0002605586592178771,
      "loss": 0.7214,
      "step": 317
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 4.508754730224609,
      "learning_rate": 0.0002601117318435754,
      "loss": 1.4874,
      "step": 318
    },
    {
      "epoch": 0.35444444444444445,
      "grad_norm": 3.953718423843384,
      "learning_rate": 0.00025966480446927375,
      "loss": 1.3512,
      "step": 319
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 7.773561000823975,
      "learning_rate": 0.00025921787709497207,
      "loss": 2.9486,
      "step": 320
    },
    {
      "epoch": 0.3566666666666667,
      "grad_norm": 3.5441782474517822,
      "learning_rate": 0.0002587709497206704,
      "loss": 1.2662,
      "step": 321
    },
    {
      "epoch": 0.35777777777777775,
      "grad_norm": 3.858428955078125,
      "learning_rate": 0.0002583240223463687,
      "loss": 1.0871,
      "step": 322
    },
    {
      "epoch": 0.35888888888888887,
      "grad_norm": 2.764256238937378,
      "learning_rate": 0.00025787709497206705,
      "loss": 1.2838,
      "step": 323
    },
    {
      "epoch": 0.36,
      "grad_norm": 7.686944484710693,
      "learning_rate": 0.00025743016759776537,
      "loss": 1.6991,
      "step": 324
    },
    {
      "epoch": 0.3611111111111111,
      "grad_norm": 3.6089789867401123,
      "learning_rate": 0.0002569832402234637,
      "loss": 1.1418,
      "step": 325
    },
    {
      "epoch": 0.3622222222222222,
      "grad_norm": 5.1786909103393555,
      "learning_rate": 0.000256536312849162,
      "loss": 1.433,
      "step": 326
    },
    {
      "epoch": 0.36333333333333334,
      "grad_norm": 5.815115928649902,
      "learning_rate": 0.00025608938547486035,
      "loss": 2.656,
      "step": 327
    },
    {
      "epoch": 0.36444444444444446,
      "grad_norm": 4.573875427246094,
      "learning_rate": 0.00025564245810055867,
      "loss": 1.6783,
      "step": 328
    },
    {
      "epoch": 0.3655555555555556,
      "grad_norm": 4.959033012390137,
      "learning_rate": 0.000255195530726257,
      "loss": 1.7048,
      "step": 329
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 3.583256721496582,
      "learning_rate": 0.0002547486033519553,
      "loss": 1.5035,
      "step": 330
    },
    {
      "epoch": 0.36777777777777776,
      "grad_norm": 5.3254570960998535,
      "learning_rate": 0.00025430167597765364,
      "loss": 2.7792,
      "step": 331
    },
    {
      "epoch": 0.3688888888888889,
      "grad_norm": 4.516152858734131,
      "learning_rate": 0.00025385474860335197,
      "loss": 1.6382,
      "step": 332
    },
    {
      "epoch": 0.37,
      "grad_norm": 2.7871267795562744,
      "learning_rate": 0.0002534078212290503,
      "loss": 1.0904,
      "step": 333
    },
    {
      "epoch": 0.3711111111111111,
      "grad_norm": 2.765316963195801,
      "learning_rate": 0.0002529608938547486,
      "loss": 1.0126,
      "step": 334
    },
    {
      "epoch": 0.37222222222222223,
      "grad_norm": 5.815384864807129,
      "learning_rate": 0.00025251396648044694,
      "loss": 1.9772,
      "step": 335
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 4.037995338439941,
      "learning_rate": 0.00025206703910614527,
      "loss": 0.8277,
      "step": 336
    },
    {
      "epoch": 0.37444444444444447,
      "grad_norm": 4.257155418395996,
      "learning_rate": 0.0002516201117318436,
      "loss": 1.41,
      "step": 337
    },
    {
      "epoch": 0.37555555555555553,
      "grad_norm": 4.317139148712158,
      "learning_rate": 0.0002511731843575419,
      "loss": 1.8481,
      "step": 338
    },
    {
      "epoch": 0.37666666666666665,
      "grad_norm": 3.2490549087524414,
      "learning_rate": 0.00025072625698324024,
      "loss": 1.1465,
      "step": 339
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 3.9486005306243896,
      "learning_rate": 0.00025027932960893857,
      "loss": 1.0977,
      "step": 340
    },
    {
      "epoch": 0.3788888888888889,
      "grad_norm": 4.290426254272461,
      "learning_rate": 0.0002498324022346369,
      "loss": 2.0531,
      "step": 341
    },
    {
      "epoch": 0.38,
      "grad_norm": 2.651726484298706,
      "learning_rate": 0.0002493854748603352,
      "loss": 0.9013,
      "step": 342
    },
    {
      "epoch": 0.3811111111111111,
      "grad_norm": 3.706700325012207,
      "learning_rate": 0.00024893854748603354,
      "loss": 1.7639,
      "step": 343
    },
    {
      "epoch": 0.38222222222222224,
      "grad_norm": 3.7018954753875732,
      "learning_rate": 0.00024849162011173186,
      "loss": 1.4298,
      "step": 344
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 3.0992236137390137,
      "learning_rate": 0.0002480446927374302,
      "loss": 1.5955,
      "step": 345
    },
    {
      "epoch": 0.3844444444444444,
      "grad_norm": 2.9273619651794434,
      "learning_rate": 0.0002475977653631285,
      "loss": 1.2167,
      "step": 346
    },
    {
      "epoch": 0.38555555555555554,
      "grad_norm": 3.0620598793029785,
      "learning_rate": 0.0002471508379888268,
      "loss": 1.0817,
      "step": 347
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 4.104405403137207,
      "learning_rate": 0.00024670391061452516,
      "loss": 2.0067,
      "step": 348
    },
    {
      "epoch": 0.3877777777777778,
      "grad_norm": 3.211301803588867,
      "learning_rate": 0.00024625698324022343,
      "loss": 0.8555,
      "step": 349
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 5.688633441925049,
      "learning_rate": 0.0002458100558659218,
      "loss": 2.0638,
      "step": 350
    },
    {
      "epoch": 0.39,
      "grad_norm": 4.44825553894043,
      "learning_rate": 0.0002453631284916201,
      "loss": 1.667,
      "step": 351
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 5.389860153198242,
      "learning_rate": 0.00024491620111731846,
      "loss": 1.8436,
      "step": 352
    },
    {
      "epoch": 0.39222222222222225,
      "grad_norm": 2.5672507286071777,
      "learning_rate": 0.00024446927374301673,
      "loss": 0.9581,
      "step": 353
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 4.035637378692627,
      "learning_rate": 0.0002440223463687151,
      "loss": 1.8344,
      "step": 354
    },
    {
      "epoch": 0.39444444444444443,
      "grad_norm": 3.134899377822876,
      "learning_rate": 0.0002435754189944134,
      "loss": 0.6417,
      "step": 355
    },
    {
      "epoch": 0.39555555555555555,
      "grad_norm": 5.600834846496582,
      "learning_rate": 0.00024312849162011176,
      "loss": 2.1927,
      "step": 356
    },
    {
      "epoch": 0.39666666666666667,
      "grad_norm": 4.222681999206543,
      "learning_rate": 0.00024268156424581006,
      "loss": 1.4164,
      "step": 357
    },
    {
      "epoch": 0.3977777777777778,
      "grad_norm": 3.677726984024048,
      "learning_rate": 0.0002422346368715084,
      "loss": 1.1386,
      "step": 358
    },
    {
      "epoch": 0.3988888888888889,
      "grad_norm": 3.3205485343933105,
      "learning_rate": 0.0002417877094972067,
      "loss": 0.5964,
      "step": 359
    },
    {
      "epoch": 0.4,
      "grad_norm": 4.06650972366333,
      "learning_rate": 0.00024134078212290506,
      "loss": 0.4912,
      "step": 360
    },
    {
      "epoch": 0.4011111111111111,
      "grad_norm": 3.8747737407684326,
      "learning_rate": 0.00024089385474860336,
      "loss": 1.2632,
      "step": 361
    },
    {
      "epoch": 0.4022222222222222,
      "grad_norm": 5.244985103607178,
      "learning_rate": 0.0002404469273743017,
      "loss": 1.5708,
      "step": 362
    },
    {
      "epoch": 0.4033333333333333,
      "grad_norm": 4.316475868225098,
      "learning_rate": 0.00024,
      "loss": 0.9918,
      "step": 363
    },
    {
      "epoch": 0.40444444444444444,
      "grad_norm": 5.299986839294434,
      "learning_rate": 0.00023955307262569836,
      "loss": 0.9104,
      "step": 364
    },
    {
      "epoch": 0.40555555555555556,
      "grad_norm": 4.02025842666626,
      "learning_rate": 0.00023910614525139666,
      "loss": 1.342,
      "step": 365
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 3.0060739517211914,
      "learning_rate": 0.000238659217877095,
      "loss": 0.7683,
      "step": 366
    },
    {
      "epoch": 0.4077777777777778,
      "grad_norm": 4.093972682952881,
      "learning_rate": 0.0002382122905027933,
      "loss": 2.0523,
      "step": 367
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 5.229570388793945,
      "learning_rate": 0.00023776536312849166,
      "loss": 1.3498,
      "step": 368
    },
    {
      "epoch": 0.41,
      "grad_norm": 3.0701866149902344,
      "learning_rate": 0.00023731843575418995,
      "loss": 1.4716,
      "step": 369
    },
    {
      "epoch": 0.4111111111111111,
      "grad_norm": 24.903728485107422,
      "learning_rate": 0.0002368715083798883,
      "loss": 1.3536,
      "step": 370
    },
    {
      "epoch": 0.4122222222222222,
      "grad_norm": 5.022215843200684,
      "learning_rate": 0.0002364245810055866,
      "loss": 1.882,
      "step": 371
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 5.043704986572266,
      "learning_rate": 0.00023597765363128496,
      "loss": 1.2746,
      "step": 372
    },
    {
      "epoch": 0.41444444444444445,
      "grad_norm": 5.2274627685546875,
      "learning_rate": 0.00023553072625698325,
      "loss": 1.4844,
      "step": 373
    },
    {
      "epoch": 0.41555555555555557,
      "grad_norm": 3.323580741882324,
      "learning_rate": 0.0002350837988826816,
      "loss": 1.0259,
      "step": 374
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 5.823974132537842,
      "learning_rate": 0.0002346368715083799,
      "loss": 2.3333,
      "step": 375
    },
    {
      "epoch": 0.4177777777777778,
      "grad_norm": 3.5501229763031006,
      "learning_rate": 0.00023418994413407825,
      "loss": 1.9477,
      "step": 376
    },
    {
      "epoch": 0.41888888888888887,
      "grad_norm": 3.4350271224975586,
      "learning_rate": 0.00023374301675977655,
      "loss": 1.0048,
      "step": 377
    },
    {
      "epoch": 0.42,
      "grad_norm": 3.4928457736968994,
      "learning_rate": 0.0002332960893854749,
      "loss": 1.4754,
      "step": 378
    },
    {
      "epoch": 0.4211111111111111,
      "grad_norm": 5.320231914520264,
      "learning_rate": 0.0002328491620111732,
      "loss": 1.7021,
      "step": 379
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 4.2802205085754395,
      "learning_rate": 0.00023240223463687155,
      "loss": 1.3355,
      "step": 380
    },
    {
      "epoch": 0.42333333333333334,
      "grad_norm": 2.7161571979522705,
      "learning_rate": 0.00023195530726256985,
      "loss": 0.5097,
      "step": 381
    },
    {
      "epoch": 0.42444444444444446,
      "grad_norm": 4.202108860015869,
      "learning_rate": 0.00023150837988826815,
      "loss": 1.2206,
      "step": 382
    },
    {
      "epoch": 0.4255555555555556,
      "grad_norm": 2.7505011558532715,
      "learning_rate": 0.0002310614525139665,
      "loss": 0.3949,
      "step": 383
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 3.5091168880462646,
      "learning_rate": 0.0002306145251396648,
      "loss": 1.5015,
      "step": 384
    },
    {
      "epoch": 0.42777777777777776,
      "grad_norm": 4.343809127807617,
      "learning_rate": 0.00023016759776536315,
      "loss": 1.961,
      "step": 385
    },
    {
      "epoch": 0.4288888888888889,
      "grad_norm": 4.721001625061035,
      "learning_rate": 0.00022972067039106145,
      "loss": 1.7527,
      "step": 386
    },
    {
      "epoch": 0.43,
      "grad_norm": 3.098681688308716,
      "learning_rate": 0.0002292737430167598,
      "loss": 1.0442,
      "step": 387
    },
    {
      "epoch": 0.4311111111111111,
      "grad_norm": 4.581486701965332,
      "learning_rate": 0.0002288268156424581,
      "loss": 1.8332,
      "step": 388
    },
    {
      "epoch": 0.43222222222222223,
      "grad_norm": 2.6849911212921143,
      "learning_rate": 0.00022837988826815645,
      "loss": 1.018,
      "step": 389
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 4.41014289855957,
      "learning_rate": 0.00022793296089385474,
      "loss": 1.1824,
      "step": 390
    },
    {
      "epoch": 0.43444444444444447,
      "grad_norm": 5.764942646026611,
      "learning_rate": 0.0002274860335195531,
      "loss": 1.9798,
      "step": 391
    },
    {
      "epoch": 0.43555555555555553,
      "grad_norm": 2.647563934326172,
      "learning_rate": 0.0002270391061452514,
      "loss": 1.2728,
      "step": 392
    },
    {
      "epoch": 0.43666666666666665,
      "grad_norm": 4.924495220184326,
      "learning_rate": 0.00022659217877094975,
      "loss": 1.4105,
      "step": 393
    },
    {
      "epoch": 0.43777777777777777,
      "grad_norm": 5.0026726722717285,
      "learning_rate": 0.00022614525139664804,
      "loss": 1.6054,
      "step": 394
    },
    {
      "epoch": 0.4388888888888889,
      "grad_norm": 4.18698263168335,
      "learning_rate": 0.0002256983240223464,
      "loss": 1.8443,
      "step": 395
    },
    {
      "epoch": 0.44,
      "grad_norm": 2.1058590412139893,
      "learning_rate": 0.0002252513966480447,
      "loss": 0.7269,
      "step": 396
    },
    {
      "epoch": 0.4411111111111111,
      "grad_norm": 2.357701063156128,
      "learning_rate": 0.00022480446927374304,
      "loss": 0.923,
      "step": 397
    },
    {
      "epoch": 0.44222222222222224,
      "grad_norm": 5.768906593322754,
      "learning_rate": 0.00022435754189944134,
      "loss": 1.6707,
      "step": 398
    },
    {
      "epoch": 0.44333333333333336,
      "grad_norm": 3.447263717651367,
      "learning_rate": 0.0002239106145251397,
      "loss": 1.0863,
      "step": 399
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 3.681950807571411,
      "learning_rate": 0.000223463687150838,
      "loss": 0.9935,
      "step": 400
    },
    {
      "epoch": 0.44555555555555554,
      "grad_norm": 4.183786392211914,
      "learning_rate": 0.00022301675977653634,
      "loss": 1.7413,
      "step": 401
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 7.364016532897949,
      "learning_rate": 0.00022256983240223464,
      "loss": 2.77,
      "step": 402
    },
    {
      "epoch": 0.4477777777777778,
      "grad_norm": 4.441112995147705,
      "learning_rate": 0.000222122905027933,
      "loss": 1.5199,
      "step": 403
    },
    {
      "epoch": 0.4488888888888889,
      "grad_norm": 5.167997360229492,
      "learning_rate": 0.0002216759776536313,
      "loss": 1.5024,
      "step": 404
    },
    {
      "epoch": 0.45,
      "grad_norm": 3.9780361652374268,
      "learning_rate": 0.00022122905027932964,
      "loss": 1.4496,
      "step": 405
    },
    {
      "epoch": 0.45111111111111113,
      "grad_norm": 2.282318353652954,
      "learning_rate": 0.00022078212290502794,
      "loss": 0.6367,
      "step": 406
    },
    {
      "epoch": 0.45222222222222225,
      "grad_norm": 2.8490700721740723,
      "learning_rate": 0.0002203351955307263,
      "loss": 0.6035,
      "step": 407
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 4.088818073272705,
      "learning_rate": 0.0002198882681564246,
      "loss": 1.4562,
      "step": 408
    },
    {
      "epoch": 0.45444444444444443,
      "grad_norm": 4.249924182891846,
      "learning_rate": 0.00021944134078212294,
      "loss": 1.5671,
      "step": 409
    },
    {
      "epoch": 0.45555555555555555,
      "grad_norm": 5.570911884307861,
      "learning_rate": 0.00021899441340782124,
      "loss": 2.0864,
      "step": 410
    },
    {
      "epoch": 0.45666666666666667,
      "grad_norm": 4.317685127258301,
      "learning_rate": 0.0002185474860335196,
      "loss": 1.6884,
      "step": 411
    },
    {
      "epoch": 0.4577777777777778,
      "grad_norm": 4.933940410614014,
      "learning_rate": 0.0002181005586592179,
      "loss": 1.327,
      "step": 412
    },
    {
      "epoch": 0.4588888888888889,
      "grad_norm": 5.66865873336792,
      "learning_rate": 0.00021765363128491624,
      "loss": 2.0775,
      "step": 413
    },
    {
      "epoch": 0.46,
      "grad_norm": 5.0407328605651855,
      "learning_rate": 0.00021720670391061454,
      "loss": 1.6489,
      "step": 414
    },
    {
      "epoch": 0.46111111111111114,
      "grad_norm": 2.7686054706573486,
      "learning_rate": 0.00021675977653631286,
      "loss": 1.5427,
      "step": 415
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 3.869060754776001,
      "learning_rate": 0.00021631284916201119,
      "loss": 0.6595,
      "step": 416
    },
    {
      "epoch": 0.4633333333333333,
      "grad_norm": 4.602139949798584,
      "learning_rate": 0.0002158659217877095,
      "loss": 1.2205,
      "step": 417
    },
    {
      "epoch": 0.46444444444444444,
      "grad_norm": 3.395735263824463,
      "learning_rate": 0.00021541899441340784,
      "loss": 1.357,
      "step": 418
    },
    {
      "epoch": 0.46555555555555556,
      "grad_norm": 2.8067829608917236,
      "learning_rate": 0.00021497206703910616,
      "loss": 0.702,
      "step": 419
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 2.9757001399993896,
      "learning_rate": 0.00021452513966480449,
      "loss": 1.2838,
      "step": 420
    },
    {
      "epoch": 0.4677777777777778,
      "grad_norm": 4.8482160568237305,
      "learning_rate": 0.0002140782122905028,
      "loss": 2.4109,
      "step": 421
    },
    {
      "epoch": 0.4688888888888889,
      "grad_norm": 4.359803199768066,
      "learning_rate": 0.00021363128491620113,
      "loss": 1.4136,
      "step": 422
    },
    {
      "epoch": 0.47,
      "grad_norm": 4.7006916999816895,
      "learning_rate": 0.00021318435754189946,
      "loss": 1.6001,
      "step": 423
    },
    {
      "epoch": 0.4711111111111111,
      "grad_norm": 4.198505401611328,
      "learning_rate": 0.00021273743016759778,
      "loss": 1.2777,
      "step": 424
    },
    {
      "epoch": 0.4722222222222222,
      "grad_norm": 3.0608651638031006,
      "learning_rate": 0.0002122905027932961,
      "loss": 1.3106,
      "step": 425
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 2.697890043258667,
      "learning_rate": 0.00021184357541899443,
      "loss": 0.6956,
      "step": 426
    },
    {
      "epoch": 0.47444444444444445,
      "grad_norm": 3.2964890003204346,
      "learning_rate": 0.00021139664804469276,
      "loss": 1.2216,
      "step": 427
    },
    {
      "epoch": 0.47555555555555556,
      "grad_norm": 3.2206335067749023,
      "learning_rate": 0.00021094972067039108,
      "loss": 1.0496,
      "step": 428
    },
    {
      "epoch": 0.4766666666666667,
      "grad_norm": 3.977961778640747,
      "learning_rate": 0.00021050279329608938,
      "loss": 1.8037,
      "step": 429
    },
    {
      "epoch": 0.4777777777777778,
      "grad_norm": 4.681517124176025,
      "learning_rate": 0.00021005586592178773,
      "loss": 2.1844,
      "step": 430
    },
    {
      "epoch": 0.47888888888888886,
      "grad_norm": 3.0056800842285156,
      "learning_rate": 0.00020960893854748603,
      "loss": 1.2205,
      "step": 431
    },
    {
      "epoch": 0.48,
      "grad_norm": 2.7606701850891113,
      "learning_rate": 0.00020916201117318438,
      "loss": 0.9824,
      "step": 432
    },
    {
      "epoch": 0.4811111111111111,
      "grad_norm": 1.9861876964569092,
      "learning_rate": 0.00020871508379888268,
      "loss": 0.813,
      "step": 433
    },
    {
      "epoch": 0.4822222222222222,
      "grad_norm": 2.0781943798065186,
      "learning_rate": 0.00020826815642458103,
      "loss": 0.8184,
      "step": 434
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 3.9334537982940674,
      "learning_rate": 0.00020782122905027933,
      "loss": 1.2388,
      "step": 435
    },
    {
      "epoch": 0.48444444444444446,
      "grad_norm": 3.2070600986480713,
      "learning_rate": 0.00020737430167597768,
      "loss": 1.0093,
      "step": 436
    },
    {
      "epoch": 0.4855555555555556,
      "grad_norm": 4.296708106994629,
      "learning_rate": 0.00020692737430167598,
      "loss": 1.4994,
      "step": 437
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 4.458811283111572,
      "learning_rate": 0.00020648044692737433,
      "loss": 1.4638,
      "step": 438
    },
    {
      "epoch": 0.48777777777777775,
      "grad_norm": 4.322737693786621,
      "learning_rate": 0.00020603351955307263,
      "loss": 1.3666,
      "step": 439
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 2.6936135292053223,
      "learning_rate": 0.00020558659217877095,
      "loss": 0.9173,
      "step": 440
    },
    {
      "epoch": 0.49,
      "grad_norm": 4.8678693771362305,
      "learning_rate": 0.00020513966480446928,
      "loss": 1.3929,
      "step": 441
    },
    {
      "epoch": 0.4911111111111111,
      "grad_norm": 4.808879375457764,
      "learning_rate": 0.0002046927374301676,
      "loss": 1.5134,
      "step": 442
    },
    {
      "epoch": 0.4922222222222222,
      "grad_norm": 4.339828014373779,
      "learning_rate": 0.00020424581005586593,
      "loss": 1.883,
      "step": 443
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 4.703568458557129,
      "learning_rate": 0.00020379888268156425,
      "loss": 1.6473,
      "step": 444
    },
    {
      "epoch": 0.49444444444444446,
      "grad_norm": 2.954569101333618,
      "learning_rate": 0.00020335195530726257,
      "loss": 0.8568,
      "step": 445
    },
    {
      "epoch": 0.4955555555555556,
      "grad_norm": 5.078303337097168,
      "learning_rate": 0.0002029050279329609,
      "loss": 1.7964,
      "step": 446
    },
    {
      "epoch": 0.49666666666666665,
      "grad_norm": 3.6193039417266846,
      "learning_rate": 0.00020245810055865922,
      "loss": 1.0565,
      "step": 447
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 3.948303461074829,
      "learning_rate": 0.00020201117318435755,
      "loss": 1.354,
      "step": 448
    },
    {
      "epoch": 0.4988888888888889,
      "grad_norm": 4.539860725402832,
      "learning_rate": 0.00020156424581005587,
      "loss": 1.6963,
      "step": 449
    },
    {
      "epoch": 0.5,
      "grad_norm": 5.705753803253174,
      "learning_rate": 0.0002011173184357542,
      "loss": 2.1576,
      "step": 450
    },
    {
      "epoch": 0.5011111111111111,
      "grad_norm": 4.767503261566162,
      "learning_rate": 0.00020067039106145252,
      "loss": 2.1675,
      "step": 451
    },
    {
      "epoch": 0.5022222222222222,
      "grad_norm": 3.004849672317505,
      "learning_rate": 0.00020022346368715085,
      "loss": 0.9114,
      "step": 452
    },
    {
      "epoch": 0.5033333333333333,
      "grad_norm": 4.95789098739624,
      "learning_rate": 0.00019977653631284917,
      "loss": 1.9378,
      "step": 453
    },
    {
      "epoch": 0.5044444444444445,
      "grad_norm": 4.459721565246582,
      "learning_rate": 0.0001993296089385475,
      "loss": 2.2648,
      "step": 454
    },
    {
      "epoch": 0.5055555555555555,
      "grad_norm": 4.7371697425842285,
      "learning_rate": 0.00019888268156424582,
      "loss": 1.3968,
      "step": 455
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 2.693242073059082,
      "learning_rate": 0.00019843575418994415,
      "loss": 0.8249,
      "step": 456
    },
    {
      "epoch": 0.5077777777777778,
      "grad_norm": 3.4749176502227783,
      "learning_rate": 0.00019798882681564247,
      "loss": 1.444,
      "step": 457
    },
    {
      "epoch": 0.5088888888888888,
      "grad_norm": 2.8350462913513184,
      "learning_rate": 0.0001975418994413408,
      "loss": 0.8092,
      "step": 458
    },
    {
      "epoch": 0.51,
      "grad_norm": 3.9661762714385986,
      "learning_rate": 0.00019709497206703912,
      "loss": 0.9937,
      "step": 459
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 2.3005857467651367,
      "learning_rate": 0.00019664804469273744,
      "loss": 0.663,
      "step": 460
    },
    {
      "epoch": 0.5122222222222222,
      "grad_norm": 3.1407082080841064,
      "learning_rate": 0.00019620111731843577,
      "loss": 1.1546,
      "step": 461
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 5.122157573699951,
      "learning_rate": 0.0001957541899441341,
      "loss": 2.2628,
      "step": 462
    },
    {
      "epoch": 0.5144444444444445,
      "grad_norm": 3.4414708614349365,
      "learning_rate": 0.00019530726256983242,
      "loss": 1.8273,
      "step": 463
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 6.664146900177002,
      "learning_rate": 0.00019486033519553074,
      "loss": 2.7477,
      "step": 464
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 3.7917160987854004,
      "learning_rate": 0.00019441340782122907,
      "loss": 2.0282,
      "step": 465
    },
    {
      "epoch": 0.5177777777777778,
      "grad_norm": 4.514062881469727,
      "learning_rate": 0.00019396648044692737,
      "loss": 1.8316,
      "step": 466
    },
    {
      "epoch": 0.5188888888888888,
      "grad_norm": 2.7819743156433105,
      "learning_rate": 0.0001935195530726257,
      "loss": 1.3322,
      "step": 467
    },
    {
      "epoch": 0.52,
      "grad_norm": 4.593914031982422,
      "learning_rate": 0.00019307262569832401,
      "loss": 1.5103,
      "step": 468
    },
    {
      "epoch": 0.5211111111111111,
      "grad_norm": 3.02433180809021,
      "learning_rate": 0.00019262569832402234,
      "loss": 1.05,
      "step": 469
    },
    {
      "epoch": 0.5222222222222223,
      "grad_norm": 2.784816026687622,
      "learning_rate": 0.00019217877094972066,
      "loss": 1.3833,
      "step": 470
    },
    {
      "epoch": 0.5233333333333333,
      "grad_norm": 4.944230556488037,
      "learning_rate": 0.000191731843575419,
      "loss": 2.0902,
      "step": 471
    },
    {
      "epoch": 0.5244444444444445,
      "grad_norm": 3.007924795150757,
      "learning_rate": 0.0001912849162011173,
      "loss": 1.1125,
      "step": 472
    },
    {
      "epoch": 0.5255555555555556,
      "grad_norm": 3.7147419452667236,
      "learning_rate": 0.00019083798882681564,
      "loss": 1.3107,
      "step": 473
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 2.690852165222168,
      "learning_rate": 0.00019039106145251396,
      "loss": 0.9633,
      "step": 474
    },
    {
      "epoch": 0.5277777777777778,
      "grad_norm": 4.949435234069824,
      "learning_rate": 0.0001899441340782123,
      "loss": 2.6751,
      "step": 475
    },
    {
      "epoch": 0.5288888888888889,
      "grad_norm": 4.042517185211182,
      "learning_rate": 0.0001894972067039106,
      "loss": 1.1268,
      "step": 476
    },
    {
      "epoch": 0.53,
      "grad_norm": 3.5580763816833496,
      "learning_rate": 0.00018905027932960894,
      "loss": 1.8548,
      "step": 477
    },
    {
      "epoch": 0.5311111111111111,
      "grad_norm": 4.383810043334961,
      "learning_rate": 0.00018860335195530726,
      "loss": 1.5928,
      "step": 478
    },
    {
      "epoch": 0.5322222222222223,
      "grad_norm": 3.1721270084381104,
      "learning_rate": 0.00018815642458100559,
      "loss": 0.9764,
      "step": 479
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 2.1257894039154053,
      "learning_rate": 0.0001877094972067039,
      "loss": 0.565,
      "step": 480
    },
    {
      "epoch": 0.5344444444444445,
      "grad_norm": 3.880401372909546,
      "learning_rate": 0.00018726256983240224,
      "loss": 1.7433,
      "step": 481
    },
    {
      "epoch": 0.5355555555555556,
      "grad_norm": 3.6695353984832764,
      "learning_rate": 0.00018681564245810056,
      "loss": 1.5471,
      "step": 482
    },
    {
      "epoch": 0.5366666666666666,
      "grad_norm": 4.938419818878174,
      "learning_rate": 0.00018636871508379888,
      "loss": 1.9732,
      "step": 483
    },
    {
      "epoch": 0.5377777777777778,
      "grad_norm": 3.4348678588867188,
      "learning_rate": 0.0001859217877094972,
      "loss": 1.503,
      "step": 484
    },
    {
      "epoch": 0.5388888888888889,
      "grad_norm": 4.549357891082764,
      "learning_rate": 0.00018547486033519553,
      "loss": 1.4295,
      "step": 485
    },
    {
      "epoch": 0.54,
      "grad_norm": 3.0747296810150146,
      "learning_rate": 0.00018502793296089386,
      "loss": 1.2174,
      "step": 486
    },
    {
      "epoch": 0.5411111111111111,
      "grad_norm": 3.880610704421997,
      "learning_rate": 0.00018458100558659218,
      "loss": 2.057,
      "step": 487
    },
    {
      "epoch": 0.5422222222222223,
      "grad_norm": 2.1783289909362793,
      "learning_rate": 0.0001841340782122905,
      "loss": 0.9771,
      "step": 488
    },
    {
      "epoch": 0.5433333333333333,
      "grad_norm": 3.9291741847991943,
      "learning_rate": 0.00018368715083798883,
      "loss": 1.5261,
      "step": 489
    },
    {
      "epoch": 0.5444444444444444,
      "grad_norm": 2.578691244125366,
      "learning_rate": 0.00018324022346368716,
      "loss": 0.85,
      "step": 490
    },
    {
      "epoch": 0.5455555555555556,
      "grad_norm": 3.421980142593384,
      "learning_rate": 0.00018279329608938548,
      "loss": 1.3097,
      "step": 491
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 2.0963633060455322,
      "learning_rate": 0.0001823463687150838,
      "loss": 0.8862,
      "step": 492
    },
    {
      "epoch": 0.5477777777777778,
      "grad_norm": 3.827646255493164,
      "learning_rate": 0.00018189944134078213,
      "loss": 1.1571,
      "step": 493
    },
    {
      "epoch": 0.5488888888888889,
      "grad_norm": 3.9395670890808105,
      "learning_rate": 0.00018145251396648046,
      "loss": 1.1795,
      "step": 494
    },
    {
      "epoch": 0.55,
      "grad_norm": 3.6662495136260986,
      "learning_rate": 0.00018100558659217878,
      "loss": 1.7414,
      "step": 495
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 3.480846643447876,
      "learning_rate": 0.0001805586592178771,
      "loss": 1.1505,
      "step": 496
    },
    {
      "epoch": 0.5522222222222222,
      "grad_norm": 3.076448678970337,
      "learning_rate": 0.00018011173184357543,
      "loss": 1.1081,
      "step": 497
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 3.1306400299072266,
      "learning_rate": 0.00017966480446927375,
      "loss": 0.9837,
      "step": 498
    },
    {
      "epoch": 0.5544444444444444,
      "grad_norm": 3.1836581230163574,
      "learning_rate": 0.00017921787709497208,
      "loss": 1.195,
      "step": 499
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 3.616222381591797,
      "learning_rate": 0.00017877094972067038,
      "loss": 1.153,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 652444180439040.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
