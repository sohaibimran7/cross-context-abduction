{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 900,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0011111111111111111,
      "grad_norm": 7.653295516967773,
      "learning_rate": 8e-05,
      "loss": 5.0982,
      "step": 1
    },
    {
      "epoch": 0.0022222222222222222,
      "grad_norm": 5.0417866706848145,
      "learning_rate": 0.00016,
      "loss": 3.9073,
      "step": 2
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 8.288346290588379,
      "learning_rate": 0.00024,
      "loss": 5.1887,
      "step": 3
    },
    {
      "epoch": 0.0044444444444444444,
      "grad_norm": 6.14718770980835,
      "learning_rate": 0.00032,
      "loss": 2.923,
      "step": 4
    },
    {
      "epoch": 0.005555555555555556,
      "grad_norm": 3.893019676208496,
      "learning_rate": 0.0004,
      "loss": 2.9679,
      "step": 5
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 7.1250834465026855,
      "learning_rate": 0.00039955307262569834,
      "loss": 3.2274,
      "step": 6
    },
    {
      "epoch": 0.0077777777777777776,
      "grad_norm": 5.4239325523376465,
      "learning_rate": 0.00039910614525139667,
      "loss": 3.3347,
      "step": 7
    },
    {
      "epoch": 0.008888888888888889,
      "grad_norm": 6.196037292480469,
      "learning_rate": 0.000398659217877095,
      "loss": 4.6541,
      "step": 8
    },
    {
      "epoch": 0.01,
      "grad_norm": 5.588382720947266,
      "learning_rate": 0.0003982122905027933,
      "loss": 3.0984,
      "step": 9
    },
    {
      "epoch": 0.011111111111111112,
      "grad_norm": 5.992269515991211,
      "learning_rate": 0.00039776536312849164,
      "loss": 3.1555,
      "step": 10
    },
    {
      "epoch": 0.012222222222222223,
      "grad_norm": 4.053394794464111,
      "learning_rate": 0.00039731843575418997,
      "loss": 1.6813,
      "step": 11
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 4.8892364501953125,
      "learning_rate": 0.0003968715083798883,
      "loss": 1.9223,
      "step": 12
    },
    {
      "epoch": 0.014444444444444444,
      "grad_norm": 5.570788860321045,
      "learning_rate": 0.0003964245810055866,
      "loss": 1.735,
      "step": 13
    },
    {
      "epoch": 0.015555555555555555,
      "grad_norm": 6.869635581970215,
      "learning_rate": 0.00039597765363128494,
      "loss": 2.8064,
      "step": 14
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 4.9903154373168945,
      "learning_rate": 0.00039553072625698327,
      "loss": 2.1074,
      "step": 15
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 6.6125006675720215,
      "learning_rate": 0.0003950837988826816,
      "loss": 2.8687,
      "step": 16
    },
    {
      "epoch": 0.01888888888888889,
      "grad_norm": 4.97861909866333,
      "learning_rate": 0.0003946368715083799,
      "loss": 1.5191,
      "step": 17
    },
    {
      "epoch": 0.02,
      "grad_norm": 7.178215980529785,
      "learning_rate": 0.00039418994413407824,
      "loss": 2.3319,
      "step": 18
    },
    {
      "epoch": 0.021111111111111112,
      "grad_norm": 5.9507951736450195,
      "learning_rate": 0.00039374301675977656,
      "loss": 1.8204,
      "step": 19
    },
    {
      "epoch": 0.022222222222222223,
      "grad_norm": 5.243410587310791,
      "learning_rate": 0.0003932960893854749,
      "loss": 1.6449,
      "step": 20
    },
    {
      "epoch": 0.023333333333333334,
      "grad_norm": 8.491789817810059,
      "learning_rate": 0.0003928491620111732,
      "loss": 2.685,
      "step": 21
    },
    {
      "epoch": 0.024444444444444446,
      "grad_norm": 8.379876136779785,
      "learning_rate": 0.00039240223463687154,
      "loss": 2.0865,
      "step": 22
    },
    {
      "epoch": 0.025555555555555557,
      "grad_norm": 6.163096904754639,
      "learning_rate": 0.00039195530726256986,
      "loss": 1.9818,
      "step": 23
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 3.7657179832458496,
      "learning_rate": 0.0003915083798882682,
      "loss": 1.3261,
      "step": 24
    },
    {
      "epoch": 0.027777777777777776,
      "grad_norm": 6.316224575042725,
      "learning_rate": 0.0003910614525139665,
      "loss": 1.6561,
      "step": 25
    },
    {
      "epoch": 0.028888888888888888,
      "grad_norm": 4.565903186798096,
      "learning_rate": 0.00039061452513966484,
      "loss": 2.4087,
      "step": 26
    },
    {
      "epoch": 0.03,
      "grad_norm": 4.992722034454346,
      "learning_rate": 0.00039016759776536316,
      "loss": 1.3619,
      "step": 27
    },
    {
      "epoch": 0.03111111111111111,
      "grad_norm": 4.053192138671875,
      "learning_rate": 0.0003897206703910615,
      "loss": 1.6826,
      "step": 28
    },
    {
      "epoch": 0.03222222222222222,
      "grad_norm": 4.369140625,
      "learning_rate": 0.0003892737430167598,
      "loss": 1.1565,
      "step": 29
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 6.414087295532227,
      "learning_rate": 0.00038882681564245814,
      "loss": 2.751,
      "step": 30
    },
    {
      "epoch": 0.034444444444444444,
      "grad_norm": 4.450449466705322,
      "learning_rate": 0.00038837988826815646,
      "loss": 1.5831,
      "step": 31
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 6.0562238693237305,
      "learning_rate": 0.00038793296089385473,
      "loss": 1.697,
      "step": 32
    },
    {
      "epoch": 0.03666666666666667,
      "grad_norm": 9.180710792541504,
      "learning_rate": 0.0003874860335195531,
      "loss": 4.5193,
      "step": 33
    },
    {
      "epoch": 0.03777777777777778,
      "grad_norm": 3.512871265411377,
      "learning_rate": 0.0003870391061452514,
      "loss": 0.9605,
      "step": 34
    },
    {
      "epoch": 0.03888888888888889,
      "grad_norm": 8.032533645629883,
      "learning_rate": 0.00038659217877094976,
      "loss": 3.9279,
      "step": 35
    },
    {
      "epoch": 0.04,
      "grad_norm": 4.99465799331665,
      "learning_rate": 0.00038614525139664803,
      "loss": 1.438,
      "step": 36
    },
    {
      "epoch": 0.04111111111111111,
      "grad_norm": 3.9108734130859375,
      "learning_rate": 0.0003856983240223464,
      "loss": 1.461,
      "step": 37
    },
    {
      "epoch": 0.042222222222222223,
      "grad_norm": 7.030559539794922,
      "learning_rate": 0.0003852513966480447,
      "loss": 3.0457,
      "step": 38
    },
    {
      "epoch": 0.043333333333333335,
      "grad_norm": 7.673288822174072,
      "learning_rate": 0.00038480446927374306,
      "loss": 3.3801,
      "step": 39
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 8.251694679260254,
      "learning_rate": 0.00038435754189944133,
      "loss": 2.6294,
      "step": 40
    },
    {
      "epoch": 0.04555555555555556,
      "grad_norm": 5.776803970336914,
      "learning_rate": 0.0003839106145251397,
      "loss": 2.4796,
      "step": 41
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 4.532406806945801,
      "learning_rate": 0.000383463687150838,
      "loss": 2.1002,
      "step": 42
    },
    {
      "epoch": 0.04777777777777778,
      "grad_norm": 4.821265697479248,
      "learning_rate": 0.00038301675977653636,
      "loss": 2.0149,
      "step": 43
    },
    {
      "epoch": 0.04888888888888889,
      "grad_norm": 13.530948638916016,
      "learning_rate": 0.0003825698324022346,
      "loss": 1.2776,
      "step": 44
    },
    {
      "epoch": 0.05,
      "grad_norm": 4.345108509063721,
      "learning_rate": 0.000382122905027933,
      "loss": 2.0751,
      "step": 45
    },
    {
      "epoch": 0.051111111111111114,
      "grad_norm": 4.899071216583252,
      "learning_rate": 0.0003816759776536313,
      "loss": 0.919,
      "step": 46
    },
    {
      "epoch": 0.052222222222222225,
      "grad_norm": 3.633986473083496,
      "learning_rate": 0.00038122905027932966,
      "loss": 1.2728,
      "step": 47
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 2.9143714904785156,
      "learning_rate": 0.0003807821229050279,
      "loss": 0.8297,
      "step": 48
    },
    {
      "epoch": 0.05444444444444444,
      "grad_norm": 3.4484035968780518,
      "learning_rate": 0.0003803351955307263,
      "loss": 1.4089,
      "step": 49
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 5.20994234085083,
      "learning_rate": 0.0003798882681564246,
      "loss": 1.545,
      "step": 50
    },
    {
      "epoch": 0.056666666666666664,
      "grad_norm": 4.178238868713379,
      "learning_rate": 0.00037944134078212295,
      "loss": 1.9381,
      "step": 51
    },
    {
      "epoch": 0.057777777777777775,
      "grad_norm": 9.307818412780762,
      "learning_rate": 0.0003789944134078212,
      "loss": 3.7096,
      "step": 52
    },
    {
      "epoch": 0.058888888888888886,
      "grad_norm": 7.409191131591797,
      "learning_rate": 0.00037854748603351955,
      "loss": 2.9814,
      "step": 53
    },
    {
      "epoch": 0.06,
      "grad_norm": 6.889983177185059,
      "learning_rate": 0.0003781005586592179,
      "loss": 2.1413,
      "step": 54
    },
    {
      "epoch": 0.06111111111111111,
      "grad_norm": 9.471173286437988,
      "learning_rate": 0.0003776536312849162,
      "loss": 1.9107,
      "step": 55
    },
    {
      "epoch": 0.06222222222222222,
      "grad_norm": 3.635561943054199,
      "learning_rate": 0.0003772067039106145,
      "loss": 1.3909,
      "step": 56
    },
    {
      "epoch": 0.06333333333333334,
      "grad_norm": 4.941038608551025,
      "learning_rate": 0.00037675977653631285,
      "loss": 1.7594,
      "step": 57
    },
    {
      "epoch": 0.06444444444444444,
      "grad_norm": 6.425600528717041,
      "learning_rate": 0.00037631284916201117,
      "loss": 2.4626,
      "step": 58
    },
    {
      "epoch": 0.06555555555555556,
      "grad_norm": 4.531485557556152,
      "learning_rate": 0.0003758659217877095,
      "loss": 1.565,
      "step": 59
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 4.673357009887695,
      "learning_rate": 0.0003754189944134078,
      "loss": 1.7195,
      "step": 60
    },
    {
      "epoch": 0.06777777777777778,
      "grad_norm": 6.012983322143555,
      "learning_rate": 0.00037497206703910615,
      "loss": 1.037,
      "step": 61
    },
    {
      "epoch": 0.06888888888888889,
      "grad_norm": 5.867706298828125,
      "learning_rate": 0.00037452513966480447,
      "loss": 2.4793,
      "step": 62
    },
    {
      "epoch": 0.07,
      "grad_norm": 2.7087223529815674,
      "learning_rate": 0.0003740782122905028,
      "loss": 0.8225,
      "step": 63
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 13.234498977661133,
      "learning_rate": 0.0003736312849162011,
      "loss": 1.9228,
      "step": 64
    },
    {
      "epoch": 0.07222222222222222,
      "grad_norm": 5.260988235473633,
      "learning_rate": 0.00037318435754189944,
      "loss": 2.8879,
      "step": 65
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 4.040969371795654,
      "learning_rate": 0.00037273743016759777,
      "loss": 1.5303,
      "step": 66
    },
    {
      "epoch": 0.07444444444444444,
      "grad_norm": 4.305866718292236,
      "learning_rate": 0.0003722905027932961,
      "loss": 1.6543,
      "step": 67
    },
    {
      "epoch": 0.07555555555555556,
      "grad_norm": 5.666847229003906,
      "learning_rate": 0.0003718435754189944,
      "loss": 1.4203,
      "step": 68
    },
    {
      "epoch": 0.07666666666666666,
      "grad_norm": 4.487039089202881,
      "learning_rate": 0.00037139664804469274,
      "loss": 2.0104,
      "step": 69
    },
    {
      "epoch": 0.07777777777777778,
      "grad_norm": 5.490307331085205,
      "learning_rate": 0.00037094972067039107,
      "loss": 2.1695,
      "step": 70
    },
    {
      "epoch": 0.07888888888888888,
      "grad_norm": 4.858982563018799,
      "learning_rate": 0.0003705027932960894,
      "loss": 2.1588,
      "step": 71
    },
    {
      "epoch": 0.08,
      "grad_norm": 3.7819790840148926,
      "learning_rate": 0.0003700558659217877,
      "loss": 1.3556,
      "step": 72
    },
    {
      "epoch": 0.0811111111111111,
      "grad_norm": 4.6344475746154785,
      "learning_rate": 0.00036960893854748604,
      "loss": 1.6206,
      "step": 73
    },
    {
      "epoch": 0.08222222222222222,
      "grad_norm": 3.9876503944396973,
      "learning_rate": 0.00036916201117318437,
      "loss": 1.7428,
      "step": 74
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 3.1269636154174805,
      "learning_rate": 0.0003687150837988827,
      "loss": 1.2165,
      "step": 75
    },
    {
      "epoch": 0.08444444444444445,
      "grad_norm": 4.988279819488525,
      "learning_rate": 0.000368268156424581,
      "loss": 1.3952,
      "step": 76
    },
    {
      "epoch": 0.08555555555555555,
      "grad_norm": 4.6598968505859375,
      "learning_rate": 0.00036782122905027934,
      "loss": 1.2145,
      "step": 77
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 5.143320083618164,
      "learning_rate": 0.00036737430167597767,
      "loss": 1.7609,
      "step": 78
    },
    {
      "epoch": 0.08777777777777777,
      "grad_norm": 7.345981121063232,
      "learning_rate": 0.000366927374301676,
      "loss": 1.439,
      "step": 79
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 5.278598785400391,
      "learning_rate": 0.0003664804469273743,
      "loss": 1.9913,
      "step": 80
    },
    {
      "epoch": 0.09,
      "grad_norm": 4.364718437194824,
      "learning_rate": 0.00036603351955307264,
      "loss": 1.7047,
      "step": 81
    },
    {
      "epoch": 0.09111111111111111,
      "grad_norm": 2.3718037605285645,
      "learning_rate": 0.00036558659217877096,
      "loss": 0.291,
      "step": 82
    },
    {
      "epoch": 0.09222222222222222,
      "grad_norm": 4.981142044067383,
      "learning_rate": 0.0003651396648044693,
      "loss": 2.1594,
      "step": 83
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 6.94635009765625,
      "learning_rate": 0.0003646927374301676,
      "loss": 3.3465,
      "step": 84
    },
    {
      "epoch": 0.09444444444444444,
      "grad_norm": 4.962390422821045,
      "learning_rate": 0.00036424581005586594,
      "loss": 1.8062,
      "step": 85
    },
    {
      "epoch": 0.09555555555555556,
      "grad_norm": 44.5148811340332,
      "learning_rate": 0.00036379888268156426,
      "loss": 3.5841,
      "step": 86
    },
    {
      "epoch": 0.09666666666666666,
      "grad_norm": 26.73061752319336,
      "learning_rate": 0.0003633519553072626,
      "loss": 0.9283,
      "step": 87
    },
    {
      "epoch": 0.09777777777777778,
      "grad_norm": 2.960094451904297,
      "learning_rate": 0.0003629050279329609,
      "loss": 0.9607,
      "step": 88
    },
    {
      "epoch": 0.09888888888888889,
      "grad_norm": 5.0768513679504395,
      "learning_rate": 0.00036245810055865924,
      "loss": 2.7598,
      "step": 89
    },
    {
      "epoch": 0.1,
      "grad_norm": 4.907212734222412,
      "learning_rate": 0.00036201117318435756,
      "loss": 1.962,
      "step": 90
    },
    {
      "epoch": 0.10111111111111111,
      "grad_norm": 4.843077659606934,
      "learning_rate": 0.0003615642458100559,
      "loss": 1.4009,
      "step": 91
    },
    {
      "epoch": 0.10222222222222223,
      "grad_norm": 5.6172895431518555,
      "learning_rate": 0.0003611173184357542,
      "loss": 2.0026,
      "step": 92
    },
    {
      "epoch": 0.10333333333333333,
      "grad_norm": 6.099035739898682,
      "learning_rate": 0.00036067039106145254,
      "loss": 2.3155,
      "step": 93
    },
    {
      "epoch": 0.10444444444444445,
      "grad_norm": 3.317135810852051,
      "learning_rate": 0.00036022346368715086,
      "loss": 0.8811,
      "step": 94
    },
    {
      "epoch": 0.10555555555555556,
      "grad_norm": 3.6359195709228516,
      "learning_rate": 0.0003597765363128492,
      "loss": 1.5053,
      "step": 95
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 3.2737581729888916,
      "learning_rate": 0.0003593296089385475,
      "loss": 1.1049,
      "step": 96
    },
    {
      "epoch": 0.10777777777777778,
      "grad_norm": 7.5419440269470215,
      "learning_rate": 0.00035888268156424583,
      "loss": 3.5433,
      "step": 97
    },
    {
      "epoch": 0.10888888888888888,
      "grad_norm": 4.187341690063477,
      "learning_rate": 0.00035843575418994416,
      "loss": 1.8157,
      "step": 98
    },
    {
      "epoch": 0.11,
      "grad_norm": 4.532643795013428,
      "learning_rate": 0.0003579888268156425,
      "loss": 2.2542,
      "step": 99
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 2.413792610168457,
      "learning_rate": 0.00035754189944134075,
      "loss": 1.1079,
      "step": 100
    },
    {
      "epoch": 0.11222222222222222,
      "grad_norm": 3.8566360473632812,
      "learning_rate": 0.00035709497206703913,
      "loss": 1.755,
      "step": 101
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 4.985625743865967,
      "learning_rate": 0.0003566480446927374,
      "loss": 0.7691,
      "step": 102
    },
    {
      "epoch": 0.11444444444444445,
      "grad_norm": 3.6404807567596436,
      "learning_rate": 0.0003562011173184358,
      "loss": 1.549,
      "step": 103
    },
    {
      "epoch": 0.11555555555555555,
      "grad_norm": 4.257077693939209,
      "learning_rate": 0.00035575418994413405,
      "loss": 2.3047,
      "step": 104
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 6.499237060546875,
      "learning_rate": 0.00035530726256983243,
      "loss": 2.7179,
      "step": 105
    },
    {
      "epoch": 0.11777777777777777,
      "grad_norm": 3.662224769592285,
      "learning_rate": 0.0003548603351955307,
      "loss": 0.7639,
      "step": 106
    },
    {
      "epoch": 0.11888888888888889,
      "grad_norm": 3.0845370292663574,
      "learning_rate": 0.0003544134078212291,
      "loss": 1.0763,
      "step": 107
    },
    {
      "epoch": 0.12,
      "grad_norm": 3.1672260761260986,
      "learning_rate": 0.00035396648044692735,
      "loss": 0.9808,
      "step": 108
    },
    {
      "epoch": 0.12111111111111111,
      "grad_norm": 4.480134010314941,
      "learning_rate": 0.00035351955307262573,
      "loss": 1.5354,
      "step": 109
    },
    {
      "epoch": 0.12222222222222222,
      "grad_norm": 3.27350115776062,
      "learning_rate": 0.000353072625698324,
      "loss": 1.5988,
      "step": 110
    },
    {
      "epoch": 0.12333333333333334,
      "grad_norm": 4.486992359161377,
      "learning_rate": 0.0003526256983240224,
      "loss": 1.7157,
      "step": 111
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 4.7463555335998535,
      "learning_rate": 0.00035217877094972065,
      "loss": 1.9003,
      "step": 112
    },
    {
      "epoch": 0.12555555555555556,
      "grad_norm": 4.480347156524658,
      "learning_rate": 0.00035173184357541903,
      "loss": 1.5758,
      "step": 113
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 4.424859523773193,
      "learning_rate": 0.0003512849162011173,
      "loss": 1.6789,
      "step": 114
    },
    {
      "epoch": 0.12777777777777777,
      "grad_norm": 7.933389663696289,
      "learning_rate": 0.0003508379888268157,
      "loss": 1.9972,
      "step": 115
    },
    {
      "epoch": 0.1288888888888889,
      "grad_norm": 6.418490409851074,
      "learning_rate": 0.00035039106145251395,
      "loss": 1.7902,
      "step": 116
    },
    {
      "epoch": 0.13,
      "grad_norm": 7.032714366912842,
      "learning_rate": 0.00034994413407821233,
      "loss": 1.9424,
      "step": 117
    },
    {
      "epoch": 0.13111111111111112,
      "grad_norm": 4.723656177520752,
      "learning_rate": 0.0003494972067039106,
      "loss": 1.3483,
      "step": 118
    },
    {
      "epoch": 0.1322222222222222,
      "grad_norm": 4.097376823425293,
      "learning_rate": 0.000349050279329609,
      "loss": 1.2044,
      "step": 119
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 5.161742687225342,
      "learning_rate": 0.00034860335195530725,
      "loss": 1.4383,
      "step": 120
    },
    {
      "epoch": 0.13444444444444445,
      "grad_norm": 5.2892022132873535,
      "learning_rate": 0.0003481564245810056,
      "loss": 1.3897,
      "step": 121
    },
    {
      "epoch": 0.13555555555555557,
      "grad_norm": 3.9891233444213867,
      "learning_rate": 0.0003477094972067039,
      "loss": 1.1008,
      "step": 122
    },
    {
      "epoch": 0.13666666666666666,
      "grad_norm": 4.3257341384887695,
      "learning_rate": 0.0003472625698324023,
      "loss": 1.7974,
      "step": 123
    },
    {
      "epoch": 0.13777777777777778,
      "grad_norm": 3.1082000732421875,
      "learning_rate": 0.00034681564245810055,
      "loss": 1.3108,
      "step": 124
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 2.79287052154541,
      "learning_rate": 0.0003463687150837989,
      "loss": 1.2907,
      "step": 125
    },
    {
      "epoch": 0.14,
      "grad_norm": 3.6954281330108643,
      "learning_rate": 0.0003459217877094972,
      "loss": 1.2163,
      "step": 126
    },
    {
      "epoch": 0.1411111111111111,
      "grad_norm": 3.123845338821411,
      "learning_rate": 0.0003454748603351956,
      "loss": 1.0035,
      "step": 127
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 3.959489583969116,
      "learning_rate": 0.00034502793296089384,
      "loss": 1.9728,
      "step": 128
    },
    {
      "epoch": 0.14333333333333334,
      "grad_norm": 4.954408645629883,
      "learning_rate": 0.0003445810055865922,
      "loss": 1.4758,
      "step": 129
    },
    {
      "epoch": 0.14444444444444443,
      "grad_norm": 4.389406681060791,
      "learning_rate": 0.0003441340782122905,
      "loss": 1.2696,
      "step": 130
    },
    {
      "epoch": 0.14555555555555555,
      "grad_norm": 3.5685184001922607,
      "learning_rate": 0.0003436871508379889,
      "loss": 1.3719,
      "step": 131
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 5.23043966293335,
      "learning_rate": 0.00034324022346368714,
      "loss": 1.7357,
      "step": 132
    },
    {
      "epoch": 0.14777777777777779,
      "grad_norm": 3.882253885269165,
      "learning_rate": 0.0003427932960893855,
      "loss": 1.3318,
      "step": 133
    },
    {
      "epoch": 0.14888888888888888,
      "grad_norm": 4.115442276000977,
      "learning_rate": 0.0003423463687150838,
      "loss": 1.2963,
      "step": 134
    },
    {
      "epoch": 0.15,
      "grad_norm": 6.429259777069092,
      "learning_rate": 0.00034189944134078217,
      "loss": 1.4418,
      "step": 135
    },
    {
      "epoch": 0.1511111111111111,
      "grad_norm": 4.4910759925842285,
      "learning_rate": 0.00034145251396648044,
      "loss": 1.0293,
      "step": 136
    },
    {
      "epoch": 0.15222222222222223,
      "grad_norm": 7.133009433746338,
      "learning_rate": 0.0003410055865921788,
      "loss": 2.2008,
      "step": 137
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 4.328670501708984,
      "learning_rate": 0.0003405586592178771,
      "loss": 1.1235,
      "step": 138
    },
    {
      "epoch": 0.15444444444444444,
      "grad_norm": 3.39153790473938,
      "learning_rate": 0.00034011173184357547,
      "loss": 1.0059,
      "step": 139
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 4.354849338531494,
      "learning_rate": 0.00033966480446927374,
      "loss": 1.8092,
      "step": 140
    },
    {
      "epoch": 0.15666666666666668,
      "grad_norm": 4.986898422241211,
      "learning_rate": 0.0003392178770949721,
      "loss": 1.8124,
      "step": 141
    },
    {
      "epoch": 0.15777777777777777,
      "grad_norm": 3.4930646419525146,
      "learning_rate": 0.0003387709497206704,
      "loss": 1.1112,
      "step": 142
    },
    {
      "epoch": 0.15888888888888889,
      "grad_norm": 3.0276505947113037,
      "learning_rate": 0.00033832402234636877,
      "loss": 0.7426,
      "step": 143
    },
    {
      "epoch": 0.16,
      "grad_norm": 4.397948265075684,
      "learning_rate": 0.00033787709497206704,
      "loss": 1.0434,
      "step": 144
    },
    {
      "epoch": 0.16111111111111112,
      "grad_norm": 4.784089088439941,
      "learning_rate": 0.0003374301675977654,
      "loss": 1.065,
      "step": 145
    },
    {
      "epoch": 0.1622222222222222,
      "grad_norm": 6.937569618225098,
      "learning_rate": 0.0003369832402234637,
      "loss": 2.5578,
      "step": 146
    },
    {
      "epoch": 0.16333333333333333,
      "grad_norm": 4.247250080108643,
      "learning_rate": 0.000336536312849162,
      "loss": 1.6105,
      "step": 147
    },
    {
      "epoch": 0.16444444444444445,
      "grad_norm": 5.28787899017334,
      "learning_rate": 0.00033608938547486034,
      "loss": 1.9502,
      "step": 148
    },
    {
      "epoch": 0.16555555555555557,
      "grad_norm": 5.599507808685303,
      "learning_rate": 0.00033564245810055866,
      "loss": 2.3676,
      "step": 149
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 4.528848171234131,
      "learning_rate": 0.000335195530726257,
      "loss": 1.6041,
      "step": 150
    },
    {
      "epoch": 0.16777777777777778,
      "grad_norm": 3.5889856815338135,
      "learning_rate": 0.0003347486033519553,
      "loss": 1.456,
      "step": 151
    },
    {
      "epoch": 0.1688888888888889,
      "grad_norm": 3.6292643547058105,
      "learning_rate": 0.00033430167597765364,
      "loss": 1.0822,
      "step": 152
    },
    {
      "epoch": 0.17,
      "grad_norm": 3.952230453491211,
      "learning_rate": 0.00033385474860335196,
      "loss": 1.7415,
      "step": 153
    },
    {
      "epoch": 0.1711111111111111,
      "grad_norm": 2.6783647537231445,
      "learning_rate": 0.0003334078212290503,
      "loss": 0.8499,
      "step": 154
    },
    {
      "epoch": 0.17222222222222222,
      "grad_norm": 4.983895301818848,
      "learning_rate": 0.0003329608938547486,
      "loss": 1.7751,
      "step": 155
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 2.4895739555358887,
      "learning_rate": 0.00033251396648044694,
      "loss": 0.6062,
      "step": 156
    },
    {
      "epoch": 0.17444444444444446,
      "grad_norm": 5.7844743728637695,
      "learning_rate": 0.00033206703910614526,
      "loss": 2.8468,
      "step": 157
    },
    {
      "epoch": 0.17555555555555555,
      "grad_norm": 7.148275375366211,
      "learning_rate": 0.0003316201117318436,
      "loss": 2.4512,
      "step": 158
    },
    {
      "epoch": 0.17666666666666667,
      "grad_norm": 19.37252426147461,
      "learning_rate": 0.0003311731843575419,
      "loss": 2.2075,
      "step": 159
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 3.361112594604492,
      "learning_rate": 0.00033072625698324023,
      "loss": 1.3939,
      "step": 160
    },
    {
      "epoch": 0.17888888888888888,
      "grad_norm": 4.417331695556641,
      "learning_rate": 0.00033027932960893856,
      "loss": 1.6235,
      "step": 161
    },
    {
      "epoch": 0.18,
      "grad_norm": 3.073518753051758,
      "learning_rate": 0.0003298324022346369,
      "loss": 1.3827,
      "step": 162
    },
    {
      "epoch": 0.1811111111111111,
      "grad_norm": 5.083468437194824,
      "learning_rate": 0.0003293854748603352,
      "loss": 1.6354,
      "step": 163
    },
    {
      "epoch": 0.18222222222222223,
      "grad_norm": 4.139428615570068,
      "learning_rate": 0.00032893854748603353,
      "loss": 1.5636,
      "step": 164
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 4.6545610427856445,
      "learning_rate": 0.00032849162011173186,
      "loss": 1.3767,
      "step": 165
    },
    {
      "epoch": 0.18444444444444444,
      "grad_norm": 4.617306232452393,
      "learning_rate": 0.0003280446927374302,
      "loss": 1.7554,
      "step": 166
    },
    {
      "epoch": 0.18555555555555556,
      "grad_norm": 6.672477722167969,
      "learning_rate": 0.0003275977653631285,
      "loss": 2.3145,
      "step": 167
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 3.6032471656799316,
      "learning_rate": 0.00032715083798882683,
      "loss": 1.3041,
      "step": 168
    },
    {
      "epoch": 0.18777777777777777,
      "grad_norm": 5.9876275062561035,
      "learning_rate": 0.00032670391061452516,
      "loss": 1.3934,
      "step": 169
    },
    {
      "epoch": 0.18888888888888888,
      "grad_norm": 4.9405837059021,
      "learning_rate": 0.0003262569832402235,
      "loss": 1.6443,
      "step": 170
    },
    {
      "epoch": 0.19,
      "grad_norm": 4.829977035522461,
      "learning_rate": 0.0003258100558659218,
      "loss": 0.8093,
      "step": 171
    },
    {
      "epoch": 0.19111111111111112,
      "grad_norm": 5.743224620819092,
      "learning_rate": 0.00032536312849162013,
      "loss": 2.1192,
      "step": 172
    },
    {
      "epoch": 0.1922222222222222,
      "grad_norm": 4.577620983123779,
      "learning_rate": 0.00032491620111731845,
      "loss": 1.8403,
      "step": 173
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 4.883712291717529,
      "learning_rate": 0.0003244692737430168,
      "loss": 1.7394,
      "step": 174
    },
    {
      "epoch": 0.19444444444444445,
      "grad_norm": 4.827327728271484,
      "learning_rate": 0.0003240223463687151,
      "loss": 1.2607,
      "step": 175
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 3.704674005508423,
      "learning_rate": 0.00032357541899441343,
      "loss": 0.9367,
      "step": 176
    },
    {
      "epoch": 0.19666666666666666,
      "grad_norm": 4.164095878601074,
      "learning_rate": 0.00032312849162011175,
      "loss": 1.8834,
      "step": 177
    },
    {
      "epoch": 0.19777777777777777,
      "grad_norm": 3.4088237285614014,
      "learning_rate": 0.0003226815642458101,
      "loss": 0.6981,
      "step": 178
    },
    {
      "epoch": 0.1988888888888889,
      "grad_norm": 4.5575127601623535,
      "learning_rate": 0.0003222346368715084,
      "loss": 1.937,
      "step": 179
    },
    {
      "epoch": 0.2,
      "grad_norm": 6.011362075805664,
      "learning_rate": 0.00032178770949720673,
      "loss": 1.8555,
      "step": 180
    },
    {
      "epoch": 0.2011111111111111,
      "grad_norm": 5.323307514190674,
      "learning_rate": 0.00032134078212290505,
      "loss": 1.2641,
      "step": 181
    },
    {
      "epoch": 0.20222222222222222,
      "grad_norm": 3.398827075958252,
      "learning_rate": 0.0003208938547486034,
      "loss": 0.8467,
      "step": 182
    },
    {
      "epoch": 0.20333333333333334,
      "grad_norm": 4.5794358253479,
      "learning_rate": 0.0003204469273743017,
      "loss": 1.7449,
      "step": 183
    },
    {
      "epoch": 0.20444444444444446,
      "grad_norm": 3.102496862411499,
      "learning_rate": 0.00032,
      "loss": 0.8409,
      "step": 184
    },
    {
      "epoch": 0.20555555555555555,
      "grad_norm": 9.012896537780762,
      "learning_rate": 0.00031955307262569835,
      "loss": 2.919,
      "step": 185
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 4.911559104919434,
      "learning_rate": 0.0003191061452513967,
      "loss": 0.7596,
      "step": 186
    },
    {
      "epoch": 0.20777777777777778,
      "grad_norm": 4.732764720916748,
      "learning_rate": 0.000318659217877095,
      "loss": 1.329,
      "step": 187
    },
    {
      "epoch": 0.2088888888888889,
      "grad_norm": 2.914370059967041,
      "learning_rate": 0.0003182122905027933,
      "loss": 1.3467,
      "step": 188
    },
    {
      "epoch": 0.21,
      "grad_norm": 3.40000581741333,
      "learning_rate": 0.00031776536312849165,
      "loss": 0.9386,
      "step": 189
    },
    {
      "epoch": 0.2111111111111111,
      "grad_norm": 3.7253870964050293,
      "learning_rate": 0.00031731843575419,
      "loss": 1.5704,
      "step": 190
    },
    {
      "epoch": 0.21222222222222223,
      "grad_norm": 4.472389221191406,
      "learning_rate": 0.0003168715083798883,
      "loss": 2.025,
      "step": 191
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 5.652095317840576,
      "learning_rate": 0.0003164245810055866,
      "loss": 1.8743,
      "step": 192
    },
    {
      "epoch": 0.21444444444444444,
      "grad_norm": 5.028915882110596,
      "learning_rate": 0.00031597765363128495,
      "loss": 1.6454,
      "step": 193
    },
    {
      "epoch": 0.21555555555555556,
      "grad_norm": 4.0819993019104,
      "learning_rate": 0.0003155307262569832,
      "loss": 1.7411,
      "step": 194
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 4.584575176239014,
      "learning_rate": 0.0003150837988826816,
      "loss": 2.1646,
      "step": 195
    },
    {
      "epoch": 0.21777777777777776,
      "grad_norm": 3.8849034309387207,
      "learning_rate": 0.00031463687150837987,
      "loss": 1.4942,
      "step": 196
    },
    {
      "epoch": 0.21888888888888888,
      "grad_norm": 2.6710121631622314,
      "learning_rate": 0.00031418994413407825,
      "loss": 1.0024,
      "step": 197
    },
    {
      "epoch": 0.22,
      "grad_norm": 3.535975456237793,
      "learning_rate": 0.0003137430167597765,
      "loss": 1.401,
      "step": 198
    },
    {
      "epoch": 0.22111111111111112,
      "grad_norm": 3.167707681655884,
      "learning_rate": 0.0003132960893854749,
      "loss": 1.1194,
      "step": 199
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 3.8941915035247803,
      "learning_rate": 0.00031284916201117317,
      "loss": 1.2419,
      "step": 200
    },
    {
      "epoch": 0.22333333333333333,
      "grad_norm": 3.6284825801849365,
      "learning_rate": 0.00031240223463687155,
      "loss": 0.9714,
      "step": 201
    },
    {
      "epoch": 0.22444444444444445,
      "grad_norm": 3.130222797393799,
      "learning_rate": 0.0003119553072625698,
      "loss": 1.1354,
      "step": 202
    },
    {
      "epoch": 0.22555555555555556,
      "grad_norm": 2.9123501777648926,
      "learning_rate": 0.0003115083798882682,
      "loss": 1.3435,
      "step": 203
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 5.095318794250488,
      "learning_rate": 0.00031106145251396647,
      "loss": 1.9007,
      "step": 204
    },
    {
      "epoch": 0.22777777777777777,
      "grad_norm": 3.767468214035034,
      "learning_rate": 0.00031061452513966484,
      "loss": 1.7241,
      "step": 205
    },
    {
      "epoch": 0.2288888888888889,
      "grad_norm": 5.5607686042785645,
      "learning_rate": 0.0003101675977653631,
      "loss": 1.3529,
      "step": 206
    },
    {
      "epoch": 0.23,
      "grad_norm": 3.8202216625213623,
      "learning_rate": 0.0003097206703910615,
      "loss": 1.4674,
      "step": 207
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 5.502582550048828,
      "learning_rate": 0.00030927374301675976,
      "loss": 1.5315,
      "step": 208
    },
    {
      "epoch": 0.23222222222222222,
      "grad_norm": 4.960947513580322,
      "learning_rate": 0.00030882681564245814,
      "loss": 1.7071,
      "step": 209
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 7.829428672790527,
      "learning_rate": 0.0003083798882681564,
      "loss": 1.4237,
      "step": 210
    },
    {
      "epoch": 0.23444444444444446,
      "grad_norm": 5.422718048095703,
      "learning_rate": 0.0003079329608938548,
      "loss": 1.3605,
      "step": 211
    },
    {
      "epoch": 0.23555555555555555,
      "grad_norm": 6.640440464019775,
      "learning_rate": 0.00030748603351955306,
      "loss": 2.8246,
      "step": 212
    },
    {
      "epoch": 0.23666666666666666,
      "grad_norm": 5.089559555053711,
      "learning_rate": 0.00030703910614525144,
      "loss": 2.2398,
      "step": 213
    },
    {
      "epoch": 0.23777777777777778,
      "grad_norm": 6.0749359130859375,
      "learning_rate": 0.0003065921787709497,
      "loss": 1.3765,
      "step": 214
    },
    {
      "epoch": 0.2388888888888889,
      "grad_norm": 5.18739652633667,
      "learning_rate": 0.0003061452513966481,
      "loss": 1.6717,
      "step": 215
    },
    {
      "epoch": 0.24,
      "grad_norm": 3.204556941986084,
      "learning_rate": 0.00030569832402234636,
      "loss": 0.8287,
      "step": 216
    },
    {
      "epoch": 0.2411111111111111,
      "grad_norm": 5.0780744552612305,
      "learning_rate": 0.00030525139664804474,
      "loss": 1.9013,
      "step": 217
    },
    {
      "epoch": 0.24222222222222223,
      "grad_norm": 6.265993118286133,
      "learning_rate": 0.000304804469273743,
      "loss": 2.7089,
      "step": 218
    },
    {
      "epoch": 0.24333333333333335,
      "grad_norm": 3.692878007888794,
      "learning_rate": 0.0003043575418994414,
      "loss": 1.457,
      "step": 219
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 3.316823959350586,
      "learning_rate": 0.00030391061452513966,
      "loss": 1.2022,
      "step": 220
    },
    {
      "epoch": 0.24555555555555555,
      "grad_norm": 3.5658514499664307,
      "learning_rate": 0.00030346368715083804,
      "loss": 1.5823,
      "step": 221
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 3.498311758041382,
      "learning_rate": 0.0003030167597765363,
      "loss": 1.1225,
      "step": 222
    },
    {
      "epoch": 0.2477777777777778,
      "grad_norm": 4.359038352966309,
      "learning_rate": 0.0003025698324022347,
      "loss": 1.9646,
      "step": 223
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 2.4604036808013916,
      "learning_rate": 0.00030212290502793296,
      "loss": 0.6206,
      "step": 224
    },
    {
      "epoch": 0.25,
      "grad_norm": 3.257032632827759,
      "learning_rate": 0.00030167597765363134,
      "loss": 1.0877,
      "step": 225
    },
    {
      "epoch": 0.2511111111111111,
      "grad_norm": 3.257833957672119,
      "learning_rate": 0.0003012290502793296,
      "loss": 1.2897,
      "step": 226
    },
    {
      "epoch": 0.25222222222222224,
      "grad_norm": 4.1108245849609375,
      "learning_rate": 0.000300782122905028,
      "loss": 1.7157,
      "step": 227
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 2.944929361343384,
      "learning_rate": 0.00030033519553072626,
      "loss": 1.1264,
      "step": 228
    },
    {
      "epoch": 0.2544444444444444,
      "grad_norm": 2.297238349914551,
      "learning_rate": 0.00029988826815642464,
      "loss": 0.6277,
      "step": 229
    },
    {
      "epoch": 0.25555555555555554,
      "grad_norm": 4.289222240447998,
      "learning_rate": 0.0002994413407821229,
      "loss": 1.9735,
      "step": 230
    },
    {
      "epoch": 0.25666666666666665,
      "grad_norm": 5.019431114196777,
      "learning_rate": 0.0002989944134078213,
      "loss": 1.8945,
      "step": 231
    },
    {
      "epoch": 0.2577777777777778,
      "grad_norm": 4.461295127868652,
      "learning_rate": 0.00029854748603351956,
      "loss": 1.4902,
      "step": 232
    },
    {
      "epoch": 0.2588888888888889,
      "grad_norm": 4.624236106872559,
      "learning_rate": 0.00029810055865921793,
      "loss": 1.5249,
      "step": 233
    },
    {
      "epoch": 0.26,
      "grad_norm": 5.118597984313965,
      "learning_rate": 0.0002976536312849162,
      "loss": 1.0502,
      "step": 234
    },
    {
      "epoch": 0.2611111111111111,
      "grad_norm": 3.682424306869507,
      "learning_rate": 0.0002972067039106146,
      "loss": 1.3622,
      "step": 235
    },
    {
      "epoch": 0.26222222222222225,
      "grad_norm": 3.5628015995025635,
      "learning_rate": 0.00029675977653631285,
      "loss": 1.0512,
      "step": 236
    },
    {
      "epoch": 0.2633333333333333,
      "grad_norm": 3.737548828125,
      "learning_rate": 0.00029631284916201123,
      "loss": 0.9042,
      "step": 237
    },
    {
      "epoch": 0.2644444444444444,
      "grad_norm": 4.798216819763184,
      "learning_rate": 0.0002958659217877095,
      "loss": 1.4463,
      "step": 238
    },
    {
      "epoch": 0.26555555555555554,
      "grad_norm": 2.688568353652954,
      "learning_rate": 0.0002954189944134079,
      "loss": 0.7716,
      "step": 239
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 3.6980979442596436,
      "learning_rate": 0.00029497206703910615,
      "loss": 1.1,
      "step": 240
    },
    {
      "epoch": 0.2677777777777778,
      "grad_norm": 6.552528381347656,
      "learning_rate": 0.0002945251396648045,
      "loss": 1.7298,
      "step": 241
    },
    {
      "epoch": 0.2688888888888889,
      "grad_norm": 2.8682961463928223,
      "learning_rate": 0.0002940782122905028,
      "loss": 1.073,
      "step": 242
    },
    {
      "epoch": 0.27,
      "grad_norm": 2.9253482818603516,
      "learning_rate": 0.00029363128491620113,
      "loss": 0.7358,
      "step": 243
    },
    {
      "epoch": 0.27111111111111114,
      "grad_norm": 4.100208282470703,
      "learning_rate": 0.00029318435754189945,
      "loss": 1.4155,
      "step": 244
    },
    {
      "epoch": 0.2722222222222222,
      "grad_norm": 4.404561996459961,
      "learning_rate": 0.0002927374301675978,
      "loss": 1.1268,
      "step": 245
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 3.710512638092041,
      "learning_rate": 0.0002922905027932961,
      "loss": 1.5473,
      "step": 246
    },
    {
      "epoch": 0.27444444444444444,
      "grad_norm": 3.2975802421569824,
      "learning_rate": 0.0002918435754189944,
      "loss": 1.1112,
      "step": 247
    },
    {
      "epoch": 0.27555555555555555,
      "grad_norm": 3.5534985065460205,
      "learning_rate": 0.00029139664804469275,
      "loss": 1.3255,
      "step": 248
    },
    {
      "epoch": 0.27666666666666667,
      "grad_norm": 3.4632229804992676,
      "learning_rate": 0.0002909497206703911,
      "loss": 0.8194,
      "step": 249
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 2.6548073291778564,
      "learning_rate": 0.0002905027932960894,
      "loss": 0.9331,
      "step": 250
    },
    {
      "epoch": 0.2788888888888889,
      "grad_norm": 3.539944648742676,
      "learning_rate": 0.0002900558659217877,
      "loss": 0.7713,
      "step": 251
    },
    {
      "epoch": 0.28,
      "grad_norm": 4.820894241333008,
      "learning_rate": 0.00028960893854748605,
      "loss": 2.3496,
      "step": 252
    },
    {
      "epoch": 0.2811111111111111,
      "grad_norm": 5.501596450805664,
      "learning_rate": 0.0002891620111731844,
      "loss": 1.5384,
      "step": 253
    },
    {
      "epoch": 0.2822222222222222,
      "grad_norm": 4.18580961227417,
      "learning_rate": 0.0002887150837988827,
      "loss": 1.6477,
      "step": 254
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 3.8226888179779053,
      "learning_rate": 0.000288268156424581,
      "loss": 1.3237,
      "step": 255
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 2.9067814350128174,
      "learning_rate": 0.00028782122905027935,
      "loss": 0.9607,
      "step": 256
    },
    {
      "epoch": 0.28555555555555556,
      "grad_norm": 2.9039981365203857,
      "learning_rate": 0.00028737430167597767,
      "loss": 1.1106,
      "step": 257
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 4.222710609436035,
      "learning_rate": 0.000286927374301676,
      "loss": 1.5485,
      "step": 258
    },
    {
      "epoch": 0.2877777777777778,
      "grad_norm": 3.719818353652954,
      "learning_rate": 0.0002864804469273743,
      "loss": 1.0776,
      "step": 259
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 3.7516114711761475,
      "learning_rate": 0.00028603351955307265,
      "loss": 1.1875,
      "step": 260
    },
    {
      "epoch": 0.29,
      "grad_norm": 5.092064380645752,
      "learning_rate": 0.00028558659217877097,
      "loss": 2.0134,
      "step": 261
    },
    {
      "epoch": 0.2911111111111111,
      "grad_norm": 4.8089118003845215,
      "learning_rate": 0.0002851396648044693,
      "loss": 0.94,
      "step": 262
    },
    {
      "epoch": 0.2922222222222222,
      "grad_norm": 4.480296611785889,
      "learning_rate": 0.0002846927374301676,
      "loss": 0.9116,
      "step": 263
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 4.037253379821777,
      "learning_rate": 0.00028424581005586595,
      "loss": 0.9978,
      "step": 264
    },
    {
      "epoch": 0.29444444444444445,
      "grad_norm": 4.599876403808594,
      "learning_rate": 0.00028379888268156427,
      "loss": 1.3938,
      "step": 265
    },
    {
      "epoch": 0.29555555555555557,
      "grad_norm": 5.718170166015625,
      "learning_rate": 0.0002833519553072626,
      "loss": 1.7598,
      "step": 266
    },
    {
      "epoch": 0.2966666666666667,
      "grad_norm": 6.650409698486328,
      "learning_rate": 0.0002829050279329609,
      "loss": 2.6467,
      "step": 267
    },
    {
      "epoch": 0.29777777777777775,
      "grad_norm": 4.298861026763916,
      "learning_rate": 0.00028245810055865924,
      "loss": 1.4325,
      "step": 268
    },
    {
      "epoch": 0.29888888888888887,
      "grad_norm": 3.275322437286377,
      "learning_rate": 0.00028201117318435757,
      "loss": 1.3493,
      "step": 269
    },
    {
      "epoch": 0.3,
      "grad_norm": 5.224730014801025,
      "learning_rate": 0.0002815642458100559,
      "loss": 1.9436,
      "step": 270
    },
    {
      "epoch": 0.3011111111111111,
      "grad_norm": 3.11421799659729,
      "learning_rate": 0.0002811173184357542,
      "loss": 0.702,
      "step": 271
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 2.2486352920532227,
      "learning_rate": 0.00028067039106145254,
      "loss": 0.726,
      "step": 272
    },
    {
      "epoch": 0.30333333333333334,
      "grad_norm": 2.179407835006714,
      "learning_rate": 0.00028022346368715087,
      "loss": 0.7757,
      "step": 273
    },
    {
      "epoch": 0.30444444444444446,
      "grad_norm": 3.9837093353271484,
      "learning_rate": 0.0002797765363128492,
      "loss": 1.7688,
      "step": 274
    },
    {
      "epoch": 0.3055555555555556,
      "grad_norm": 3.767000675201416,
      "learning_rate": 0.0002793296089385475,
      "loss": 1.5688,
      "step": 275
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 2.724942207336426,
      "learning_rate": 0.00027888268156424584,
      "loss": 0.8724,
      "step": 276
    },
    {
      "epoch": 0.30777777777777776,
      "grad_norm": 5.210695743560791,
      "learning_rate": 0.00027843575418994417,
      "loss": 1.6145,
      "step": 277
    },
    {
      "epoch": 0.3088888888888889,
      "grad_norm": 4.961248874664307,
      "learning_rate": 0.0002779888268156425,
      "loss": 1.5492,
      "step": 278
    },
    {
      "epoch": 0.31,
      "grad_norm": 4.732204437255859,
      "learning_rate": 0.0002775418994413408,
      "loss": 1.7267,
      "step": 279
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 7.363171577453613,
      "learning_rate": 0.00027709497206703914,
      "loss": 1.4974,
      "step": 280
    },
    {
      "epoch": 0.31222222222222223,
      "grad_norm": 3.7252190113067627,
      "learning_rate": 0.00027664804469273746,
      "loss": 1.0634,
      "step": 281
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 3.9955549240112305,
      "learning_rate": 0.0002762011173184358,
      "loss": 1.6069,
      "step": 282
    },
    {
      "epoch": 0.31444444444444447,
      "grad_norm": 5.626176834106445,
      "learning_rate": 0.0002757541899441341,
      "loss": 1.5824,
      "step": 283
    },
    {
      "epoch": 0.31555555555555553,
      "grad_norm": 2.6134145259857178,
      "learning_rate": 0.00027530726256983244,
      "loss": 0.6905,
      "step": 284
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 3.9546806812286377,
      "learning_rate": 0.00027486033519553076,
      "loss": 1.6899,
      "step": 285
    },
    {
      "epoch": 0.31777777777777777,
      "grad_norm": 4.451503753662109,
      "learning_rate": 0.0002744134078212291,
      "loss": 1.6183,
      "step": 286
    },
    {
      "epoch": 0.3188888888888889,
      "grad_norm": 3.971996307373047,
      "learning_rate": 0.0002739664804469274,
      "loss": 1.8411,
      "step": 287
    },
    {
      "epoch": 0.32,
      "grad_norm": 5.418331623077393,
      "learning_rate": 0.0002735195530726257,
      "loss": 2.0589,
      "step": 288
    },
    {
      "epoch": 0.3211111111111111,
      "grad_norm": 5.297604084014893,
      "learning_rate": 0.00027307262569832406,
      "loss": 2.0741,
      "step": 289
    },
    {
      "epoch": 0.32222222222222224,
      "grad_norm": 2.501091241836548,
      "learning_rate": 0.00027262569832402233,
      "loss": 0.9965,
      "step": 290
    },
    {
      "epoch": 0.3233333333333333,
      "grad_norm": 2.973466157913208,
      "learning_rate": 0.0002721787709497207,
      "loss": 1.0671,
      "step": 291
    },
    {
      "epoch": 0.3244444444444444,
      "grad_norm": 4.958658218383789,
      "learning_rate": 0.000271731843575419,
      "loss": 2.1835,
      "step": 292
    },
    {
      "epoch": 0.32555555555555554,
      "grad_norm": 3.101989507675171,
      "learning_rate": 0.00027128491620111736,
      "loss": 1.4378,
      "step": 293
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 3.0990958213806152,
      "learning_rate": 0.00027083798882681563,
      "loss": 1.0931,
      "step": 294
    },
    {
      "epoch": 0.3277777777777778,
      "grad_norm": 3.1660823822021484,
      "learning_rate": 0.00027039106145251396,
      "loss": 1.0701,
      "step": 295
    },
    {
      "epoch": 0.3288888888888889,
      "grad_norm": 3.4880926609039307,
      "learning_rate": 0.0002699441340782123,
      "loss": 0.9874,
      "step": 296
    },
    {
      "epoch": 0.33,
      "grad_norm": 5.140780448913574,
      "learning_rate": 0.0002694972067039106,
      "loss": 2.2204,
      "step": 297
    },
    {
      "epoch": 0.33111111111111113,
      "grad_norm": 3.39481258392334,
      "learning_rate": 0.00026905027932960893,
      "loss": 1.5994,
      "step": 298
    },
    {
      "epoch": 0.3322222222222222,
      "grad_norm": 3.7615461349487305,
      "learning_rate": 0.00026860335195530725,
      "loss": 1.0581,
      "step": 299
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 4.339396953582764,
      "learning_rate": 0.0002681564245810056,
      "loss": 2.1343,
      "step": 300
    },
    {
      "epoch": 0.33444444444444443,
      "grad_norm": 3.5225515365600586,
      "learning_rate": 0.0002677094972067039,
      "loss": 1.2731,
      "step": 301
    },
    {
      "epoch": 0.33555555555555555,
      "grad_norm": 5.636123180389404,
      "learning_rate": 0.00026726256983240223,
      "loss": 1.5017,
      "step": 302
    },
    {
      "epoch": 0.33666666666666667,
      "grad_norm": 5.2279839515686035,
      "learning_rate": 0.00026681564245810055,
      "loss": 1.6236,
      "step": 303
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 4.244686126708984,
      "learning_rate": 0.0002663687150837989,
      "loss": 1.2539,
      "step": 304
    },
    {
      "epoch": 0.3388888888888889,
      "grad_norm": 4.766215801239014,
      "learning_rate": 0.0002659217877094972,
      "loss": 1.6735,
      "step": 305
    },
    {
      "epoch": 0.34,
      "grad_norm": 3.277238130569458,
      "learning_rate": 0.00026547486033519553,
      "loss": 1.1155,
      "step": 306
    },
    {
      "epoch": 0.3411111111111111,
      "grad_norm": 3.2521724700927734,
      "learning_rate": 0.00026502793296089385,
      "loss": 0.9741,
      "step": 307
    },
    {
      "epoch": 0.3422222222222222,
      "grad_norm": 5.499513626098633,
      "learning_rate": 0.0002645810055865922,
      "loss": 1.9306,
      "step": 308
    },
    {
      "epoch": 0.3433333333333333,
      "grad_norm": 4.765658378601074,
      "learning_rate": 0.0002641340782122905,
      "loss": 1.0119,
      "step": 309
    },
    {
      "epoch": 0.34444444444444444,
      "grad_norm": 4.238681793212891,
      "learning_rate": 0.0002636871508379888,
      "loss": 1.4241,
      "step": 310
    },
    {
      "epoch": 0.34555555555555556,
      "grad_norm": 2.183424949645996,
      "learning_rate": 0.00026324022346368715,
      "loss": 0.9831,
      "step": 311
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 4.269412994384766,
      "learning_rate": 0.0002627932960893855,
      "loss": 1.3823,
      "step": 312
    },
    {
      "epoch": 0.3477777777777778,
      "grad_norm": 4.7104716300964355,
      "learning_rate": 0.0002623463687150838,
      "loss": 1.3506,
      "step": 313
    },
    {
      "epoch": 0.3488888888888889,
      "grad_norm": 2.7845394611358643,
      "learning_rate": 0.0002618994413407821,
      "loss": 1.0812,
      "step": 314
    },
    {
      "epoch": 0.35,
      "grad_norm": 2.694079637527466,
      "learning_rate": 0.00026145251396648045,
      "loss": 1.1276,
      "step": 315
    },
    {
      "epoch": 0.3511111111111111,
      "grad_norm": 3.8265771865844727,
      "learning_rate": 0.0002610055865921788,
      "loss": 1.0873,
      "step": 316
    },
    {
      "epoch": 0.3522222222222222,
      "grad_norm": 3.1917495727539062,
      "learning_rate": 0.0002605586592178771,
      "loss": 0.7214,
      "step": 317
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 4.508754730224609,
      "learning_rate": 0.0002601117318435754,
      "loss": 1.4874,
      "step": 318
    },
    {
      "epoch": 0.35444444444444445,
      "grad_norm": 3.953718423843384,
      "learning_rate": 0.00025966480446927375,
      "loss": 1.3512,
      "step": 319
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 7.773561000823975,
      "learning_rate": 0.00025921787709497207,
      "loss": 2.9486,
      "step": 320
    },
    {
      "epoch": 0.3566666666666667,
      "grad_norm": 3.5441782474517822,
      "learning_rate": 0.0002587709497206704,
      "loss": 1.2662,
      "step": 321
    },
    {
      "epoch": 0.35777777777777775,
      "grad_norm": 3.858428955078125,
      "learning_rate": 0.0002583240223463687,
      "loss": 1.0871,
      "step": 322
    },
    {
      "epoch": 0.35888888888888887,
      "grad_norm": 2.764256238937378,
      "learning_rate": 0.00025787709497206705,
      "loss": 1.2838,
      "step": 323
    },
    {
      "epoch": 0.36,
      "grad_norm": 7.686944484710693,
      "learning_rate": 0.00025743016759776537,
      "loss": 1.6991,
      "step": 324
    },
    {
      "epoch": 0.3611111111111111,
      "grad_norm": 3.6089789867401123,
      "learning_rate": 0.0002569832402234637,
      "loss": 1.1418,
      "step": 325
    },
    {
      "epoch": 0.3622222222222222,
      "grad_norm": 5.1786909103393555,
      "learning_rate": 0.000256536312849162,
      "loss": 1.433,
      "step": 326
    },
    {
      "epoch": 0.36333333333333334,
      "grad_norm": 5.815115928649902,
      "learning_rate": 0.00025608938547486035,
      "loss": 2.656,
      "step": 327
    },
    {
      "epoch": 0.36444444444444446,
      "grad_norm": 4.573875427246094,
      "learning_rate": 0.00025564245810055867,
      "loss": 1.6783,
      "step": 328
    },
    {
      "epoch": 0.3655555555555556,
      "grad_norm": 4.959033012390137,
      "learning_rate": 0.000255195530726257,
      "loss": 1.7048,
      "step": 329
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 3.583256721496582,
      "learning_rate": 0.0002547486033519553,
      "loss": 1.5035,
      "step": 330
    },
    {
      "epoch": 0.36777777777777776,
      "grad_norm": 5.3254570960998535,
      "learning_rate": 0.00025430167597765364,
      "loss": 2.7792,
      "step": 331
    },
    {
      "epoch": 0.3688888888888889,
      "grad_norm": 4.516152858734131,
      "learning_rate": 0.00025385474860335197,
      "loss": 1.6382,
      "step": 332
    },
    {
      "epoch": 0.37,
      "grad_norm": 2.7871267795562744,
      "learning_rate": 0.0002534078212290503,
      "loss": 1.0904,
      "step": 333
    },
    {
      "epoch": 0.3711111111111111,
      "grad_norm": 2.765316963195801,
      "learning_rate": 0.0002529608938547486,
      "loss": 1.0126,
      "step": 334
    },
    {
      "epoch": 0.37222222222222223,
      "grad_norm": 5.815384864807129,
      "learning_rate": 0.00025251396648044694,
      "loss": 1.9772,
      "step": 335
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 4.037995338439941,
      "learning_rate": 0.00025206703910614527,
      "loss": 0.8277,
      "step": 336
    },
    {
      "epoch": 0.37444444444444447,
      "grad_norm": 4.257155418395996,
      "learning_rate": 0.0002516201117318436,
      "loss": 1.41,
      "step": 337
    },
    {
      "epoch": 0.37555555555555553,
      "grad_norm": 4.317139148712158,
      "learning_rate": 0.0002511731843575419,
      "loss": 1.8481,
      "step": 338
    },
    {
      "epoch": 0.37666666666666665,
      "grad_norm": 3.2490549087524414,
      "learning_rate": 0.00025072625698324024,
      "loss": 1.1465,
      "step": 339
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 3.9486005306243896,
      "learning_rate": 0.00025027932960893857,
      "loss": 1.0977,
      "step": 340
    },
    {
      "epoch": 0.3788888888888889,
      "grad_norm": 4.290426254272461,
      "learning_rate": 0.0002498324022346369,
      "loss": 2.0531,
      "step": 341
    },
    {
      "epoch": 0.38,
      "grad_norm": 2.651726484298706,
      "learning_rate": 0.0002493854748603352,
      "loss": 0.9013,
      "step": 342
    },
    {
      "epoch": 0.3811111111111111,
      "grad_norm": 3.706700325012207,
      "learning_rate": 0.00024893854748603354,
      "loss": 1.7639,
      "step": 343
    },
    {
      "epoch": 0.38222222222222224,
      "grad_norm": 3.7018954753875732,
      "learning_rate": 0.00024849162011173186,
      "loss": 1.4298,
      "step": 344
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 3.0992236137390137,
      "learning_rate": 0.0002480446927374302,
      "loss": 1.5955,
      "step": 345
    },
    {
      "epoch": 0.3844444444444444,
      "grad_norm": 2.9273619651794434,
      "learning_rate": 0.0002475977653631285,
      "loss": 1.2167,
      "step": 346
    },
    {
      "epoch": 0.38555555555555554,
      "grad_norm": 3.0620598793029785,
      "learning_rate": 0.0002471508379888268,
      "loss": 1.0817,
      "step": 347
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 4.104405403137207,
      "learning_rate": 0.00024670391061452516,
      "loss": 2.0067,
      "step": 348
    },
    {
      "epoch": 0.3877777777777778,
      "grad_norm": 3.211301803588867,
      "learning_rate": 0.00024625698324022343,
      "loss": 0.8555,
      "step": 349
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 5.688633441925049,
      "learning_rate": 0.0002458100558659218,
      "loss": 2.0638,
      "step": 350
    },
    {
      "epoch": 0.39,
      "grad_norm": 4.44825553894043,
      "learning_rate": 0.0002453631284916201,
      "loss": 1.667,
      "step": 351
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 5.389860153198242,
      "learning_rate": 0.00024491620111731846,
      "loss": 1.8436,
      "step": 352
    },
    {
      "epoch": 0.39222222222222225,
      "grad_norm": 2.5672507286071777,
      "learning_rate": 0.00024446927374301673,
      "loss": 0.9581,
      "step": 353
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 4.035637378692627,
      "learning_rate": 0.0002440223463687151,
      "loss": 1.8344,
      "step": 354
    },
    {
      "epoch": 0.39444444444444443,
      "grad_norm": 3.134899377822876,
      "learning_rate": 0.0002435754189944134,
      "loss": 0.6417,
      "step": 355
    },
    {
      "epoch": 0.39555555555555555,
      "grad_norm": 5.600834846496582,
      "learning_rate": 0.00024312849162011176,
      "loss": 2.1927,
      "step": 356
    },
    {
      "epoch": 0.39666666666666667,
      "grad_norm": 4.222681999206543,
      "learning_rate": 0.00024268156424581006,
      "loss": 1.4164,
      "step": 357
    },
    {
      "epoch": 0.3977777777777778,
      "grad_norm": 3.677726984024048,
      "learning_rate": 0.0002422346368715084,
      "loss": 1.1386,
      "step": 358
    },
    {
      "epoch": 0.3988888888888889,
      "grad_norm": 3.3205485343933105,
      "learning_rate": 0.0002417877094972067,
      "loss": 0.5964,
      "step": 359
    },
    {
      "epoch": 0.4,
      "grad_norm": 4.06650972366333,
      "learning_rate": 0.00024134078212290506,
      "loss": 0.4912,
      "step": 360
    },
    {
      "epoch": 0.4011111111111111,
      "grad_norm": 3.8747737407684326,
      "learning_rate": 0.00024089385474860336,
      "loss": 1.2632,
      "step": 361
    },
    {
      "epoch": 0.4022222222222222,
      "grad_norm": 5.244985103607178,
      "learning_rate": 0.0002404469273743017,
      "loss": 1.5708,
      "step": 362
    },
    {
      "epoch": 0.4033333333333333,
      "grad_norm": 4.316475868225098,
      "learning_rate": 0.00024,
      "loss": 0.9918,
      "step": 363
    },
    {
      "epoch": 0.40444444444444444,
      "grad_norm": 5.299986839294434,
      "learning_rate": 0.00023955307262569836,
      "loss": 0.9104,
      "step": 364
    },
    {
      "epoch": 0.40555555555555556,
      "grad_norm": 4.02025842666626,
      "learning_rate": 0.00023910614525139666,
      "loss": 1.342,
      "step": 365
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 3.0060739517211914,
      "learning_rate": 0.000238659217877095,
      "loss": 0.7683,
      "step": 366
    },
    {
      "epoch": 0.4077777777777778,
      "grad_norm": 4.093972682952881,
      "learning_rate": 0.0002382122905027933,
      "loss": 2.0523,
      "step": 367
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 5.229570388793945,
      "learning_rate": 0.00023776536312849166,
      "loss": 1.3498,
      "step": 368
    },
    {
      "epoch": 0.41,
      "grad_norm": 3.0701866149902344,
      "learning_rate": 0.00023731843575418995,
      "loss": 1.4716,
      "step": 369
    },
    {
      "epoch": 0.4111111111111111,
      "grad_norm": 24.903728485107422,
      "learning_rate": 0.0002368715083798883,
      "loss": 1.3536,
      "step": 370
    },
    {
      "epoch": 0.4122222222222222,
      "grad_norm": 5.022215843200684,
      "learning_rate": 0.0002364245810055866,
      "loss": 1.882,
      "step": 371
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 5.043704986572266,
      "learning_rate": 0.00023597765363128496,
      "loss": 1.2746,
      "step": 372
    },
    {
      "epoch": 0.41444444444444445,
      "grad_norm": 5.2274627685546875,
      "learning_rate": 0.00023553072625698325,
      "loss": 1.4844,
      "step": 373
    },
    {
      "epoch": 0.41555555555555557,
      "grad_norm": 3.323580741882324,
      "learning_rate": 0.0002350837988826816,
      "loss": 1.0259,
      "step": 374
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 5.823974132537842,
      "learning_rate": 0.0002346368715083799,
      "loss": 2.3333,
      "step": 375
    },
    {
      "epoch": 0.4177777777777778,
      "grad_norm": 3.5501229763031006,
      "learning_rate": 0.00023418994413407825,
      "loss": 1.9477,
      "step": 376
    },
    {
      "epoch": 0.41888888888888887,
      "grad_norm": 3.4350271224975586,
      "learning_rate": 0.00023374301675977655,
      "loss": 1.0048,
      "step": 377
    },
    {
      "epoch": 0.42,
      "grad_norm": 3.4928457736968994,
      "learning_rate": 0.0002332960893854749,
      "loss": 1.4754,
      "step": 378
    },
    {
      "epoch": 0.4211111111111111,
      "grad_norm": 5.320231914520264,
      "learning_rate": 0.0002328491620111732,
      "loss": 1.7021,
      "step": 379
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 4.2802205085754395,
      "learning_rate": 0.00023240223463687155,
      "loss": 1.3355,
      "step": 380
    },
    {
      "epoch": 0.42333333333333334,
      "grad_norm": 2.7161571979522705,
      "learning_rate": 0.00023195530726256985,
      "loss": 0.5097,
      "step": 381
    },
    {
      "epoch": 0.42444444444444446,
      "grad_norm": 4.202108860015869,
      "learning_rate": 0.00023150837988826815,
      "loss": 1.2206,
      "step": 382
    },
    {
      "epoch": 0.4255555555555556,
      "grad_norm": 2.7505011558532715,
      "learning_rate": 0.0002310614525139665,
      "loss": 0.3949,
      "step": 383
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 3.5091168880462646,
      "learning_rate": 0.0002306145251396648,
      "loss": 1.5015,
      "step": 384
    },
    {
      "epoch": 0.42777777777777776,
      "grad_norm": 4.343809127807617,
      "learning_rate": 0.00023016759776536315,
      "loss": 1.961,
      "step": 385
    },
    {
      "epoch": 0.4288888888888889,
      "grad_norm": 4.721001625061035,
      "learning_rate": 0.00022972067039106145,
      "loss": 1.7527,
      "step": 386
    },
    {
      "epoch": 0.43,
      "grad_norm": 3.098681688308716,
      "learning_rate": 0.0002292737430167598,
      "loss": 1.0442,
      "step": 387
    },
    {
      "epoch": 0.4311111111111111,
      "grad_norm": 4.581486701965332,
      "learning_rate": 0.0002288268156424581,
      "loss": 1.8332,
      "step": 388
    },
    {
      "epoch": 0.43222222222222223,
      "grad_norm": 2.6849911212921143,
      "learning_rate": 0.00022837988826815645,
      "loss": 1.018,
      "step": 389
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 4.41014289855957,
      "learning_rate": 0.00022793296089385474,
      "loss": 1.1824,
      "step": 390
    },
    {
      "epoch": 0.43444444444444447,
      "grad_norm": 5.764942646026611,
      "learning_rate": 0.0002274860335195531,
      "loss": 1.9798,
      "step": 391
    },
    {
      "epoch": 0.43555555555555553,
      "grad_norm": 2.647563934326172,
      "learning_rate": 0.0002270391061452514,
      "loss": 1.2728,
      "step": 392
    },
    {
      "epoch": 0.43666666666666665,
      "grad_norm": 4.924495220184326,
      "learning_rate": 0.00022659217877094975,
      "loss": 1.4105,
      "step": 393
    },
    {
      "epoch": 0.43777777777777777,
      "grad_norm": 5.0026726722717285,
      "learning_rate": 0.00022614525139664804,
      "loss": 1.6054,
      "step": 394
    },
    {
      "epoch": 0.4388888888888889,
      "grad_norm": 4.18698263168335,
      "learning_rate": 0.0002256983240223464,
      "loss": 1.8443,
      "step": 395
    },
    {
      "epoch": 0.44,
      "grad_norm": 2.1058590412139893,
      "learning_rate": 0.0002252513966480447,
      "loss": 0.7269,
      "step": 396
    },
    {
      "epoch": 0.4411111111111111,
      "grad_norm": 2.357701063156128,
      "learning_rate": 0.00022480446927374304,
      "loss": 0.923,
      "step": 397
    },
    {
      "epoch": 0.44222222222222224,
      "grad_norm": 5.768906593322754,
      "learning_rate": 0.00022435754189944134,
      "loss": 1.6707,
      "step": 398
    },
    {
      "epoch": 0.44333333333333336,
      "grad_norm": 3.447263717651367,
      "learning_rate": 0.0002239106145251397,
      "loss": 1.0863,
      "step": 399
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 3.681950807571411,
      "learning_rate": 0.000223463687150838,
      "loss": 0.9935,
      "step": 400
    },
    {
      "epoch": 0.44555555555555554,
      "grad_norm": 4.183786392211914,
      "learning_rate": 0.00022301675977653634,
      "loss": 1.7413,
      "step": 401
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 7.364016532897949,
      "learning_rate": 0.00022256983240223464,
      "loss": 2.77,
      "step": 402
    },
    {
      "epoch": 0.4477777777777778,
      "grad_norm": 4.441112995147705,
      "learning_rate": 0.000222122905027933,
      "loss": 1.5199,
      "step": 403
    },
    {
      "epoch": 0.4488888888888889,
      "grad_norm": 5.167997360229492,
      "learning_rate": 0.0002216759776536313,
      "loss": 1.5024,
      "step": 404
    },
    {
      "epoch": 0.45,
      "grad_norm": 3.9780361652374268,
      "learning_rate": 0.00022122905027932964,
      "loss": 1.4496,
      "step": 405
    },
    {
      "epoch": 0.45111111111111113,
      "grad_norm": 2.282318353652954,
      "learning_rate": 0.00022078212290502794,
      "loss": 0.6367,
      "step": 406
    },
    {
      "epoch": 0.45222222222222225,
      "grad_norm": 2.8490700721740723,
      "learning_rate": 0.0002203351955307263,
      "loss": 0.6035,
      "step": 407
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 4.088818073272705,
      "learning_rate": 0.0002198882681564246,
      "loss": 1.4562,
      "step": 408
    },
    {
      "epoch": 0.45444444444444443,
      "grad_norm": 4.249924182891846,
      "learning_rate": 0.00021944134078212294,
      "loss": 1.5671,
      "step": 409
    },
    {
      "epoch": 0.45555555555555555,
      "grad_norm": 5.570911884307861,
      "learning_rate": 0.00021899441340782124,
      "loss": 2.0864,
      "step": 410
    },
    {
      "epoch": 0.45666666666666667,
      "grad_norm": 4.317685127258301,
      "learning_rate": 0.0002185474860335196,
      "loss": 1.6884,
      "step": 411
    },
    {
      "epoch": 0.4577777777777778,
      "grad_norm": 4.933940410614014,
      "learning_rate": 0.0002181005586592179,
      "loss": 1.327,
      "step": 412
    },
    {
      "epoch": 0.4588888888888889,
      "grad_norm": 5.66865873336792,
      "learning_rate": 0.00021765363128491624,
      "loss": 2.0775,
      "step": 413
    },
    {
      "epoch": 0.46,
      "grad_norm": 5.0407328605651855,
      "learning_rate": 0.00021720670391061454,
      "loss": 1.6489,
      "step": 414
    },
    {
      "epoch": 0.46111111111111114,
      "grad_norm": 2.7686054706573486,
      "learning_rate": 0.00021675977653631286,
      "loss": 1.5427,
      "step": 415
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 3.869060754776001,
      "learning_rate": 0.00021631284916201119,
      "loss": 0.6595,
      "step": 416
    },
    {
      "epoch": 0.4633333333333333,
      "grad_norm": 4.602139949798584,
      "learning_rate": 0.0002158659217877095,
      "loss": 1.2205,
      "step": 417
    },
    {
      "epoch": 0.46444444444444444,
      "grad_norm": 3.395735263824463,
      "learning_rate": 0.00021541899441340784,
      "loss": 1.357,
      "step": 418
    },
    {
      "epoch": 0.46555555555555556,
      "grad_norm": 2.8067829608917236,
      "learning_rate": 0.00021497206703910616,
      "loss": 0.702,
      "step": 419
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 2.9757001399993896,
      "learning_rate": 0.00021452513966480449,
      "loss": 1.2838,
      "step": 420
    },
    {
      "epoch": 0.4677777777777778,
      "grad_norm": 4.8482160568237305,
      "learning_rate": 0.0002140782122905028,
      "loss": 2.4109,
      "step": 421
    },
    {
      "epoch": 0.4688888888888889,
      "grad_norm": 4.359803199768066,
      "learning_rate": 0.00021363128491620113,
      "loss": 1.4136,
      "step": 422
    },
    {
      "epoch": 0.47,
      "grad_norm": 4.7006916999816895,
      "learning_rate": 0.00021318435754189946,
      "loss": 1.6001,
      "step": 423
    },
    {
      "epoch": 0.4711111111111111,
      "grad_norm": 4.198505401611328,
      "learning_rate": 0.00021273743016759778,
      "loss": 1.2777,
      "step": 424
    },
    {
      "epoch": 0.4722222222222222,
      "grad_norm": 3.0608651638031006,
      "learning_rate": 0.0002122905027932961,
      "loss": 1.3106,
      "step": 425
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 2.697890043258667,
      "learning_rate": 0.00021184357541899443,
      "loss": 0.6956,
      "step": 426
    },
    {
      "epoch": 0.47444444444444445,
      "grad_norm": 3.2964890003204346,
      "learning_rate": 0.00021139664804469276,
      "loss": 1.2216,
      "step": 427
    },
    {
      "epoch": 0.47555555555555556,
      "grad_norm": 3.2206335067749023,
      "learning_rate": 0.00021094972067039108,
      "loss": 1.0496,
      "step": 428
    },
    {
      "epoch": 0.4766666666666667,
      "grad_norm": 3.977961778640747,
      "learning_rate": 0.00021050279329608938,
      "loss": 1.8037,
      "step": 429
    },
    {
      "epoch": 0.4777777777777778,
      "grad_norm": 4.681517124176025,
      "learning_rate": 0.00021005586592178773,
      "loss": 2.1844,
      "step": 430
    },
    {
      "epoch": 0.47888888888888886,
      "grad_norm": 3.0056800842285156,
      "learning_rate": 0.00020960893854748603,
      "loss": 1.2205,
      "step": 431
    },
    {
      "epoch": 0.48,
      "grad_norm": 2.7606701850891113,
      "learning_rate": 0.00020916201117318438,
      "loss": 0.9824,
      "step": 432
    },
    {
      "epoch": 0.4811111111111111,
      "grad_norm": 1.9861876964569092,
      "learning_rate": 0.00020871508379888268,
      "loss": 0.813,
      "step": 433
    },
    {
      "epoch": 0.4822222222222222,
      "grad_norm": 2.0781943798065186,
      "learning_rate": 0.00020826815642458103,
      "loss": 0.8184,
      "step": 434
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 3.9334537982940674,
      "learning_rate": 0.00020782122905027933,
      "loss": 1.2388,
      "step": 435
    },
    {
      "epoch": 0.48444444444444446,
      "grad_norm": 3.2070600986480713,
      "learning_rate": 0.00020737430167597768,
      "loss": 1.0093,
      "step": 436
    },
    {
      "epoch": 0.4855555555555556,
      "grad_norm": 4.296708106994629,
      "learning_rate": 0.00020692737430167598,
      "loss": 1.4994,
      "step": 437
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 4.458811283111572,
      "learning_rate": 0.00020648044692737433,
      "loss": 1.4638,
      "step": 438
    },
    {
      "epoch": 0.48777777777777775,
      "grad_norm": 4.322737693786621,
      "learning_rate": 0.00020603351955307263,
      "loss": 1.3666,
      "step": 439
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 2.6936135292053223,
      "learning_rate": 0.00020558659217877095,
      "loss": 0.9173,
      "step": 440
    },
    {
      "epoch": 0.49,
      "grad_norm": 4.8678693771362305,
      "learning_rate": 0.00020513966480446928,
      "loss": 1.3929,
      "step": 441
    },
    {
      "epoch": 0.4911111111111111,
      "grad_norm": 4.808879375457764,
      "learning_rate": 0.0002046927374301676,
      "loss": 1.5134,
      "step": 442
    },
    {
      "epoch": 0.4922222222222222,
      "grad_norm": 4.339828014373779,
      "learning_rate": 0.00020424581005586593,
      "loss": 1.883,
      "step": 443
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 4.703568458557129,
      "learning_rate": 0.00020379888268156425,
      "loss": 1.6473,
      "step": 444
    },
    {
      "epoch": 0.49444444444444446,
      "grad_norm": 2.954569101333618,
      "learning_rate": 0.00020335195530726257,
      "loss": 0.8568,
      "step": 445
    },
    {
      "epoch": 0.4955555555555556,
      "grad_norm": 5.078303337097168,
      "learning_rate": 0.0002029050279329609,
      "loss": 1.7964,
      "step": 446
    },
    {
      "epoch": 0.49666666666666665,
      "grad_norm": 3.6193039417266846,
      "learning_rate": 0.00020245810055865922,
      "loss": 1.0565,
      "step": 447
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 3.948303461074829,
      "learning_rate": 0.00020201117318435755,
      "loss": 1.354,
      "step": 448
    },
    {
      "epoch": 0.4988888888888889,
      "grad_norm": 4.539860725402832,
      "learning_rate": 0.00020156424581005587,
      "loss": 1.6963,
      "step": 449
    },
    {
      "epoch": 0.5,
      "grad_norm": 5.705753803253174,
      "learning_rate": 0.0002011173184357542,
      "loss": 2.1576,
      "step": 450
    },
    {
      "epoch": 0.5011111111111111,
      "grad_norm": 4.767503261566162,
      "learning_rate": 0.00020067039106145252,
      "loss": 2.1675,
      "step": 451
    },
    {
      "epoch": 0.5022222222222222,
      "grad_norm": 3.004849672317505,
      "learning_rate": 0.00020022346368715085,
      "loss": 0.9114,
      "step": 452
    },
    {
      "epoch": 0.5033333333333333,
      "grad_norm": 4.95789098739624,
      "learning_rate": 0.00019977653631284917,
      "loss": 1.9378,
      "step": 453
    },
    {
      "epoch": 0.5044444444444445,
      "grad_norm": 4.459721565246582,
      "learning_rate": 0.0001993296089385475,
      "loss": 2.2648,
      "step": 454
    },
    {
      "epoch": 0.5055555555555555,
      "grad_norm": 4.7371697425842285,
      "learning_rate": 0.00019888268156424582,
      "loss": 1.3968,
      "step": 455
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 2.693242073059082,
      "learning_rate": 0.00019843575418994415,
      "loss": 0.8249,
      "step": 456
    },
    {
      "epoch": 0.5077777777777778,
      "grad_norm": 3.4749176502227783,
      "learning_rate": 0.00019798882681564247,
      "loss": 1.444,
      "step": 457
    },
    {
      "epoch": 0.5088888888888888,
      "grad_norm": 2.8350462913513184,
      "learning_rate": 0.0001975418994413408,
      "loss": 0.8092,
      "step": 458
    },
    {
      "epoch": 0.51,
      "grad_norm": 3.9661762714385986,
      "learning_rate": 0.00019709497206703912,
      "loss": 0.9937,
      "step": 459
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 2.3005857467651367,
      "learning_rate": 0.00019664804469273744,
      "loss": 0.663,
      "step": 460
    },
    {
      "epoch": 0.5122222222222222,
      "grad_norm": 3.1407082080841064,
      "learning_rate": 0.00019620111731843577,
      "loss": 1.1546,
      "step": 461
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 5.122157573699951,
      "learning_rate": 0.0001957541899441341,
      "loss": 2.2628,
      "step": 462
    },
    {
      "epoch": 0.5144444444444445,
      "grad_norm": 3.4414708614349365,
      "learning_rate": 0.00019530726256983242,
      "loss": 1.8273,
      "step": 463
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 6.664146900177002,
      "learning_rate": 0.00019486033519553074,
      "loss": 2.7477,
      "step": 464
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 3.7917160987854004,
      "learning_rate": 0.00019441340782122907,
      "loss": 2.0282,
      "step": 465
    },
    {
      "epoch": 0.5177777777777778,
      "grad_norm": 4.514062881469727,
      "learning_rate": 0.00019396648044692737,
      "loss": 1.8316,
      "step": 466
    },
    {
      "epoch": 0.5188888888888888,
      "grad_norm": 2.7819743156433105,
      "learning_rate": 0.0001935195530726257,
      "loss": 1.3322,
      "step": 467
    },
    {
      "epoch": 0.52,
      "grad_norm": 4.593914031982422,
      "learning_rate": 0.00019307262569832401,
      "loss": 1.5103,
      "step": 468
    },
    {
      "epoch": 0.5211111111111111,
      "grad_norm": 3.02433180809021,
      "learning_rate": 0.00019262569832402234,
      "loss": 1.05,
      "step": 469
    },
    {
      "epoch": 0.5222222222222223,
      "grad_norm": 2.784816026687622,
      "learning_rate": 0.00019217877094972066,
      "loss": 1.3833,
      "step": 470
    },
    {
      "epoch": 0.5233333333333333,
      "grad_norm": 4.944230556488037,
      "learning_rate": 0.000191731843575419,
      "loss": 2.0902,
      "step": 471
    },
    {
      "epoch": 0.5244444444444445,
      "grad_norm": 3.007924795150757,
      "learning_rate": 0.0001912849162011173,
      "loss": 1.1125,
      "step": 472
    },
    {
      "epoch": 0.5255555555555556,
      "grad_norm": 3.7147419452667236,
      "learning_rate": 0.00019083798882681564,
      "loss": 1.3107,
      "step": 473
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 2.690852165222168,
      "learning_rate": 0.00019039106145251396,
      "loss": 0.9633,
      "step": 474
    },
    {
      "epoch": 0.5277777777777778,
      "grad_norm": 4.949435234069824,
      "learning_rate": 0.0001899441340782123,
      "loss": 2.6751,
      "step": 475
    },
    {
      "epoch": 0.5288888888888889,
      "grad_norm": 4.042517185211182,
      "learning_rate": 0.0001894972067039106,
      "loss": 1.1268,
      "step": 476
    },
    {
      "epoch": 0.53,
      "grad_norm": 3.5580763816833496,
      "learning_rate": 0.00018905027932960894,
      "loss": 1.8548,
      "step": 477
    },
    {
      "epoch": 0.5311111111111111,
      "grad_norm": 4.383810043334961,
      "learning_rate": 0.00018860335195530726,
      "loss": 1.5928,
      "step": 478
    },
    {
      "epoch": 0.5322222222222223,
      "grad_norm": 3.1721270084381104,
      "learning_rate": 0.00018815642458100559,
      "loss": 0.9764,
      "step": 479
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 2.1257894039154053,
      "learning_rate": 0.0001877094972067039,
      "loss": 0.565,
      "step": 480
    },
    {
      "epoch": 0.5344444444444445,
      "grad_norm": 3.880401372909546,
      "learning_rate": 0.00018726256983240224,
      "loss": 1.7433,
      "step": 481
    },
    {
      "epoch": 0.5355555555555556,
      "grad_norm": 3.6695353984832764,
      "learning_rate": 0.00018681564245810056,
      "loss": 1.5471,
      "step": 482
    },
    {
      "epoch": 0.5366666666666666,
      "grad_norm": 4.938419818878174,
      "learning_rate": 0.00018636871508379888,
      "loss": 1.9732,
      "step": 483
    },
    {
      "epoch": 0.5377777777777778,
      "grad_norm": 3.4348678588867188,
      "learning_rate": 0.0001859217877094972,
      "loss": 1.503,
      "step": 484
    },
    {
      "epoch": 0.5388888888888889,
      "grad_norm": 4.549357891082764,
      "learning_rate": 0.00018547486033519553,
      "loss": 1.4295,
      "step": 485
    },
    {
      "epoch": 0.54,
      "grad_norm": 3.0747296810150146,
      "learning_rate": 0.00018502793296089386,
      "loss": 1.2174,
      "step": 486
    },
    {
      "epoch": 0.5411111111111111,
      "grad_norm": 3.880610704421997,
      "learning_rate": 0.00018458100558659218,
      "loss": 2.057,
      "step": 487
    },
    {
      "epoch": 0.5422222222222223,
      "grad_norm": 2.1783289909362793,
      "learning_rate": 0.0001841340782122905,
      "loss": 0.9771,
      "step": 488
    },
    {
      "epoch": 0.5433333333333333,
      "grad_norm": 3.9291741847991943,
      "learning_rate": 0.00018368715083798883,
      "loss": 1.5261,
      "step": 489
    },
    {
      "epoch": 0.5444444444444444,
      "grad_norm": 2.578691244125366,
      "learning_rate": 0.00018324022346368716,
      "loss": 0.85,
      "step": 490
    },
    {
      "epoch": 0.5455555555555556,
      "grad_norm": 3.421980142593384,
      "learning_rate": 0.00018279329608938548,
      "loss": 1.3097,
      "step": 491
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 2.0963633060455322,
      "learning_rate": 0.0001823463687150838,
      "loss": 0.8862,
      "step": 492
    },
    {
      "epoch": 0.5477777777777778,
      "grad_norm": 3.827646255493164,
      "learning_rate": 0.00018189944134078213,
      "loss": 1.1571,
      "step": 493
    },
    {
      "epoch": 0.5488888888888889,
      "grad_norm": 3.9395670890808105,
      "learning_rate": 0.00018145251396648046,
      "loss": 1.1795,
      "step": 494
    },
    {
      "epoch": 0.55,
      "grad_norm": 3.6662495136260986,
      "learning_rate": 0.00018100558659217878,
      "loss": 1.7414,
      "step": 495
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 3.480846643447876,
      "learning_rate": 0.0001805586592178771,
      "loss": 1.1505,
      "step": 496
    },
    {
      "epoch": 0.5522222222222222,
      "grad_norm": 3.076448678970337,
      "learning_rate": 0.00018011173184357543,
      "loss": 1.1081,
      "step": 497
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 3.1306400299072266,
      "learning_rate": 0.00017966480446927375,
      "loss": 0.9837,
      "step": 498
    },
    {
      "epoch": 0.5544444444444444,
      "grad_norm": 3.1836581230163574,
      "learning_rate": 0.00017921787709497208,
      "loss": 1.195,
      "step": 499
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 3.616222381591797,
      "learning_rate": 0.00017877094972067038,
      "loss": 1.153,
      "step": 500
    },
    {
      "epoch": 0.5566666666666666,
      "grad_norm": 3.7530324459075928,
      "learning_rate": 0.0001783240223463687,
      "loss": 1.443,
      "step": 501
    },
    {
      "epoch": 0.5577777777777778,
      "grad_norm": 2.3186140060424805,
      "learning_rate": 0.00017787709497206703,
      "loss": 0.7281,
      "step": 502
    },
    {
      "epoch": 0.5588888888888889,
      "grad_norm": 3.1192328929901123,
      "learning_rate": 0.00017743016759776535,
      "loss": 1.0285,
      "step": 503
    },
    {
      "epoch": 0.56,
      "grad_norm": 6.315285682678223,
      "learning_rate": 0.00017698324022346368,
      "loss": 2.2002,
      "step": 504
    },
    {
      "epoch": 0.5611111111111111,
      "grad_norm": 2.3284459114074707,
      "learning_rate": 0.000176536312849162,
      "loss": 0.6402,
      "step": 505
    },
    {
      "epoch": 0.5622222222222222,
      "grad_norm": 2.993849039077759,
      "learning_rate": 0.00017608938547486033,
      "loss": 1.2658,
      "step": 506
    },
    {
      "epoch": 0.5633333333333334,
      "grad_norm": 4.461086750030518,
      "learning_rate": 0.00017564245810055865,
      "loss": 1.5341,
      "step": 507
    },
    {
      "epoch": 0.5644444444444444,
      "grad_norm": 2.2987492084503174,
      "learning_rate": 0.00017519553072625697,
      "loss": 0.7205,
      "step": 508
    },
    {
      "epoch": 0.5655555555555556,
      "grad_norm": 3.041975975036621,
      "learning_rate": 0.0001747486033519553,
      "loss": 1.1947,
      "step": 509
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 3.9705119132995605,
      "learning_rate": 0.00017430167597765362,
      "loss": 1.6221,
      "step": 510
    },
    {
      "epoch": 0.5677777777777778,
      "grad_norm": 3.028735399246216,
      "learning_rate": 0.00017385474860335195,
      "loss": 0.8616,
      "step": 511
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 3.213134765625,
      "learning_rate": 0.00017340782122905027,
      "loss": 0.9153,
      "step": 512
    },
    {
      "epoch": 0.57,
      "grad_norm": 5.5430426597595215,
      "learning_rate": 0.0001729608938547486,
      "loss": 1.7064,
      "step": 513
    },
    {
      "epoch": 0.5711111111111111,
      "grad_norm": 3.0290141105651855,
      "learning_rate": 0.00017251396648044692,
      "loss": 0.5328,
      "step": 514
    },
    {
      "epoch": 0.5722222222222222,
      "grad_norm": 7.354659557342529,
      "learning_rate": 0.00017206703910614525,
      "loss": 2.5353,
      "step": 515
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 3.0715363025665283,
      "learning_rate": 0.00017162011173184357,
      "loss": 1.3357,
      "step": 516
    },
    {
      "epoch": 0.5744444444444444,
      "grad_norm": 5.257683277130127,
      "learning_rate": 0.0001711731843575419,
      "loss": 2.0986,
      "step": 517
    },
    {
      "epoch": 0.5755555555555556,
      "grad_norm": 3.355302333831787,
      "learning_rate": 0.00017072625698324022,
      "loss": 1.6538,
      "step": 518
    },
    {
      "epoch": 0.5766666666666667,
      "grad_norm": 4.138921737670898,
      "learning_rate": 0.00017027932960893855,
      "loss": 1.5559,
      "step": 519
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 5.308683395385742,
      "learning_rate": 0.00016983240223463687,
      "loss": 2.5769,
      "step": 520
    },
    {
      "epoch": 0.5788888888888889,
      "grad_norm": 2.8735148906707764,
      "learning_rate": 0.0001693854748603352,
      "loss": 1.1746,
      "step": 521
    },
    {
      "epoch": 0.58,
      "grad_norm": 3.2139759063720703,
      "learning_rate": 0.00016893854748603352,
      "loss": 1.2738,
      "step": 522
    },
    {
      "epoch": 0.5811111111111111,
      "grad_norm": 3.504892587661743,
      "learning_rate": 0.00016849162011173184,
      "loss": 1.4678,
      "step": 523
    },
    {
      "epoch": 0.5822222222222222,
      "grad_norm": 3.2661352157592773,
      "learning_rate": 0.00016804469273743017,
      "loss": 1.3803,
      "step": 524
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 4.991024971008301,
      "learning_rate": 0.0001675977653631285,
      "loss": 1.2091,
      "step": 525
    },
    {
      "epoch": 0.5844444444444444,
      "grad_norm": 2.900259494781494,
      "learning_rate": 0.00016715083798882682,
      "loss": 0.9254,
      "step": 526
    },
    {
      "epoch": 0.5855555555555556,
      "grad_norm": 4.723787784576416,
      "learning_rate": 0.00016670391061452514,
      "loss": 1.8238,
      "step": 527
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 4.4301581382751465,
      "learning_rate": 0.00016625698324022347,
      "loss": 1.1669,
      "step": 528
    },
    {
      "epoch": 0.5877777777777777,
      "grad_norm": 4.098521709442139,
      "learning_rate": 0.0001658100558659218,
      "loss": 0.9476,
      "step": 529
    },
    {
      "epoch": 0.5888888888888889,
      "grad_norm": 3.851198196411133,
      "learning_rate": 0.00016536312849162012,
      "loss": 1.3834,
      "step": 530
    },
    {
      "epoch": 0.59,
      "grad_norm": 3.800182342529297,
      "learning_rate": 0.00016491620111731844,
      "loss": 1.9223,
      "step": 531
    },
    {
      "epoch": 0.5911111111111111,
      "grad_norm": 3.2955873012542725,
      "learning_rate": 0.00016446927374301677,
      "loss": 1.5702,
      "step": 532
    },
    {
      "epoch": 0.5922222222222222,
      "grad_norm": 2.8507015705108643,
      "learning_rate": 0.0001640223463687151,
      "loss": 0.8194,
      "step": 533
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 4.559532165527344,
      "learning_rate": 0.00016357541899441342,
      "loss": 1.0788,
      "step": 534
    },
    {
      "epoch": 0.5944444444444444,
      "grad_norm": 2.9955596923828125,
      "learning_rate": 0.00016312849162011174,
      "loss": 0.6957,
      "step": 535
    },
    {
      "epoch": 0.5955555555555555,
      "grad_norm": 1.9262032508850098,
      "learning_rate": 0.00016268156424581007,
      "loss": 0.7673,
      "step": 536
    },
    {
      "epoch": 0.5966666666666667,
      "grad_norm": 2.2690982818603516,
      "learning_rate": 0.0001622346368715084,
      "loss": 0.73,
      "step": 537
    },
    {
      "epoch": 0.5977777777777777,
      "grad_norm": 3.4168143272399902,
      "learning_rate": 0.00016178770949720671,
      "loss": 0.9153,
      "step": 538
    },
    {
      "epoch": 0.5988888888888889,
      "grad_norm": 3.623147487640381,
      "learning_rate": 0.00016134078212290504,
      "loss": 1.9726,
      "step": 539
    },
    {
      "epoch": 0.6,
      "grad_norm": 3.669076442718506,
      "learning_rate": 0.00016089385474860336,
      "loss": 1.892,
      "step": 540
    },
    {
      "epoch": 0.6011111111111112,
      "grad_norm": 4.459663391113281,
      "learning_rate": 0.0001604469273743017,
      "loss": 2.0881,
      "step": 541
    },
    {
      "epoch": 0.6022222222222222,
      "grad_norm": 2.765329122543335,
      "learning_rate": 0.00016,
      "loss": 1.2443,
      "step": 542
    },
    {
      "epoch": 0.6033333333333334,
      "grad_norm": 3.424025535583496,
      "learning_rate": 0.00015955307262569834,
      "loss": 1.0838,
      "step": 543
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 3.2491841316223145,
      "learning_rate": 0.00015910614525139666,
      "loss": 1.7615,
      "step": 544
    },
    {
      "epoch": 0.6055555555555555,
      "grad_norm": 3.0452797412872314,
      "learning_rate": 0.000158659217877095,
      "loss": 1.5711,
      "step": 545
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 4.622325897216797,
      "learning_rate": 0.0001582122905027933,
      "loss": 2.0219,
      "step": 546
    },
    {
      "epoch": 0.6077777777777778,
      "grad_norm": 1.3547860383987427,
      "learning_rate": 0.0001577653631284916,
      "loss": 0.3556,
      "step": 547
    },
    {
      "epoch": 0.6088888888888889,
      "grad_norm": 3.242140531539917,
      "learning_rate": 0.00015731843575418993,
      "loss": 1.5512,
      "step": 548
    },
    {
      "epoch": 0.61,
      "grad_norm": 2.555124521255493,
      "learning_rate": 0.00015687150837988826,
      "loss": 1.1398,
      "step": 549
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 3.346846342086792,
      "learning_rate": 0.00015642458100558658,
      "loss": 1.3703,
      "step": 550
    },
    {
      "epoch": 0.6122222222222222,
      "grad_norm": 3.146183967590332,
      "learning_rate": 0.0001559776536312849,
      "loss": 1.8172,
      "step": 551
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 5.744021892547607,
      "learning_rate": 0.00015553072625698323,
      "loss": 1.7034,
      "step": 552
    },
    {
      "epoch": 0.6144444444444445,
      "grad_norm": 3.5769176483154297,
      "learning_rate": 0.00015508379888268156,
      "loss": 1.8877,
      "step": 553
    },
    {
      "epoch": 0.6155555555555555,
      "grad_norm": 3.443875551223755,
      "learning_rate": 0.00015463687150837988,
      "loss": 1.1876,
      "step": 554
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 2.733896255493164,
      "learning_rate": 0.0001541899441340782,
      "loss": 0.5112,
      "step": 555
    },
    {
      "epoch": 0.6177777777777778,
      "grad_norm": 3.2098348140716553,
      "learning_rate": 0.00015374301675977653,
      "loss": 1.2906,
      "step": 556
    },
    {
      "epoch": 0.6188888888888889,
      "grad_norm": 4.684952735900879,
      "learning_rate": 0.00015329608938547486,
      "loss": 2.5,
      "step": 557
    },
    {
      "epoch": 0.62,
      "grad_norm": 3.0638856887817383,
      "learning_rate": 0.00015284916201117318,
      "loss": 0.7099,
      "step": 558
    },
    {
      "epoch": 0.6211111111111111,
      "grad_norm": 2.3900229930877686,
      "learning_rate": 0.0001524022346368715,
      "loss": 0.8869,
      "step": 559
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 3.991628885269165,
      "learning_rate": 0.00015195530726256983,
      "loss": 1.6051,
      "step": 560
    },
    {
      "epoch": 0.6233333333333333,
      "grad_norm": 3.199223279953003,
      "learning_rate": 0.00015150837988826815,
      "loss": 1.3014,
      "step": 561
    },
    {
      "epoch": 0.6244444444444445,
      "grad_norm": 6.567105293273926,
      "learning_rate": 0.00015106145251396648,
      "loss": 1.3662,
      "step": 562
    },
    {
      "epoch": 0.6255555555555555,
      "grad_norm": 2.2834742069244385,
      "learning_rate": 0.0001506145251396648,
      "loss": 0.7248,
      "step": 563
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 3.346742630004883,
      "learning_rate": 0.00015016759776536313,
      "loss": 1.1,
      "step": 564
    },
    {
      "epoch": 0.6277777777777778,
      "grad_norm": 4.566150188446045,
      "learning_rate": 0.00014972067039106145,
      "loss": 1.951,
      "step": 565
    },
    {
      "epoch": 0.6288888888888889,
      "grad_norm": 3.8907876014709473,
      "learning_rate": 0.00014927374301675978,
      "loss": 1.7915,
      "step": 566
    },
    {
      "epoch": 0.63,
      "grad_norm": 3.9017698764801025,
      "learning_rate": 0.0001488268156424581,
      "loss": 1.3312,
      "step": 567
    },
    {
      "epoch": 0.6311111111111111,
      "grad_norm": 3.280416965484619,
      "learning_rate": 0.00014837988826815643,
      "loss": 1.158,
      "step": 568
    },
    {
      "epoch": 0.6322222222222222,
      "grad_norm": 3.180821418762207,
      "learning_rate": 0.00014793296089385475,
      "loss": 0.7849,
      "step": 569
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 3.233377456665039,
      "learning_rate": 0.00014748603351955308,
      "loss": 0.7597,
      "step": 570
    },
    {
      "epoch": 0.6344444444444445,
      "grad_norm": 2.3141026496887207,
      "learning_rate": 0.0001470391061452514,
      "loss": 0.8585,
      "step": 571
    },
    {
      "epoch": 0.6355555555555555,
      "grad_norm": 3.515099048614502,
      "learning_rate": 0.00014659217877094973,
      "loss": 1.2594,
      "step": 572
    },
    {
      "epoch": 0.6366666666666667,
      "grad_norm": 4.041418075561523,
      "learning_rate": 0.00014614525139664805,
      "loss": 1.7998,
      "step": 573
    },
    {
      "epoch": 0.6377777777777778,
      "grad_norm": 2.7985763549804688,
      "learning_rate": 0.00014569832402234638,
      "loss": 0.881,
      "step": 574
    },
    {
      "epoch": 0.6388888888888888,
      "grad_norm": 2.4731862545013428,
      "learning_rate": 0.0001452513966480447,
      "loss": 0.7386,
      "step": 575
    },
    {
      "epoch": 0.64,
      "grad_norm": 2.8067915439605713,
      "learning_rate": 0.00014480446927374302,
      "loss": 1.2505,
      "step": 576
    },
    {
      "epoch": 0.6411111111111111,
      "grad_norm": 2.664181709289551,
      "learning_rate": 0.00014435754189944135,
      "loss": 0.7522,
      "step": 577
    },
    {
      "epoch": 0.6422222222222222,
      "grad_norm": 5.019392490386963,
      "learning_rate": 0.00014391061452513967,
      "loss": 1.6348,
      "step": 578
    },
    {
      "epoch": 0.6433333333333333,
      "grad_norm": 3.218597650527954,
      "learning_rate": 0.000143463687150838,
      "loss": 0.7567,
      "step": 579
    },
    {
      "epoch": 0.6444444444444445,
      "grad_norm": 3.6393074989318848,
      "learning_rate": 0.00014301675977653632,
      "loss": 0.9241,
      "step": 580
    },
    {
      "epoch": 0.6455555555555555,
      "grad_norm": 3.6428446769714355,
      "learning_rate": 0.00014256983240223465,
      "loss": 1.8826,
      "step": 581
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 4.780536651611328,
      "learning_rate": 0.00014212290502793297,
      "loss": 1.7846,
      "step": 582
    },
    {
      "epoch": 0.6477777777777778,
      "grad_norm": 1.853205680847168,
      "learning_rate": 0.0001416759776536313,
      "loss": 0.6233,
      "step": 583
    },
    {
      "epoch": 0.6488888888888888,
      "grad_norm": 2.8558669090270996,
      "learning_rate": 0.00014122905027932962,
      "loss": 1.0094,
      "step": 584
    },
    {
      "epoch": 0.65,
      "grad_norm": 2.4842400550842285,
      "learning_rate": 0.00014078212290502795,
      "loss": 0.7809,
      "step": 585
    },
    {
      "epoch": 0.6511111111111111,
      "grad_norm": 3.3915464878082275,
      "learning_rate": 0.00014033519553072627,
      "loss": 1.2724,
      "step": 586
    },
    {
      "epoch": 0.6522222222222223,
      "grad_norm": 3.0322184562683105,
      "learning_rate": 0.0001398882681564246,
      "loss": 1.1699,
      "step": 587
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 2.682934284210205,
      "learning_rate": 0.00013944134078212292,
      "loss": 1.0709,
      "step": 588
    },
    {
      "epoch": 0.6544444444444445,
      "grad_norm": 2.76462984085083,
      "learning_rate": 0.00013899441340782125,
      "loss": 0.9083,
      "step": 589
    },
    {
      "epoch": 0.6555555555555556,
      "grad_norm": 3.037336587905884,
      "learning_rate": 0.00013854748603351957,
      "loss": 0.9244,
      "step": 590
    },
    {
      "epoch": 0.6566666666666666,
      "grad_norm": 2.109992742538452,
      "learning_rate": 0.0001381005586592179,
      "loss": 0.5265,
      "step": 591
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 5.074723243713379,
      "learning_rate": 0.00013765363128491622,
      "loss": 1.3827,
      "step": 592
    },
    {
      "epoch": 0.6588888888888889,
      "grad_norm": 4.490645885467529,
      "learning_rate": 0.00013720670391061454,
      "loss": 1.3606,
      "step": 593
    },
    {
      "epoch": 0.66,
      "grad_norm": 3.5211713314056396,
      "learning_rate": 0.00013675977653631284,
      "loss": 0.7635,
      "step": 594
    },
    {
      "epoch": 0.6611111111111111,
      "grad_norm": 4.549127101898193,
      "learning_rate": 0.00013631284916201117,
      "loss": 1.8281,
      "step": 595
    },
    {
      "epoch": 0.6622222222222223,
      "grad_norm": 4.11577033996582,
      "learning_rate": 0.0001358659217877095,
      "loss": 1.3999,
      "step": 596
    },
    {
      "epoch": 0.6633333333333333,
      "grad_norm": 4.0047502517700195,
      "learning_rate": 0.00013541899441340782,
      "loss": 1.1503,
      "step": 597
    },
    {
      "epoch": 0.6644444444444444,
      "grad_norm": 3.193779468536377,
      "learning_rate": 0.00013497206703910614,
      "loss": 1.0961,
      "step": 598
    },
    {
      "epoch": 0.6655555555555556,
      "grad_norm": 5.2115583419799805,
      "learning_rate": 0.00013452513966480446,
      "loss": 1.9703,
      "step": 599
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 4.5173258781433105,
      "learning_rate": 0.0001340782122905028,
      "loss": 1.5525,
      "step": 600
    },
    {
      "epoch": 0.6677777777777778,
      "grad_norm": 2.6869611740112305,
      "learning_rate": 0.00013363128491620111,
      "loss": 0.8329,
      "step": 601
    },
    {
      "epoch": 0.6688888888888889,
      "grad_norm": 5.173563003540039,
      "learning_rate": 0.00013318435754189944,
      "loss": 1.8977,
      "step": 602
    },
    {
      "epoch": 0.67,
      "grad_norm": 3.0859885215759277,
      "learning_rate": 0.00013273743016759776,
      "loss": 1.2111,
      "step": 603
    },
    {
      "epoch": 0.6711111111111111,
      "grad_norm": 4.278791904449463,
      "learning_rate": 0.0001322905027932961,
      "loss": 1.9727,
      "step": 604
    },
    {
      "epoch": 0.6722222222222223,
      "grad_norm": 3.591649055480957,
      "learning_rate": 0.0001318435754189944,
      "loss": 1.6019,
      "step": 605
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 8.005546569824219,
      "learning_rate": 0.00013139664804469274,
      "loss": 2.9725,
      "step": 606
    },
    {
      "epoch": 0.6744444444444444,
      "grad_norm": 4.428956985473633,
      "learning_rate": 0.00013094972067039106,
      "loss": 1.6597,
      "step": 607
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 3.928454637527466,
      "learning_rate": 0.0001305027932960894,
      "loss": 1.7606,
      "step": 608
    },
    {
      "epoch": 0.6766666666666666,
      "grad_norm": 3.5955374240875244,
      "learning_rate": 0.0001300558659217877,
      "loss": 1.1088,
      "step": 609
    },
    {
      "epoch": 0.6777777777777778,
      "grad_norm": 5.720571041107178,
      "learning_rate": 0.00012960893854748604,
      "loss": 2.3613,
      "step": 610
    },
    {
      "epoch": 0.6788888888888889,
      "grad_norm": 2.5548620223999023,
      "learning_rate": 0.00012916201117318436,
      "loss": 1.3631,
      "step": 611
    },
    {
      "epoch": 0.68,
      "grad_norm": 3.0225443840026855,
      "learning_rate": 0.00012871508379888269,
      "loss": 1.0912,
      "step": 612
    },
    {
      "epoch": 0.6811111111111111,
      "grad_norm": 3.295347213745117,
      "learning_rate": 0.000128268156424581,
      "loss": 1.2142,
      "step": 613
    },
    {
      "epoch": 0.6822222222222222,
      "grad_norm": 6.718338966369629,
      "learning_rate": 0.00012782122905027933,
      "loss": 2.4853,
      "step": 614
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 2.8793766498565674,
      "learning_rate": 0.00012737430167597766,
      "loss": 0.9907,
      "step": 615
    },
    {
      "epoch": 0.6844444444444444,
      "grad_norm": 2.1346113681793213,
      "learning_rate": 0.00012692737430167598,
      "loss": 0.7155,
      "step": 616
    },
    {
      "epoch": 0.6855555555555556,
      "grad_norm": 3.0243144035339355,
      "learning_rate": 0.0001264804469273743,
      "loss": 1.2262,
      "step": 617
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 3.006108522415161,
      "learning_rate": 0.00012603351955307263,
      "loss": 1.0489,
      "step": 618
    },
    {
      "epoch": 0.6877777777777778,
      "grad_norm": 2.7638087272644043,
      "learning_rate": 0.00012558659217877096,
      "loss": 0.9529,
      "step": 619
    },
    {
      "epoch": 0.6888888888888889,
      "grad_norm": 2.601201295852661,
      "learning_rate": 0.00012513966480446928,
      "loss": 0.8824,
      "step": 620
    },
    {
      "epoch": 0.69,
      "grad_norm": 2.6221516132354736,
      "learning_rate": 0.0001246927374301676,
      "loss": 0.9462,
      "step": 621
    },
    {
      "epoch": 0.6911111111111111,
      "grad_norm": 3.6022965908050537,
      "learning_rate": 0.00012424581005586593,
      "loss": 1.8976,
      "step": 622
    },
    {
      "epoch": 0.6922222222222222,
      "grad_norm": 3.511023998260498,
      "learning_rate": 0.00012379888268156426,
      "loss": 1.4365,
      "step": 623
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 3.5157389640808105,
      "learning_rate": 0.00012335195530726258,
      "loss": 1.6598,
      "step": 624
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 2.107045888900757,
      "learning_rate": 0.0001229050279329609,
      "loss": 0.8358,
      "step": 625
    },
    {
      "epoch": 0.6955555555555556,
      "grad_norm": 3.346082925796509,
      "learning_rate": 0.00012245810055865923,
      "loss": 1.5378,
      "step": 626
    },
    {
      "epoch": 0.6966666666666667,
      "grad_norm": 3.2636497020721436,
      "learning_rate": 0.00012201117318435756,
      "loss": 1.1302,
      "step": 627
    },
    {
      "epoch": 0.6977777777777778,
      "grad_norm": 2.4100522994995117,
      "learning_rate": 0.00012156424581005588,
      "loss": 0.834,
      "step": 628
    },
    {
      "epoch": 0.6988888888888889,
      "grad_norm": 2.8196628093719482,
      "learning_rate": 0.0001211173184357542,
      "loss": 1.0549,
      "step": 629
    },
    {
      "epoch": 0.7,
      "grad_norm": 2.458043098449707,
      "learning_rate": 0.00012067039106145253,
      "loss": 1.2537,
      "step": 630
    },
    {
      "epoch": 0.7011111111111111,
      "grad_norm": 2.3609981536865234,
      "learning_rate": 0.00012022346368715085,
      "loss": 0.7233,
      "step": 631
    },
    {
      "epoch": 0.7022222222222222,
      "grad_norm": 2.7002813816070557,
      "learning_rate": 0.00011977653631284918,
      "loss": 0.6096,
      "step": 632
    },
    {
      "epoch": 0.7033333333333334,
      "grad_norm": 2.9790070056915283,
      "learning_rate": 0.0001193296089385475,
      "loss": 1.2092,
      "step": 633
    },
    {
      "epoch": 0.7044444444444444,
      "grad_norm": 3.598569631576538,
      "learning_rate": 0.00011888268156424583,
      "loss": 1.4416,
      "step": 634
    },
    {
      "epoch": 0.7055555555555556,
      "grad_norm": 2.9327263832092285,
      "learning_rate": 0.00011843575418994415,
      "loss": 1.5201,
      "step": 635
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 2.770737886428833,
      "learning_rate": 0.00011798882681564248,
      "loss": 0.735,
      "step": 636
    },
    {
      "epoch": 0.7077777777777777,
      "grad_norm": 2.6937873363494873,
      "learning_rate": 0.0001175418994413408,
      "loss": 1.0552,
      "step": 637
    },
    {
      "epoch": 0.7088888888888889,
      "grad_norm": 3.536719799041748,
      "learning_rate": 0.00011709497206703913,
      "loss": 1.2147,
      "step": 638
    },
    {
      "epoch": 0.71,
      "grad_norm": 3.504875421524048,
      "learning_rate": 0.00011664804469273745,
      "loss": 1.6697,
      "step": 639
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 4.475005149841309,
      "learning_rate": 0.00011620111731843578,
      "loss": 1.584,
      "step": 640
    },
    {
      "epoch": 0.7122222222222222,
      "grad_norm": 2.7245397567749023,
      "learning_rate": 0.00011575418994413407,
      "loss": 1.1068,
      "step": 641
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 3.2163219451904297,
      "learning_rate": 0.0001153072625698324,
      "loss": 1.2286,
      "step": 642
    },
    {
      "epoch": 0.7144444444444444,
      "grad_norm": 2.31982421875,
      "learning_rate": 0.00011486033519553072,
      "loss": 1.0943,
      "step": 643
    },
    {
      "epoch": 0.7155555555555555,
      "grad_norm": 3.4693636894226074,
      "learning_rate": 0.00011441340782122905,
      "loss": 1.1501,
      "step": 644
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 3.408534049987793,
      "learning_rate": 0.00011396648044692737,
      "loss": 1.7696,
      "step": 645
    },
    {
      "epoch": 0.7177777777777777,
      "grad_norm": 3.414240837097168,
      "learning_rate": 0.0001135195530726257,
      "loss": 1.2226,
      "step": 646
    },
    {
      "epoch": 0.7188888888888889,
      "grad_norm": 3.1824495792388916,
      "learning_rate": 0.00011307262569832402,
      "loss": 1.6346,
      "step": 647
    },
    {
      "epoch": 0.72,
      "grad_norm": 2.5616960525512695,
      "learning_rate": 0.00011262569832402235,
      "loss": 1.2359,
      "step": 648
    },
    {
      "epoch": 0.7211111111111111,
      "grad_norm": 3.9767494201660156,
      "learning_rate": 0.00011217877094972067,
      "loss": 1.5238,
      "step": 649
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 4.195959568023682,
      "learning_rate": 0.000111731843575419,
      "loss": 1.5282,
      "step": 650
    },
    {
      "epoch": 0.7233333333333334,
      "grad_norm": 4.323666572570801,
      "learning_rate": 0.00011128491620111732,
      "loss": 2.3181,
      "step": 651
    },
    {
      "epoch": 0.7244444444444444,
      "grad_norm": 3.336482524871826,
      "learning_rate": 0.00011083798882681565,
      "loss": 1.4525,
      "step": 652
    },
    {
      "epoch": 0.7255555555555555,
      "grad_norm": 5.098277568817139,
      "learning_rate": 0.00011039106145251397,
      "loss": 2.3444,
      "step": 653
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 5.242316246032715,
      "learning_rate": 0.0001099441340782123,
      "loss": 1.4621,
      "step": 654
    },
    {
      "epoch": 0.7277777777777777,
      "grad_norm": 2.181295156478882,
      "learning_rate": 0.00010949720670391062,
      "loss": 0.8817,
      "step": 655
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 4.185325622558594,
      "learning_rate": 0.00010905027932960894,
      "loss": 1.4108,
      "step": 656
    },
    {
      "epoch": 0.73,
      "grad_norm": 2.5826449394226074,
      "learning_rate": 0.00010860335195530727,
      "loss": 1.0542,
      "step": 657
    },
    {
      "epoch": 0.7311111111111112,
      "grad_norm": 3.600538730621338,
      "learning_rate": 0.00010815642458100559,
      "loss": 1.0484,
      "step": 658
    },
    {
      "epoch": 0.7322222222222222,
      "grad_norm": 3.781038522720337,
      "learning_rate": 0.00010770949720670392,
      "loss": 1.6905,
      "step": 659
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 2.6771771907806396,
      "learning_rate": 0.00010726256983240224,
      "loss": 1.4052,
      "step": 660
    },
    {
      "epoch": 0.7344444444444445,
      "grad_norm": 4.639039516448975,
      "learning_rate": 0.00010681564245810057,
      "loss": 1.1811,
      "step": 661
    },
    {
      "epoch": 0.7355555555555555,
      "grad_norm": 3.6592485904693604,
      "learning_rate": 0.00010636871508379889,
      "loss": 1.4958,
      "step": 662
    },
    {
      "epoch": 0.7366666666666667,
      "grad_norm": 2.2769505977630615,
      "learning_rate": 0.00010592178770949722,
      "loss": 0.8871,
      "step": 663
    },
    {
      "epoch": 0.7377777777777778,
      "grad_norm": 4.89472770690918,
      "learning_rate": 0.00010547486033519554,
      "loss": 1.7393,
      "step": 664
    },
    {
      "epoch": 0.7388888888888889,
      "grad_norm": 1.9030897617340088,
      "learning_rate": 0.00010502793296089387,
      "loss": 0.6728,
      "step": 665
    },
    {
      "epoch": 0.74,
      "grad_norm": 1.8392930030822754,
      "learning_rate": 0.00010458100558659219,
      "loss": 0.8279,
      "step": 666
    },
    {
      "epoch": 0.7411111111111112,
      "grad_norm": 3.290851593017578,
      "learning_rate": 0.00010413407821229052,
      "loss": 1.2129,
      "step": 667
    },
    {
      "epoch": 0.7422222222222222,
      "grad_norm": 2.4270894527435303,
      "learning_rate": 0.00010368715083798884,
      "loss": 0.4899,
      "step": 668
    },
    {
      "epoch": 0.7433333333333333,
      "grad_norm": 2.4765403270721436,
      "learning_rate": 0.00010324022346368716,
      "loss": 0.8655,
      "step": 669
    },
    {
      "epoch": 0.7444444444444445,
      "grad_norm": 4.012538433074951,
      "learning_rate": 0.00010279329608938548,
      "loss": 2.0459,
      "step": 670
    },
    {
      "epoch": 0.7455555555555555,
      "grad_norm": 2.7275843620300293,
      "learning_rate": 0.0001023463687150838,
      "loss": 1.43,
      "step": 671
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 2.686953544616699,
      "learning_rate": 0.00010189944134078212,
      "loss": 0.9498,
      "step": 672
    },
    {
      "epoch": 0.7477777777777778,
      "grad_norm": 3.797987461090088,
      "learning_rate": 0.00010145251396648045,
      "loss": 1.4519,
      "step": 673
    },
    {
      "epoch": 0.7488888888888889,
      "grad_norm": 2.5647525787353516,
      "learning_rate": 0.00010100558659217877,
      "loss": 1.1954,
      "step": 674
    },
    {
      "epoch": 0.75,
      "grad_norm": 3.9258100986480713,
      "learning_rate": 0.0001005586592178771,
      "loss": 1.7568,
      "step": 675
    },
    {
      "epoch": 0.7511111111111111,
      "grad_norm": 3.7229273319244385,
      "learning_rate": 0.00010011173184357542,
      "loss": 1.5602,
      "step": 676
    },
    {
      "epoch": 0.7522222222222222,
      "grad_norm": 5.672342777252197,
      "learning_rate": 9.966480446927375e-05,
      "loss": 2.5048,
      "step": 677
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 3.863764762878418,
      "learning_rate": 9.921787709497207e-05,
      "loss": 1.5658,
      "step": 678
    },
    {
      "epoch": 0.7544444444444445,
      "grad_norm": 2.9476916790008545,
      "learning_rate": 9.87709497206704e-05,
      "loss": 1.2409,
      "step": 679
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 2.436434745788574,
      "learning_rate": 9.832402234636872e-05,
      "loss": 0.9963,
      "step": 680
    },
    {
      "epoch": 0.7566666666666667,
      "grad_norm": 4.077342987060547,
      "learning_rate": 9.787709497206705e-05,
      "loss": 1.0905,
      "step": 681
    },
    {
      "epoch": 0.7577777777777778,
      "grad_norm": 2.6263630390167236,
      "learning_rate": 9.743016759776537e-05,
      "loss": 0.9805,
      "step": 682
    },
    {
      "epoch": 0.7588888888888888,
      "grad_norm": 2.3604018688201904,
      "learning_rate": 9.698324022346368e-05,
      "loss": 0.6356,
      "step": 683
    },
    {
      "epoch": 0.76,
      "grad_norm": 2.660698413848877,
      "learning_rate": 9.653631284916201e-05,
      "loss": 0.9401,
      "step": 684
    },
    {
      "epoch": 0.7611111111111111,
      "grad_norm": 2.78410267829895,
      "learning_rate": 9.608938547486033e-05,
      "loss": 0.661,
      "step": 685
    },
    {
      "epoch": 0.7622222222222222,
      "grad_norm": 3.7020013332366943,
      "learning_rate": 9.564245810055866e-05,
      "loss": 0.898,
      "step": 686
    },
    {
      "epoch": 0.7633333333333333,
      "grad_norm": 3.464754819869995,
      "learning_rate": 9.519553072625698e-05,
      "loss": 1.2995,
      "step": 687
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 2.468215227127075,
      "learning_rate": 9.47486033519553e-05,
      "loss": 0.6763,
      "step": 688
    },
    {
      "epoch": 0.7655555555555555,
      "grad_norm": 3.886690616607666,
      "learning_rate": 9.430167597765363e-05,
      "loss": 1.5413,
      "step": 689
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 3.5890510082244873,
      "learning_rate": 9.385474860335196e-05,
      "loss": 1.1054,
      "step": 690
    },
    {
      "epoch": 0.7677777777777778,
      "grad_norm": 3.6790449619293213,
      "learning_rate": 9.340782122905028e-05,
      "loss": 1.4699,
      "step": 691
    },
    {
      "epoch": 0.7688888888888888,
      "grad_norm": 2.670865774154663,
      "learning_rate": 9.29608938547486e-05,
      "loss": 1.3285,
      "step": 692
    },
    {
      "epoch": 0.77,
      "grad_norm": 3.7150537967681885,
      "learning_rate": 9.251396648044693e-05,
      "loss": 1.1413,
      "step": 693
    },
    {
      "epoch": 0.7711111111111111,
      "grad_norm": 7.61472225189209,
      "learning_rate": 9.206703910614525e-05,
      "loss": 1.9396,
      "step": 694
    },
    {
      "epoch": 0.7722222222222223,
      "grad_norm": 4.13601541519165,
      "learning_rate": 9.162011173184358e-05,
      "loss": 1.123,
      "step": 695
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 4.362366676330566,
      "learning_rate": 9.11731843575419e-05,
      "loss": 1.5844,
      "step": 696
    },
    {
      "epoch": 0.7744444444444445,
      "grad_norm": 3.3437142372131348,
      "learning_rate": 9.072625698324023e-05,
      "loss": 0.7133,
      "step": 697
    },
    {
      "epoch": 0.7755555555555556,
      "grad_norm": 3.6488265991210938,
      "learning_rate": 9.027932960893855e-05,
      "loss": 1.4231,
      "step": 698
    },
    {
      "epoch": 0.7766666666666666,
      "grad_norm": 2.653496503829956,
      "learning_rate": 8.983240223463688e-05,
      "loss": 0.7148,
      "step": 699
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 3.395660638809204,
      "learning_rate": 8.938547486033519e-05,
      "loss": 1.4866,
      "step": 700
    },
    {
      "epoch": 0.7788888888888889,
      "grad_norm": 3.9383599758148193,
      "learning_rate": 8.893854748603351e-05,
      "loss": 1.4165,
      "step": 701
    },
    {
      "epoch": 0.78,
      "grad_norm": 3.7050225734710693,
      "learning_rate": 8.849162011173184e-05,
      "loss": 1.0709,
      "step": 702
    },
    {
      "epoch": 0.7811111111111111,
      "grad_norm": 2.5929794311523438,
      "learning_rate": 8.804469273743016e-05,
      "loss": 1.0352,
      "step": 703
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 2.7147583961486816,
      "learning_rate": 8.759776536312849e-05,
      "loss": 0.637,
      "step": 704
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 4.080643653869629,
      "learning_rate": 8.715083798882681e-05,
      "loss": 1.1781,
      "step": 705
    },
    {
      "epoch": 0.7844444444444445,
      "grad_norm": 4.587851524353027,
      "learning_rate": 8.670391061452514e-05,
      "loss": 1.66,
      "step": 706
    },
    {
      "epoch": 0.7855555555555556,
      "grad_norm": 3.664957284927368,
      "learning_rate": 8.625698324022346e-05,
      "loss": 1.4651,
      "step": 707
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 3.105710029602051,
      "learning_rate": 8.581005586592179e-05,
      "loss": 1.2131,
      "step": 708
    },
    {
      "epoch": 0.7877777777777778,
      "grad_norm": 2.9316155910491943,
      "learning_rate": 8.536312849162011e-05,
      "loss": 1.2462,
      "step": 709
    },
    {
      "epoch": 0.7888888888888889,
      "grad_norm": 3.7060656547546387,
      "learning_rate": 8.491620111731844e-05,
      "loss": 1.0367,
      "step": 710
    },
    {
      "epoch": 0.79,
      "grad_norm": 3.205615758895874,
      "learning_rate": 8.446927374301676e-05,
      "loss": 1.324,
      "step": 711
    },
    {
      "epoch": 0.7911111111111111,
      "grad_norm": 3.5098633766174316,
      "learning_rate": 8.402234636871508e-05,
      "loss": 1.7559,
      "step": 712
    },
    {
      "epoch": 0.7922222222222223,
      "grad_norm": 4.6356306076049805,
      "learning_rate": 8.357541899441341e-05,
      "loss": 2.1924,
      "step": 713
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 3.177180528640747,
      "learning_rate": 8.312849162011173e-05,
      "loss": 1.2508,
      "step": 714
    },
    {
      "epoch": 0.7944444444444444,
      "grad_norm": 4.288872718811035,
      "learning_rate": 8.268156424581006e-05,
      "loss": 1.5468,
      "step": 715
    },
    {
      "epoch": 0.7955555555555556,
      "grad_norm": 3.437873363494873,
      "learning_rate": 8.223463687150838e-05,
      "loss": 1.1473,
      "step": 716
    },
    {
      "epoch": 0.7966666666666666,
      "grad_norm": 4.207977771759033,
      "learning_rate": 8.178770949720671e-05,
      "loss": 0.864,
      "step": 717
    },
    {
      "epoch": 0.7977777777777778,
      "grad_norm": 2.2324156761169434,
      "learning_rate": 8.134078212290503e-05,
      "loss": 0.6758,
      "step": 718
    },
    {
      "epoch": 0.7988888888888889,
      "grad_norm": 2.7081363201141357,
      "learning_rate": 8.089385474860336e-05,
      "loss": 1.0867,
      "step": 719
    },
    {
      "epoch": 0.8,
      "grad_norm": 3.233891725540161,
      "learning_rate": 8.044692737430168e-05,
      "loss": 1.0217,
      "step": 720
    },
    {
      "epoch": 0.8011111111111111,
      "grad_norm": 3.4947731494903564,
      "learning_rate": 8e-05,
      "loss": 0.9974,
      "step": 721
    },
    {
      "epoch": 0.8022222222222222,
      "grad_norm": 2.205308437347412,
      "learning_rate": 7.955307262569833e-05,
      "loss": 0.7406,
      "step": 722
    },
    {
      "epoch": 0.8033333333333333,
      "grad_norm": 3.42423415184021,
      "learning_rate": 7.910614525139666e-05,
      "loss": 1.2043,
      "step": 723
    },
    {
      "epoch": 0.8044444444444444,
      "grad_norm": 2.8122599124908447,
      "learning_rate": 7.865921787709497e-05,
      "loss": 0.5938,
      "step": 724
    },
    {
      "epoch": 0.8055555555555556,
      "grad_norm": 3.1350722312927246,
      "learning_rate": 7.821229050279329e-05,
      "loss": 1.296,
      "step": 725
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 2.897282123565674,
      "learning_rate": 7.776536312849162e-05,
      "loss": 1.1974,
      "step": 726
    },
    {
      "epoch": 0.8077777777777778,
      "grad_norm": 5.6201958656311035,
      "learning_rate": 7.731843575418994e-05,
      "loss": 2.9691,
      "step": 727
    },
    {
      "epoch": 0.8088888888888889,
      "grad_norm": 5.999977111816406,
      "learning_rate": 7.687150837988827e-05,
      "loss": 1.3804,
      "step": 728
    },
    {
      "epoch": 0.81,
      "grad_norm": 4.122852802276611,
      "learning_rate": 7.642458100558659e-05,
      "loss": 1.021,
      "step": 729
    },
    {
      "epoch": 0.8111111111111111,
      "grad_norm": 3.248612642288208,
      "learning_rate": 7.597765363128491e-05,
      "loss": 1.0816,
      "step": 730
    },
    {
      "epoch": 0.8122222222222222,
      "grad_norm": 3.566041946411133,
      "learning_rate": 7.553072625698324e-05,
      "loss": 1.2746,
      "step": 731
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 1.9816935062408447,
      "learning_rate": 7.508379888268156e-05,
      "loss": 0.6806,
      "step": 732
    },
    {
      "epoch": 0.8144444444444444,
      "grad_norm": 3.069567918777466,
      "learning_rate": 7.463687150837989e-05,
      "loss": 1.4381,
      "step": 733
    },
    {
      "epoch": 0.8155555555555556,
      "grad_norm": 3.485628366470337,
      "learning_rate": 7.418994413407821e-05,
      "loss": 1.327,
      "step": 734
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 3.074195384979248,
      "learning_rate": 7.374301675977654e-05,
      "loss": 0.9255,
      "step": 735
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 4.094756126403809,
      "learning_rate": 7.329608938547486e-05,
      "loss": 1.8924,
      "step": 736
    },
    {
      "epoch": 0.8188888888888889,
      "grad_norm": 2.8394157886505127,
      "learning_rate": 7.284916201117319e-05,
      "loss": 1.6152,
      "step": 737
    },
    {
      "epoch": 0.82,
      "grad_norm": 2.5208678245544434,
      "learning_rate": 7.240223463687151e-05,
      "loss": 0.7912,
      "step": 738
    },
    {
      "epoch": 0.8211111111111111,
      "grad_norm": 2.844087600708008,
      "learning_rate": 7.195530726256984e-05,
      "loss": 0.7384,
      "step": 739
    },
    {
      "epoch": 0.8222222222222222,
      "grad_norm": 3.832871198654175,
      "learning_rate": 7.150837988826816e-05,
      "loss": 1.5095,
      "step": 740
    },
    {
      "epoch": 0.8233333333333334,
      "grad_norm": 3.306448221206665,
      "learning_rate": 7.106145251396649e-05,
      "loss": 1.2167,
      "step": 741
    },
    {
      "epoch": 0.8244444444444444,
      "grad_norm": 2.8404605388641357,
      "learning_rate": 7.061452513966481e-05,
      "loss": 1.2192,
      "step": 742
    },
    {
      "epoch": 0.8255555555555556,
      "grad_norm": 3.030716896057129,
      "learning_rate": 7.016759776536314e-05,
      "loss": 1.3686,
      "step": 743
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 2.772805690765381,
      "learning_rate": 6.972067039106146e-05,
      "loss": 0.9045,
      "step": 744
    },
    {
      "epoch": 0.8277777777777777,
      "grad_norm": 2.68326473236084,
      "learning_rate": 6.927374301675979e-05,
      "loss": 1.2698,
      "step": 745
    },
    {
      "epoch": 0.8288888888888889,
      "grad_norm": 5.65036153793335,
      "learning_rate": 6.882681564245811e-05,
      "loss": 1.4765,
      "step": 746
    },
    {
      "epoch": 0.83,
      "grad_norm": 3.895022392272949,
      "learning_rate": 6.837988826815642e-05,
      "loss": 1.4953,
      "step": 747
    },
    {
      "epoch": 0.8311111111111111,
      "grad_norm": 3.1119372844696045,
      "learning_rate": 6.793296089385475e-05,
      "loss": 1.4319,
      "step": 748
    },
    {
      "epoch": 0.8322222222222222,
      "grad_norm": 2.5633163452148438,
      "learning_rate": 6.748603351955307e-05,
      "loss": 0.8715,
      "step": 749
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 4.544686794281006,
      "learning_rate": 6.70391061452514e-05,
      "loss": 1.4886,
      "step": 750
    },
    {
      "epoch": 0.8344444444444444,
      "grad_norm": 3.02939510345459,
      "learning_rate": 6.659217877094972e-05,
      "loss": 1.3716,
      "step": 751
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 2.57867693901062,
      "learning_rate": 6.614525139664804e-05,
      "loss": 1.0705,
      "step": 752
    },
    {
      "epoch": 0.8366666666666667,
      "grad_norm": 2.402165412902832,
      "learning_rate": 6.569832402234637e-05,
      "loss": 0.9209,
      "step": 753
    },
    {
      "epoch": 0.8377777777777777,
      "grad_norm": 3.0656895637512207,
      "learning_rate": 6.52513966480447e-05,
      "loss": 1.2659,
      "step": 754
    },
    {
      "epoch": 0.8388888888888889,
      "grad_norm": 2.8303351402282715,
      "learning_rate": 6.480446927374302e-05,
      "loss": 0.9693,
      "step": 755
    },
    {
      "epoch": 0.84,
      "grad_norm": 2.772653102874756,
      "learning_rate": 6.435754189944134e-05,
      "loss": 1.5599,
      "step": 756
    },
    {
      "epoch": 0.8411111111111111,
      "grad_norm": 3.7701833248138428,
      "learning_rate": 6.391061452513967e-05,
      "loss": 1.3159,
      "step": 757
    },
    {
      "epoch": 0.8422222222222222,
      "grad_norm": 3.8215863704681396,
      "learning_rate": 6.346368715083799e-05,
      "loss": 1.3219,
      "step": 758
    },
    {
      "epoch": 0.8433333333333334,
      "grad_norm": 3.5121641159057617,
      "learning_rate": 6.301675977653632e-05,
      "loss": 1.7448,
      "step": 759
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 2.2144205570220947,
      "learning_rate": 6.256983240223464e-05,
      "loss": 0.6243,
      "step": 760
    },
    {
      "epoch": 0.8455555555555555,
      "grad_norm": 2.3898024559020996,
      "learning_rate": 6.212290502793297e-05,
      "loss": 0.8485,
      "step": 761
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 4.848909378051758,
      "learning_rate": 6.167597765363129e-05,
      "loss": 1.4615,
      "step": 762
    },
    {
      "epoch": 0.8477777777777777,
      "grad_norm": 4.328464508056641,
      "learning_rate": 6.122905027932962e-05,
      "loss": 1.4275,
      "step": 763
    },
    {
      "epoch": 0.8488888888888889,
      "grad_norm": 3.413908004760742,
      "learning_rate": 6.078212290502794e-05,
      "loss": 0.9214,
      "step": 764
    },
    {
      "epoch": 0.85,
      "grad_norm": 3.8479576110839844,
      "learning_rate": 6.0335195530726265e-05,
      "loss": 1.2818,
      "step": 765
    },
    {
      "epoch": 0.8511111111111112,
      "grad_norm": 3.5020368099212646,
      "learning_rate": 5.988826815642459e-05,
      "loss": 1.3227,
      "step": 766
    },
    {
      "epoch": 0.8522222222222222,
      "grad_norm": 2.7249817848205566,
      "learning_rate": 5.9441340782122914e-05,
      "loss": 0.9674,
      "step": 767
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 2.3546831607818604,
      "learning_rate": 5.899441340782124e-05,
      "loss": 0.6852,
      "step": 768
    },
    {
      "epoch": 0.8544444444444445,
      "grad_norm": 2.455782175064087,
      "learning_rate": 5.8547486033519563e-05,
      "loss": 1.1229,
      "step": 769
    },
    {
      "epoch": 0.8555555555555555,
      "grad_norm": 2.203794002532959,
      "learning_rate": 5.810055865921789e-05,
      "loss": 0.8521,
      "step": 770
    },
    {
      "epoch": 0.8566666666666667,
      "grad_norm": 3.3704802989959717,
      "learning_rate": 5.76536312849162e-05,
      "loss": 0.8661,
      "step": 771
    },
    {
      "epoch": 0.8577777777777778,
      "grad_norm": 2.9535908699035645,
      "learning_rate": 5.7206703910614524e-05,
      "loss": 1.1987,
      "step": 772
    },
    {
      "epoch": 0.8588888888888889,
      "grad_norm": 3.431732416152954,
      "learning_rate": 5.675977653631285e-05,
      "loss": 1.0757,
      "step": 773
    },
    {
      "epoch": 0.86,
      "grad_norm": 2.474515199661255,
      "learning_rate": 5.631284916201117e-05,
      "loss": 0.987,
      "step": 774
    },
    {
      "epoch": 0.8611111111111112,
      "grad_norm": 4.518620014190674,
      "learning_rate": 5.58659217877095e-05,
      "loss": 2.3151,
      "step": 775
    },
    {
      "epoch": 0.8622222222222222,
      "grad_norm": 3.2915143966674805,
      "learning_rate": 5.541899441340782e-05,
      "loss": 1.0423,
      "step": 776
    },
    {
      "epoch": 0.8633333333333333,
      "grad_norm": 2.4405124187469482,
      "learning_rate": 5.497206703910615e-05,
      "loss": 0.8717,
      "step": 777
    },
    {
      "epoch": 0.8644444444444445,
      "grad_norm": 4.784186363220215,
      "learning_rate": 5.452513966480447e-05,
      "loss": 1.0043,
      "step": 778
    },
    {
      "epoch": 0.8655555555555555,
      "grad_norm": 5.141296863555908,
      "learning_rate": 5.4078212290502797e-05,
      "loss": 2.0403,
      "step": 779
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 3.1308419704437256,
      "learning_rate": 5.363128491620112e-05,
      "loss": 0.9075,
      "step": 780
    },
    {
      "epoch": 0.8677777777777778,
      "grad_norm": 2.6198976039886475,
      "learning_rate": 5.3184357541899446e-05,
      "loss": 0.8904,
      "step": 781
    },
    {
      "epoch": 0.8688888888888889,
      "grad_norm": 2.364896774291992,
      "learning_rate": 5.273743016759777e-05,
      "loss": 1.1433,
      "step": 782
    },
    {
      "epoch": 0.87,
      "grad_norm": 2.2918169498443604,
      "learning_rate": 5.2290502793296095e-05,
      "loss": 1.0942,
      "step": 783
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 4.084898948669434,
      "learning_rate": 5.184357541899442e-05,
      "loss": 1.6859,
      "step": 784
    },
    {
      "epoch": 0.8722222222222222,
      "grad_norm": 2.826397657394409,
      "learning_rate": 5.139664804469274e-05,
      "loss": 1.1121,
      "step": 785
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 3.3024370670318604,
      "learning_rate": 5.094972067039106e-05,
      "loss": 0.9624,
      "step": 786
    },
    {
      "epoch": 0.8744444444444445,
      "grad_norm": 3.2736520767211914,
      "learning_rate": 5.050279329608939e-05,
      "loss": 1.2152,
      "step": 787
    },
    {
      "epoch": 0.8755555555555555,
      "grad_norm": 3.2641799449920654,
      "learning_rate": 5.005586592178771e-05,
      "loss": 1.0376,
      "step": 788
    },
    {
      "epoch": 0.8766666666666667,
      "grad_norm": 2.4934139251708984,
      "learning_rate": 4.9608938547486036e-05,
      "loss": 0.7104,
      "step": 789
    },
    {
      "epoch": 0.8777777777777778,
      "grad_norm": 2.9164047241210938,
      "learning_rate": 4.916201117318436e-05,
      "loss": 0.9737,
      "step": 790
    },
    {
      "epoch": 0.8788888888888889,
      "grad_norm": 4.891539096832275,
      "learning_rate": 4.8715083798882686e-05,
      "loss": 1.5708,
      "step": 791
    },
    {
      "epoch": 0.88,
      "grad_norm": 2.232678174972534,
      "learning_rate": 4.8268156424581004e-05,
      "loss": 0.8779,
      "step": 792
    },
    {
      "epoch": 0.8811111111111111,
      "grad_norm": 3.8979647159576416,
      "learning_rate": 4.782122905027933e-05,
      "loss": 1.4696,
      "step": 793
    },
    {
      "epoch": 0.8822222222222222,
      "grad_norm": 2.7546682357788086,
      "learning_rate": 4.737430167597765e-05,
      "loss": 1.4845,
      "step": 794
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 3.36250638961792,
      "learning_rate": 4.692737430167598e-05,
      "loss": 1.6175,
      "step": 795
    },
    {
      "epoch": 0.8844444444444445,
      "grad_norm": 2.2510790824890137,
      "learning_rate": 4.64804469273743e-05,
      "loss": 0.6951,
      "step": 796
    },
    {
      "epoch": 0.8855555555555555,
      "grad_norm": 4.727654457092285,
      "learning_rate": 4.603351955307263e-05,
      "loss": 1.7439,
      "step": 797
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 2.5035769939422607,
      "learning_rate": 4.558659217877095e-05,
      "loss": 0.8173,
      "step": 798
    },
    {
      "epoch": 0.8877777777777778,
      "grad_norm": 2.835341453552246,
      "learning_rate": 4.5139664804469276e-05,
      "loss": 0.989,
      "step": 799
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 2.9276888370513916,
      "learning_rate": 4.4692737430167594e-05,
      "loss": 0.6106,
      "step": 800
    },
    {
      "epoch": 0.89,
      "grad_norm": 3.526817798614502,
      "learning_rate": 4.424581005586592e-05,
      "loss": 1.2297,
      "step": 801
    },
    {
      "epoch": 0.8911111111111111,
      "grad_norm": 3.1021876335144043,
      "learning_rate": 4.3798882681564244e-05,
      "loss": 1.33,
      "step": 802
    },
    {
      "epoch": 0.8922222222222222,
      "grad_norm": 5.2570905685424805,
      "learning_rate": 4.335195530726257e-05,
      "loss": 1.3611,
      "step": 803
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 4.231399059295654,
      "learning_rate": 4.290502793296089e-05,
      "loss": 1.8347,
      "step": 804
    },
    {
      "epoch": 0.8944444444444445,
      "grad_norm": 4.325575351715088,
      "learning_rate": 4.245810055865922e-05,
      "loss": 1.6484,
      "step": 805
    },
    {
      "epoch": 0.8955555555555555,
      "grad_norm": 1.9469066858291626,
      "learning_rate": 4.201117318435754e-05,
      "loss": 0.7062,
      "step": 806
    },
    {
      "epoch": 0.8966666666666666,
      "grad_norm": 4.523123741149902,
      "learning_rate": 4.156424581005587e-05,
      "loss": 1.671,
      "step": 807
    },
    {
      "epoch": 0.8977777777777778,
      "grad_norm": 3.146721363067627,
      "learning_rate": 4.111731843575419e-05,
      "loss": 1.343,
      "step": 808
    },
    {
      "epoch": 0.8988888888888888,
      "grad_norm": 1.7432913780212402,
      "learning_rate": 4.0670391061452516e-05,
      "loss": 0.6987,
      "step": 809
    },
    {
      "epoch": 0.9,
      "grad_norm": 2.7636172771453857,
      "learning_rate": 4.022346368715084e-05,
      "loss": 1.0673,
      "step": 810
    },
    {
      "epoch": 0.9011111111111111,
      "grad_norm": 5.624182224273682,
      "learning_rate": 3.9776536312849166e-05,
      "loss": 1.9344,
      "step": 811
    },
    {
      "epoch": 0.9022222222222223,
      "grad_norm": 2.4138615131378174,
      "learning_rate": 3.9329608938547483e-05,
      "loss": 0.5818,
      "step": 812
    },
    {
      "epoch": 0.9033333333333333,
      "grad_norm": 2.8316750526428223,
      "learning_rate": 3.888268156424581e-05,
      "loss": 1.0446,
      "step": 813
    },
    {
      "epoch": 0.9044444444444445,
      "grad_norm": 1.996889591217041,
      "learning_rate": 3.843575418994413e-05,
      "loss": 0.4791,
      "step": 814
    },
    {
      "epoch": 0.9055555555555556,
      "grad_norm": 2.782519817352295,
      "learning_rate": 3.798882681564246e-05,
      "loss": 0.8532,
      "step": 815
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 2.4989724159240723,
      "learning_rate": 3.754189944134078e-05,
      "loss": 1.2669,
      "step": 816
    },
    {
      "epoch": 0.9077777777777778,
      "grad_norm": 3.2763888835906982,
      "learning_rate": 3.709497206703911e-05,
      "loss": 1.4879,
      "step": 817
    },
    {
      "epoch": 0.9088888888888889,
      "grad_norm": 2.2168831825256348,
      "learning_rate": 3.664804469273743e-05,
      "loss": 0.7581,
      "step": 818
    },
    {
      "epoch": 0.91,
      "grad_norm": 3.0824711322784424,
      "learning_rate": 3.6201117318435756e-05,
      "loss": 0.9708,
      "step": 819
    },
    {
      "epoch": 0.9111111111111111,
      "grad_norm": 2.782954454421997,
      "learning_rate": 3.575418994413408e-05,
      "loss": 0.8562,
      "step": 820
    },
    {
      "epoch": 0.9122222222222223,
      "grad_norm": 2.8458545207977295,
      "learning_rate": 3.5307262569832406e-05,
      "loss": 0.7607,
      "step": 821
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 4.067529678344727,
      "learning_rate": 3.486033519553073e-05,
      "loss": 0.7959,
      "step": 822
    },
    {
      "epoch": 0.9144444444444444,
      "grad_norm": 3.5979018211364746,
      "learning_rate": 3.4413407821229055e-05,
      "loss": 1.6289,
      "step": 823
    },
    {
      "epoch": 0.9155555555555556,
      "grad_norm": 4.6360039710998535,
      "learning_rate": 3.396648044692737e-05,
      "loss": 1.6494,
      "step": 824
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 2.705359697341919,
      "learning_rate": 3.35195530726257e-05,
      "loss": 1.0058,
      "step": 825
    },
    {
      "epoch": 0.9177777777777778,
      "grad_norm": 2.753403663635254,
      "learning_rate": 3.307262569832402e-05,
      "loss": 0.9449,
      "step": 826
    },
    {
      "epoch": 0.9188888888888889,
      "grad_norm": 2.2853200435638428,
      "learning_rate": 3.262569832402235e-05,
      "loss": 0.7892,
      "step": 827
    },
    {
      "epoch": 0.92,
      "grad_norm": 3.0838944911956787,
      "learning_rate": 3.217877094972067e-05,
      "loss": 0.9913,
      "step": 828
    },
    {
      "epoch": 0.9211111111111111,
      "grad_norm": 1.6793662309646606,
      "learning_rate": 3.1731843575418996e-05,
      "loss": 0.4241,
      "step": 829
    },
    {
      "epoch": 0.9222222222222223,
      "grad_norm": 3.1753392219543457,
      "learning_rate": 3.128491620111732e-05,
      "loss": 1.6712,
      "step": 830
    },
    {
      "epoch": 0.9233333333333333,
      "grad_norm": 4.084089279174805,
      "learning_rate": 3.0837988826815645e-05,
      "loss": 1.4478,
      "step": 831
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 3.3375282287597656,
      "learning_rate": 3.039106145251397e-05,
      "loss": 0.955,
      "step": 832
    },
    {
      "epoch": 0.9255555555555556,
      "grad_norm": 4.833009719848633,
      "learning_rate": 2.9944134078212295e-05,
      "loss": 1.6496,
      "step": 833
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 2.9815003871917725,
      "learning_rate": 2.949720670391062e-05,
      "loss": 0.7555,
      "step": 834
    },
    {
      "epoch": 0.9277777777777778,
      "grad_norm": 4.799415588378906,
      "learning_rate": 2.9050279329608944e-05,
      "loss": 2.5915,
      "step": 835
    },
    {
      "epoch": 0.9288888888888889,
      "grad_norm": 3.6089000701904297,
      "learning_rate": 2.8603351955307262e-05,
      "loss": 1.1496,
      "step": 836
    },
    {
      "epoch": 0.93,
      "grad_norm": 3.3712596893310547,
      "learning_rate": 2.8156424581005587e-05,
      "loss": 1.6518,
      "step": 837
    },
    {
      "epoch": 0.9311111111111111,
      "grad_norm": 1.7675639390945435,
      "learning_rate": 2.770949720670391e-05,
      "loss": 0.6438,
      "step": 838
    },
    {
      "epoch": 0.9322222222222222,
      "grad_norm": 2.8053417205810547,
      "learning_rate": 2.7262569832402236e-05,
      "loss": 0.7261,
      "step": 839
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 2.858264923095703,
      "learning_rate": 2.681564245810056e-05,
      "loss": 1.1753,
      "step": 840
    },
    {
      "epoch": 0.9344444444444444,
      "grad_norm": 2.7372429370880127,
      "learning_rate": 2.6368715083798885e-05,
      "loss": 1.0936,
      "step": 841
    },
    {
      "epoch": 0.9355555555555556,
      "grad_norm": 2.9731287956237793,
      "learning_rate": 2.592178770949721e-05,
      "loss": 1.0666,
      "step": 842
    },
    {
      "epoch": 0.9366666666666666,
      "grad_norm": 3.4093668460845947,
      "learning_rate": 2.547486033519553e-05,
      "loss": 1.6108,
      "step": 843
    },
    {
      "epoch": 0.9377777777777778,
      "grad_norm": 2.7337839603424072,
      "learning_rate": 2.5027932960893856e-05,
      "loss": 1.2887,
      "step": 844
    },
    {
      "epoch": 0.9388888888888889,
      "grad_norm": 3.803877830505371,
      "learning_rate": 2.458100558659218e-05,
      "loss": 1.3313,
      "step": 845
    },
    {
      "epoch": 0.94,
      "grad_norm": 3.269416332244873,
      "learning_rate": 2.4134078212290502e-05,
      "loss": 1.7301,
      "step": 846
    },
    {
      "epoch": 0.9411111111111111,
      "grad_norm": 6.133298397064209,
      "learning_rate": 2.3687150837988827e-05,
      "loss": 1.7004,
      "step": 847
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 4.724261283874512,
      "learning_rate": 2.324022346368715e-05,
      "loss": 2.0609,
      "step": 848
    },
    {
      "epoch": 0.9433333333333334,
      "grad_norm": 2.5852181911468506,
      "learning_rate": 2.2793296089385476e-05,
      "loss": 1.0232,
      "step": 849
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 2.624230146408081,
      "learning_rate": 2.2346368715083797e-05,
      "loss": 0.8316,
      "step": 850
    },
    {
      "epoch": 0.9455555555555556,
      "grad_norm": 3.055145502090454,
      "learning_rate": 2.1899441340782122e-05,
      "loss": 0.8212,
      "step": 851
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 2.4404101371765137,
      "learning_rate": 2.1452513966480446e-05,
      "loss": 0.6882,
      "step": 852
    },
    {
      "epoch": 0.9477777777777778,
      "grad_norm": 2.4780161380767822,
      "learning_rate": 2.100558659217877e-05,
      "loss": 0.6815,
      "step": 853
    },
    {
      "epoch": 0.9488888888888889,
      "grad_norm": 2.3335375785827637,
      "learning_rate": 2.0558659217877096e-05,
      "loss": 0.8411,
      "step": 854
    },
    {
      "epoch": 0.95,
      "grad_norm": 3.281268835067749,
      "learning_rate": 2.011173184357542e-05,
      "loss": 0.7031,
      "step": 855
    },
    {
      "epoch": 0.9511111111111111,
      "grad_norm": 4.329751491546631,
      "learning_rate": 1.9664804469273742e-05,
      "loss": 1.7896,
      "step": 856
    },
    {
      "epoch": 0.9522222222222222,
      "grad_norm": 3.2071657180786133,
      "learning_rate": 1.9217877094972066e-05,
      "loss": 0.907,
      "step": 857
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 3.431771755218506,
      "learning_rate": 1.877094972067039e-05,
      "loss": 0.7657,
      "step": 858
    },
    {
      "epoch": 0.9544444444444444,
      "grad_norm": 3.165252447128296,
      "learning_rate": 1.8324022346368716e-05,
      "loss": 1.0928,
      "step": 859
    },
    {
      "epoch": 0.9555555555555556,
      "grad_norm": 3.1815848350524902,
      "learning_rate": 1.787709497206704e-05,
      "loss": 1.2243,
      "step": 860
    },
    {
      "epoch": 0.9566666666666667,
      "grad_norm": 2.8818345069885254,
      "learning_rate": 1.7430167597765365e-05,
      "loss": 1.0762,
      "step": 861
    },
    {
      "epoch": 0.9577777777777777,
      "grad_norm": 2.373987913131714,
      "learning_rate": 1.6983240223463686e-05,
      "loss": 0.5484,
      "step": 862
    },
    {
      "epoch": 0.9588888888888889,
      "grad_norm": 2.509645938873291,
      "learning_rate": 1.653631284916201e-05,
      "loss": 1.0742,
      "step": 863
    },
    {
      "epoch": 0.96,
      "grad_norm": 3.0786314010620117,
      "learning_rate": 1.6089385474860336e-05,
      "loss": 1.0717,
      "step": 864
    },
    {
      "epoch": 0.9611111111111111,
      "grad_norm": 2.1858468055725098,
      "learning_rate": 1.564245810055866e-05,
      "loss": 0.3764,
      "step": 865
    },
    {
      "epoch": 0.9622222222222222,
      "grad_norm": 4.302984714508057,
      "learning_rate": 1.5195530726256985e-05,
      "loss": 2.1989,
      "step": 866
    },
    {
      "epoch": 0.9633333333333334,
      "grad_norm": 2.9651029109954834,
      "learning_rate": 1.474860335195531e-05,
      "loss": 1.0697,
      "step": 867
    },
    {
      "epoch": 0.9644444444444444,
      "grad_norm": 3.5304248332977295,
      "learning_rate": 1.4301675977653631e-05,
      "loss": 1.1385,
      "step": 868
    },
    {
      "epoch": 0.9655555555555555,
      "grad_norm": 4.337067604064941,
      "learning_rate": 1.3854748603351956e-05,
      "loss": 1.714,
      "step": 869
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 3.4769513607025146,
      "learning_rate": 1.340782122905028e-05,
      "loss": 1.0471,
      "step": 870
    },
    {
      "epoch": 0.9677777777777777,
      "grad_norm": 2.0073795318603516,
      "learning_rate": 1.2960893854748605e-05,
      "loss": 0.5807,
      "step": 871
    },
    {
      "epoch": 0.9688888888888889,
      "grad_norm": 2.129613161087036,
      "learning_rate": 1.2513966480446928e-05,
      "loss": 0.5733,
      "step": 872
    },
    {
      "epoch": 0.97,
      "grad_norm": 4.755823612213135,
      "learning_rate": 1.2067039106145251e-05,
      "loss": 2.2178,
      "step": 873
    },
    {
      "epoch": 0.9711111111111111,
      "grad_norm": 4.001639366149902,
      "learning_rate": 1.1620111731843576e-05,
      "loss": 1.2428,
      "step": 874
    },
    {
      "epoch": 0.9722222222222222,
      "grad_norm": 2.4158270359039307,
      "learning_rate": 1.1173184357541899e-05,
      "loss": 1.0766,
      "step": 875
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 3.1225194931030273,
      "learning_rate": 1.0726256983240223e-05,
      "loss": 0.8314,
      "step": 876
    },
    {
      "epoch": 0.9744444444444444,
      "grad_norm": 3.5932540893554688,
      "learning_rate": 1.0279329608938548e-05,
      "loss": 1.1594,
      "step": 877
    },
    {
      "epoch": 0.9755555555555555,
      "grad_norm": 6.028049945831299,
      "learning_rate": 9.832402234636871e-06,
      "loss": 1.3021,
      "step": 878
    },
    {
      "epoch": 0.9766666666666667,
      "grad_norm": 2.9221999645233154,
      "learning_rate": 9.385474860335196e-06,
      "loss": 0.8495,
      "step": 879
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 3.469708204269409,
      "learning_rate": 8.93854748603352e-06,
      "loss": 1.1128,
      "step": 880
    },
    {
      "epoch": 0.9788888888888889,
      "grad_norm": 3.0232086181640625,
      "learning_rate": 8.491620111731843e-06,
      "loss": 1.0325,
      "step": 881
    },
    {
      "epoch": 0.98,
      "grad_norm": 2.6581547260284424,
      "learning_rate": 8.044692737430168e-06,
      "loss": 0.9452,
      "step": 882
    },
    {
      "epoch": 0.9811111111111112,
      "grad_norm": 4.582785129547119,
      "learning_rate": 7.5977653631284925e-06,
      "loss": 1.6712,
      "step": 883
    },
    {
      "epoch": 0.9822222222222222,
      "grad_norm": 3.281116485595703,
      "learning_rate": 7.1508379888268155e-06,
      "loss": 1.4392,
      "step": 884
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 4.959157466888428,
      "learning_rate": 6.70391061452514e-06,
      "loss": 1.7158,
      "step": 885
    },
    {
      "epoch": 0.9844444444444445,
      "grad_norm": 2.7686569690704346,
      "learning_rate": 6.256983240223464e-06,
      "loss": 1.0996,
      "step": 886
    },
    {
      "epoch": 0.9855555555555555,
      "grad_norm": 4.4961700439453125,
      "learning_rate": 5.810055865921788e-06,
      "loss": 1.9359,
      "step": 887
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 2.8460378646850586,
      "learning_rate": 5.363128491620112e-06,
      "loss": 0.7458,
      "step": 888
    },
    {
      "epoch": 0.9877777777777778,
      "grad_norm": 3.584243059158325,
      "learning_rate": 4.9162011173184354e-06,
      "loss": 1.0465,
      "step": 889
    },
    {
      "epoch": 0.9888888888888889,
      "grad_norm": 3.4118897914886475,
      "learning_rate": 4.46927374301676e-06,
      "loss": 1.3917,
      "step": 890
    },
    {
      "epoch": 0.99,
      "grad_norm": 1.727115273475647,
      "learning_rate": 4.022346368715084e-06,
      "loss": 0.3084,
      "step": 891
    },
    {
      "epoch": 0.9911111111111112,
      "grad_norm": 2.819120168685913,
      "learning_rate": 3.5754189944134077e-06,
      "loss": 0.5049,
      "step": 892
    },
    {
      "epoch": 0.9922222222222222,
      "grad_norm": 2.0998878479003906,
      "learning_rate": 3.128491620111732e-06,
      "loss": 0.4501,
      "step": 893
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 5.110441207885742,
      "learning_rate": 2.681564245810056e-06,
      "loss": 2.2156,
      "step": 894
    },
    {
      "epoch": 0.9944444444444445,
      "grad_norm": 1.9603396654129028,
      "learning_rate": 2.23463687150838e-06,
      "loss": 0.4525,
      "step": 895
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 3.751204490661621,
      "learning_rate": 1.7877094972067039e-06,
      "loss": 1.8559,
      "step": 896
    },
    {
      "epoch": 0.9966666666666667,
      "grad_norm": 3.0773651599884033,
      "learning_rate": 1.340782122905028e-06,
      "loss": 0.8355,
      "step": 897
    },
    {
      "epoch": 0.9977777777777778,
      "grad_norm": 3.9164958000183105,
      "learning_rate": 8.938547486033519e-07,
      "loss": 1.6073,
      "step": 898
    },
    {
      "epoch": 0.9988888888888889,
      "grad_norm": 2.4907827377319336,
      "learning_rate": 4.4692737430167597e-07,
      "loss": 1.154,
      "step": 899
    },
    {
      "epoch": 1.0,
      "grad_norm": 3.1281039714813232,
      "learning_rate": 0.0,
      "loss": 1.264,
      "step": 900
    }
  ],
  "logging_steps": 1,
  "max_steps": 900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1171450854850560.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
